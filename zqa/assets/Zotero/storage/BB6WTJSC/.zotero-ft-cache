Lecture Notes in Artificial Intelligence 3176
Edited by J. G. Carbonell and J. Siekmann
Subseries of Lecture Notes in Computer Science


Olivier Bousquet Ulrike von Luxburg Gunnar Rätsch (Eds.)
Advanced Lectures
on Machine Learning
ML Summer Schools 2003 Canberra, Australia, February 2-14, 2003 Tübingen, Germany, August 4-16, 2003 Revised Lectures
13


Series Editors
Jaime G. Carbonell, Carnegie Mellon University, Pittsburgh, PA, USA Jörg Siekmann, University of Saarland, Saarbrücken, Germany
Volume Editors
Olivier Bousquet Ulrike von Luxburg Max Planck Institute for Biological Cybernetics Spemannstr. 38, 72076 Tübingen, Germany E-mail: {bousquet, ule}@tuebingen.mpg.de
Gunnar Rätsch Fraunhofer FIRST Kekuléstr. 7, 10245 Berlin, Germany and Max Planck Institute for Biological Cybernetics Spemannstr. 38, 72076 Tübingen, Germany E-mail: Gunnar.Raetsch@tuebingen.mpg.de
Library of Congress Control Number: 2004111357
CR Subject Classification (1998): I.2.6, I.2, F.1, F.2, I.5
ISSN 0302-9743 ISBN 3-540-23122-6 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting, reproduction on microfilms or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer. Violations are liable to prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springeronline.com
© Springer-Verlag Berlin Heidelberg 2004 Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientific Publishing Services, Chennai, India Printed on acid-free paper SPIN: 11322894 06/3142 5 4 3 2 1 0


Preface
Machine Learning has become a key enabling technology for many engineering applications, investigating scientific questions and theoretical problems alike. To stimulate discussions and to disseminate new results, a series of summer schools was started in February 2002. One year later two more of such summer schools were held, one at the Australian National University in Canberra, Australia, and the other one in the Max-Planck Institute for Biological Cybernetics, in T ̈ubingen, Germany.
The current book contains a collection of main talks held during those two summer schools, presented as tutorial chapters on topics such as Pattern Recognition, Bayesian Inference, Unsupervised Learning and Statistical Learning Theory. The papers provide an in-depth overview of these exciting new areas, contain a large set of references, and thereby provide the interested reader with further information to start or to pursue his own research in these directions.
Complementary to the book, photos and slides of the presentations can be obtained at
http://mlg.anu.edu.au/summer2003 and
http://www.irccyn.ec-nantes.fr/mlschool/mlss03/home03.php.
The general entry point for past and future Machine Learning Summer Schools is
http://www.mlss.cc
It is our hope that graduate students, lecturers, and researchers alike will find this book useful in learning and teaching Machine Learning, thereby continuing the mission of the Machine Learning Summer Schools.
T ̈ubingen, June 2004 Olivier Bousquet Ulrike von Luxburg Gunnar Ra ̈tsch
Empirical Inference for Machine Learning and Perception Max-Planck Institute for Biological Cybernetics


Acknowledgments
We gratefully thank all the individuals and organizations responsible for the success of the summer schools.
Local Arrangements
Canberra
Special thanks go to Michelle Moravec and Heather Slater for all their support during the preparations, to Joe Elso, Kim Holburn, and Fergus McKenzieKay for IT support, to Cheng Soon-Ong, Kristy Sim, Edward Harrington, Evan Greensmith, and the students at the Computer Sciences Laboratory for their help throughout the course of the Summer School.
T ̈ubingen
Special thanks go to Sabrina Nielebock for all her work during the preparation and on the site, to Dorothea Epting and the staff of the Max Planck Guest House, to Sebastian Stark for IT support, to all the students and administration of the Max-Planck Institute for Biological Cybernetics for their help throughout the Summer School.
Sponsoring Institutions
Canberra
– Research School of Information Sciences and Engineering, Australia – The National Institute of Engineering and Information Science, Australia
T ̈ubingen
– Centre National de la Recherche Scientifique, France – French-German University – Max-Planck Institute for Biological Cybernetics, Germany


Speakers
Canberra
Shun-Ichi Amari Eleazar Eskin Zoubin Ghahramani Peter Hall Markus Hegland
Gabor Lugosi Jyrki Kivinen John Lloyd Shahar Mendelson Mike Osborne
Petra Phillips Gunnar Ra ̈tsch Alex Smola S.V.N. Vishwanathan Robert C. Williamson
T ̈ubingen
Christophe Andrieu Pierre Baldi L ́eon Bottou Ste ́phane Boucheron Olivier Bousquet Chris Burges Jean-Franc ̧ois Cardoso Manuel Davy
Andre ́ Elisseeff Arthur Gretton Peter Gru ̈nwald Thorsten Joachims Massimiliano Pontil Carl Rasmussen Mike Tipping Bernhard Scho ̈lkopf
Steve Smale Alex Smola Vladimir Vapnik Jason Weston Elad Yom-Tov Ding-Xuan Zhou
Organization Committees
Canberra: Gunnar Ra ̈tsch and Alex Smola T ̈ubigen: Olivier Bousquet, Manuel Davy, Fre ́d ́eric Desobry, Ulrike von Luxburg and Bernhard Scho ̈lkopf


Table of Contents
An Introduction to Pattern Classification
Elad Yom-Tov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
Some Notes on Applied Mathematics for Machine Learning
Christopher J.C. Burges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Bayesian Inference: An Introduction to Principles and Practice in Machine Learning
Michael E. Tipping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
Gaussian Processes in Machine Learning
Carl Edward Rasmussen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
Unsupervised Learning
Zoubin Ghahramani . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
Monte Carlo Methods for Absolute Beginners
Christophe Andrieu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Stochastic Learning
L ́eon Bottou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
Introduction to Statistical Learning Theory
Olivier Bousquet, Ste ́phane Boucheron, Ga ́bor Lugosi . . . . . . . . . . . . . . . 169
Concentration Inequalities
St ́ephane Boucheron, Ga ́bor Lugosi, Olivier Bousquet . . . . . . . . . . . . . . . 208
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241


An Introduction to Pattern Classification
Elad Yom-Tov
IBM Haifa Research Labs, University Campus, Haifa 31905, Israel yomtov@il.ibm.com
1 Introduction
Pattern classification is the field devoted to the study of methods designed to categorize data into distinct classes. This categorization can be either distinct labeling of the data (supervised learning), division of the data into classes (unsupervised learning), selection of the most significant features of the data (feature selection), or a combination of more than one of these tasks. Pattern classification is one of a class of problems that humans (under most circumstances) are able to accomplish extremely well, but are difficult for computers to perform. This subject has been under extensive study for many years. However during the past decade, with the introduction of several new classes of pattern classification algorithms this field seems to achieve performance much better than previously attained. The goal of the following article is to give the reader a broad overview of the field. As such, it attempts to introduce the reader to important aspects of pattern classification, without delving deeply into any of the subject matters. The exceptions to this rule are those points deemed especially important or those that are of special interest. Finally, we note that the focus of this article are statistical methods for pattern recognition. Thus, methods such as fuzzy logic and rule-based methods are outside the scope of this article.
2 What Is Pattern Classification?
Pattern classification, also referred to as pattern recognition, attempts to build algorithms capable of automatically constructing methods for distinguishing between different exemplars, based on their differentiating patterns. Watanabe [53] described a pattern as ”the opposite of chaos; it is an entity, vaguely defined, that could be given a name.” Examples of patterns are human faces, handwritten letters, and the DNA sequences that may cause a certain disease. More formally, the goal of a (supervised) pattern classification task is to find a functional mapping between the input data X, used to describe an input pattern, to a class label Y so that Y = f (X). Construction of the mapping is based on training data supplied to the pattern classification algorithm. The mapping f should give the smallest possible error in the mapping, i.e. the minimum number of examples where Y will be the wrong label, especially on test data not seen by the algorithm during the learning phase.
O. Bousquet et al. (Eds.): Machine Learning 2003, LNAI 3176, pp. 1–20, 2004. c© Springer-Verlag Berlin Heidelberg 2004


2 E. Yom-Tov
An important division of pattern classification tasks are supervised as opposed to unsupervised classification. In supervised tasks the training data consists of training patterns, as well as their required labeling. An example are DNA sequences labeled to show which examples are known to harbor a genetic trait and which ones do not. In unsupervised classification tasks the labels are not provided, and the task of the algorithm is to find a ”good” partition of the data into clusters. Examples for this kind of task are grouping of Web pages into sets so that each set is concerned with a single subject matter. A pattern is described by its features. These are the characteristics of the examples for a given problem. For example, in a face recognition task some features could be the color of the eyes or the distance between the eyes. Thus, the input to a pattern recognition task can be viewed as a two-dimensional matrix, whose axes are the examples and the features. Pattern classification tasks are customarily divided into several distinct blocks. These are:
1. Data collection and representation. 2. Feature selection and/or feature reduction. 3. Clustering. 4. Classification.
Data collection and representation are mostly problem-specific. Therefore it is difficult to give general statements about this step of the process. In broad terms, one should try to find invariant features, that describe the differences in classes as best as possible. Feature selection and feature reduction attempt to reduce the dimensionality (i.e. the number of features) for the remaining steps of the task. Clustering methods are used in order to reduce the number of training examples to the task. Finally, the classification phase of the process finds the actual mapping between patterns and labels (or targets). In many applications not all steps are needed. Indeed, as computational power grows, the need to reduce the number of patterns used as input to the classification task decreases, and may therefore make the clustering stage superfluous for many applications. In the following pages we describe feature selection and reduction, clustering, and classification.
3 Feature Selection and Feature Reduction: Removing Excess Data
When data is collected for later classification, it may seem reasonable to assume that if more features describing the data are collected it will be easier to classify these data correctly. In fact, as Trunk [50] demonstrated, more data may be detrimental to classification, especially if the additional data is highly correlated with previous data. Furthermore, noisy and irrelevant features are detrimental to classification as they are known to cause the classifier to have poor generalization,


An Introduction to Pattern Classification 3
increase the computational complexity, and require many training samples to reach a given accuracy [4]. Conversely, selecting too few features will lead to the ugly duckling theorem [53], that is, it will be impossible to distinguish between the classes because there is too little data to differentiate the classes. For example, suppose we wish to classify a vertebrated animal into one of the vertebra classes (Mammals, Birds, Fish, Reptiles, or Amphibians). A feature that will tell us if the animal has skin is superfluous, since all vertebrates have skins. However, a feature that measures if the animal has warm blood is highly significant for the classification. A feature selection algorithm should be able to identify and remove the former feature, while preserving the latter. Hence the goal of this stage in the processing is to choose a subset of features or some combination of the input features that will best represent the data. We refer to the process of choosing a subset of the features as feature selection, and to finding a good combination of the features as feature reduction. Feature selection is a difficult combinatorial optimization problem. Finding the best subset of features by testing all possible combinations is practically impossible even when the number of input features is modest. For example, attempting to test all possible combinations of 100 input features will require testing 1030 combinations. It is not uncommon for text classification problems to have 104 to 107 features [27]. Consequently numerous methods have been proposed for finding a (suboptimal) solution by testing a fraction of the possible combinations. Feature selection methods can be divided into three main types [4]:
1. Wrapper methods: The feature selection is performed around (and with) a given classification algorithm. The classification algorithm is used for ranking possible feature combinations. 2. Embedded methods: The feature selection is embedded within the classification algorithm. 3. Filter methods: Features are selected for classification independently of the classification algorithm.
Most feature selection methods are of the wrapper type. The simplest algorithms in this category are the exhaustive search, which is practical only when the number of features is small, sequential forward feature selection (SFFS) and sequential backward feature selection (SBFS). In sequential forward feature selection the feature with which the lowest classification error is reached is selected. Then, the feature that, when added, causes the largest reduction in error is added to the set of selected features. This process is continued iteratively until the maximum number of features needed are found or until the classification error starts to increase. Although sequential feature selection does not assume dependence between features, it usually attains surprisingly reasonable results. There are several minor modifications to SFFS and SBFS, such as Sequential Floating Search [41] or the ”Plus n, take away m” features. One of the major drawbacks of methods that select and add a single feature at each step is that they might not find combinations of features that perform well


4 E. Yom-Tov
together, but are poor predictors individually. More sophisticated methods for feature selection use simulated annealing or genetic algorithms [56] for solving the optimization problem of feature selection. The latter approach has shown promise in solving problems where the number of input features is extremely large. An interesting approach to feature selection is based in information theoretic considerations [25]. This algorithm estimates the cross-entropy between every pair of features, and discards those features that have a large cross-entropy with other features, thus removing features that add little additional classification information. This is because the cross-entropy estimates the amount of knowledge that one feature provides on other features. The algorithm is appealing in that it is independent of the classification algorithm, i.e. it is a filter algorithm. However, the need to estimate the cross entropy between features limits its use to applications where the datasets are large or to cases where features are discrete. As mentioned above, a second approach to reducing the dimension of the features is to find a lower-dimensional combination (linear or non-linear) of the features which represent the data as well as possible in the required dimension. The most commonly used technique for feature reduction is principal component analysis (PCA), also known as the Karhunen-Loeve Transform (KLT). PCA reshapes the data along the directions of maximal variance. PCA works by computing the eigenvectors corresponding to the largest eigenvalues of the covariance matrix of the data, and returning the projection of the data on these eigenvectors. An example of feature reduction using PCA is given in Figure 1.
Fig. 1. Feature reduction using principle component analysis. The figure on the left shows the original data. Note that most of the variance in the data is along a single direction. The figure on the right shows probability density function of the same data after feature reduction to a dimension of 1 using PCA
Principle component analysis does not take into account the labels of the data. As such, it is an unsupervised method. A somewhat similar, albeit supervised, linear method is the Fisher Discriminant Analysis (FDA). This method projects the data on a single dimension, while maximizing the separation between the classes of the data. A more sophisticated projection method is Independent Component Analysis (ICA)[8]. This method finds a linear mixture of the data, in the


An Introduction to Pattern Classification 5
same dimension of the data or lower. ICA attempts to find a mixture matrix such that each of the projections will be as independent as possible from the other projections. Instead of finding a linear mixture of the feature, it is also possible to find a nonlinear mixture of the data. This is usually done through modifications of the above-mentioned linear methods. Examples of such methods are nonlinear component analysis [33], nonlinear FDA [32], and Kernel PCA[46]. The latter method works by remapping data by way of a kernel function into feature space where the principle components of the data are found. As a final note on feature selection and feature reduction, one should note that as the ratio between the number of features and the number of training examples increases, it becomes likelier for a noisy and irrelevant feature to seem relevant for the specific set of examples. Indeed, feature selection is sometimes viewed as an ill-posed problem [52], which is why application of such methods should be performed with care. For example, if possible, the feature selection algorithm should be run several times, and the results tested for consistency.
4 Clustering
The second stage of the classification process endeavors to reduce the number of data points by clustering the data and finding representative data points (for example, cluster centers), or by removing superfluous data points. This stage is usually performed using unsupervised methods. A cluster of points is not a well-defined object. Instead, clusters are defined based on their environment and the scale at which the data is examined. Figure 2 demonstrates the nature of the problem. Two possible definitions for clusters[23] are: (I) Patterns within a cluster are more similar to each other than are patterns belonging to other clusters. (II) A cluster is a volume of high-density points separated from other clusters by a relatively low density volumes. Both these definitions do not suggest a practical solution to the problem of finding clusters. In practice one usually specifies a criterion for joining points into clusters or the number of clusters to be found, and these are used by the clustering algorithm in place of a definition of a cluster. This practicality results in a major drawback of clustering algorithms: A clustering algorithm will find clusters even if there are no clusters in the data. Returning to the vertebrate classification problem discussed earlier, if we are given data on all vertebrate species, we may find that this comprises of too many training examples. It may be enough to find a representative sample for each of the classes and use it to build the classifier. Clustering algorithms attempt to find such representatives. Note that representative samples can be either actual samples drawn from the data (for example, a human as an example for a mammal) or an average of several samples (i.e. an animal with some given percentage of hair on its body as a representative mammal). The computational cost of finding an optimal partition of a dataset into a given number of clusters is usually prohibitively high. Therefore, in most cases


6 E. Yom-Tov
Fig. 2. An example of data points for clustering. Many possible clustering configurations can be made for this data, based on the scale at which the data is examined, the shape of the clusters, etc
clustering algorithms attempt to find a suboptimal partition in a reasonable number of computations. Clustering algorithms can be divided into Top-Down (or partitional) algorithms and Bottom-Up (or hierarchical) algorithms. A simple example for Bottom-Up algorithms is the Agglomerative Hierarchical Clustering Algorithm (AGHC). This algorithm is an iterative algorithm, which starts by assuming that each data point is a cluster. At each iteration two clusters are merged until a preset number of clusters is reached. The decision on which clusters are to be merged can be done using one of several functions, i.e. distance between cluster centers, distance between the two nearest points in different clusters, etc. AGHC is a very simple, intuitive scheme. However, it is computationally intensive and thus impractical for medium and large datasets. Top-Down methods are the type more frequently used for clustering due to their lower computational cost, despite the fact that they usually find an inferior solution compared to Bottom-Up algorithms. Probably the most popular amongst Top-Down clustering algorithms in the K-means algorithm [28], a pseudo-code of which is given in figure 3. K-means is usually reasonably fast, but care should be taken in the initial setting of the cluster centers so as to attain a good partition of the data. There are probably hundreds of Top-Down clustering algorithms, but popular algorithms include fuzzy k-means [3], Kohonen maps [24], and competitive learning [44]. Recently, with the advent of kernel-based methods several algorithms for clustering using kernels have been suggested (e.g. [2]). The basic idea behind these algorithms is to map the data into a higher dimension using a non-linear function of the input features, and to cluster the data using simple clustering algorithms at the higher dimension. More details regarding kernels are given in the Classification section of this paper. One of the main advantages of kernel methods is that simple clusters (for example, ellipsoid clusters) formed in a higher dimension correspond to complex clusters in the input space. These methods seem to provide excellent clustering results, with reasonable computational costs.


An Introduction to Pattern Classification 7
A related class of clustering algorithms are the Spectral Clustering methods [37, 11]. These methods first map the data into a matrix representing the distance between the input patterns. The matrix is then projected onto its k largest eigenvectors, and the clustering is performed on this projection. These methods demonstrated impressive results on several datasets, with computational costs slightly higher than those of kernel-based algorithms.
The K-means clustering algorithm
1. Begin initialize N random cluster centers. 2. Assign each of the data points the nearest of the N cluster centers. 3. Recompute the cluster centers by averaging the points assigned to each cluster. 4. Repeat steps 2-4 until the there is no change in the location of the cluster centers. 5. Return the cluster centers.
5 Classification
Classification, the final stage of a pattern classifier, is the process of assigning labels to test patterns, based on previously labeled training patterns. This process is commonly divided into a learning phase, where the classification algorithm is trained, and a classification phase, where the algorithm labels new data. The general model for statistical pattern classification is one where patterns are drawn from an unknown distribution P , which depends on the label of the data (i.e., P (x|ωi) i = 1, . . . , N , where N is the number of labels in the data). During the learning phase the classification algorithm is trained with the goal of minimizing the error that will be obtained when classifying some test data. This error is known as the risk or the expected loss. When discussing the pros and cons of classification algorithms, it is important to set criteria for assessing these algorithms. In the following pages we describe several classification algorithms and later summarize (in table 1) their strong and weak points with regard to the following points:
– How small are the classification errors reached by the algorithm? – What is the computational cost and the memory requirements for both training and testing? – How difficult is it for a novice user to build and train an efficient classifier? – Is the algorithm able to learn on-line (i.e. as the data appears, allowing each data point to be addressed only once)? – Can one gain insight about the problem from examining the trained classifier?
It is important to note that when discussing the classification errors of classifiers one is usually interested in the errors obtained when classifying test data. Many classifiers can be trained to classify all the training data correctly. This
Fig. 3. Pseudo-code of the K-means clustering algorithm


8 E. Yom-Tov
does not imply that they will perform well on unseen test data. In fact, it is more often the case that if a classification algorithm reduces the training set error to zeros it has been over-fitted to the training data. Section 6 discusses methods that can be applied in order to avoid over-fitting. A good classifier should be able to generalize from the training data, i.e. learn a set of rules from the training data that will then be used to classify the test data. The first question one should address in the context of classification is, is there an optimal classification rule (with regard to the classification error)? Surprisingly, such a rule exists, but in practice one can rarely use it. The optimal classification rule is the Bayes rule. Suppose that we wish to minimize the expected loss function: R (ωi|x) = ∑ L (ωi, ωj) P (ωj|x) where L is the loss function for deciding on class i given that the correct class is class j. If the zero/one loss is used (i.e. a wrong decision entails a loss of one, and a correct decision results in a loss of zero) the Bayes rule simplifies to the Maximum Aposteriory (MAP) rule, which requires that we label an input sample x with the label i if P (ωi|x) > P (ωj|x) for all j = i. As mentioned above, it is usually impossible to use the Bayes rule because it requires full knowledge of the class-conditional densities of the data. Thus, one is frequently left with one of two options. If a model for the class-conditional densities is known (for example, if it is known that the data consists of two Gaussians for one class and a single uniform distribution for the other class), one can use plug-in rules to build a classifier. Here, given the model, its parameters are estimated, and then the MAP rule can be used. If a model for the data cannot be provided, classifiers can proceed by estimating the density of the data or the decision boundaries between the different classes. The simplest plug-in model for data is to assume that each class of data is drawn from a single Gaussian. Under this assumption, the mean and variance of each class is estimated, and the labeling of test points is achieved through the MAP rule. If the data is known to contain more than one variate (e.g. Gaussian or uniform) distribution, the parameters of these distributions can be computed through algorithms such as Expectation-Maximization (EM) [12] algorithm. In order to operate the EM algorithm, the number of components in each class must be known in advance. This is not always simple, and an incorrect number might result in an erroneous solution. It is possible to alleviate this effect by estimating the number of components in the data using ML-II [29] or MDL [1]. Most classification algorithms do not attempt to find or even to approximate the Bayes decision region. Instead, these algorithms classify points by estimating decision regions or through estimation of densities. Arguably the simplest of these methods is the k-Nearest Neighbor classifier. Here the k points of the training data closest to the test point are found, and a label is given to the test point by a majority vote between the k points. This method is highly intuitive and attains a remarkably low classification errors, but it is computationally intensive and requires a large memory to store the training data. Another intuitive class of classification algorithms are decision trees. These algorithms solve the classification problem by repeatedly partitioning the input


An Introduction to Pattern Classification 9
Fig. 4. An example a decision tree. Classification of a new test point is achieved by moving from top to bottom along the branches of the tree, starting from the root node, until a terminal node (square) is reached
space, so as to build a tree whose nodes are as pure as possible (that is, they contain points of a single class). An example of a tree for classifying vertebrates into classes is shown in 4. Classification of a new test point is achieved by moving from top to bottom along the branches of the tree, starting from the root node, until a terminal node is reached. Decision trees are simple yet effective classification schemes for small datasets. Large datasets tend to result in complicated trees, which in turn require a large memory for storage. There is considerable literature on methods for simplifying and pruning decision trees (for example [30]). Another drawback of decision trees is their relative sensitivity to noise, especially if the size of the training data is small. The most commonly used algorithms for building decision trees, all developed by Quinlan, are CART [6], ID3 [42], and C4.5 [43]. An important approach to classification is through estimation of the density of data for each of the classes and classifying test points according to the maximum posterior probability. A useful algorithm for density estimation is the Parzen windows estimation[39]. Parzen windows estimate the probability of a point in the input space by weighing training points using a Gaussian window function (the farther a training sample is from the test sample, the lower its weight). This method is, however, expensive both computationally and memory wise. Furthermore, many training points are required for correct estimation of the class densities. Another approach for classification is to optimize a functional mapping from input patterns to output labels so that the training error will be as small as possible. If, for example, we assume a linear mapping (i.e. that the classifier takes the form of a weighted sum of the input patterns), it is possible to find a closed-form solution to the optimization (under a least-squares criterion) through the Moore-Penrose pseudo-inverse. Suppose the training patterns are placed in


10 E. Yom-Tov
a matrix of size N × D where D is the input dimension and N the number of examples, and that the corresponding labels are placed in a N × 1 vector T . We wish to find a weight vector w so that:
P ·w=T
The least-squares (LS) solution to this problem is:
w = (P T · P )−1 · P T · T
Assuming that the labels of the data are either −1 or +1, the labeling of a new test point x will be:
tˆ = sign (wT · x) =
{ +1 if wT · x > 0 −1 if wT · x < 0
LS is extremely efficient in both memory requirement and computational effort, but it is usually too simplistic a model to obtain sufficiently good results for the data. The optimization approach to pattern classification has been utilized in numerous other algorithms. An interesting example is the use of Genetic Programming (GP) for classification. Genetic algorithms are computational models inspired by evolution [55]. As such, they encode potential solutions to an optimization problem as a chromosome-like data structure and apply recombination operators on these structures. These recombination operators are designed so as to gradually improve the solutions, much like evolution improves individuals in a population. In genetic programming the encoded solution is a function, and the goal is to search in function space for a mapping of inputs to labels that will reduce the training error. GP can sometimes find a very good solution with both a low error and small computational and memory requirements, but there is no proof that it will converge (At all or to a good solution) and thus it is not a popular algorithm. Perhaps one of the commonly used approaches to classification that solves an optimization problem are Neural Networks (NN). Neural networks (suggested first by Alan Turing [51]) are a computational model inspired by the connectivity of neurons in animate nervous systems. A further boost to their popularity came with the proof that they can approximate any function mapping via the Universal Approximation Theorem [22]. A simple scheme for a neural network is shown in 5. Each circle denotes a computational element referred to as a neuron. A neuron computes a weighted sum of its inputs, and possibly performs a nonlinear function on this sum. If certain classes of nonlinear functions are used, the function computed by the network can approximate any function (specifically a mapping from the training patterns to the training targets), provided enough neurons exist in the network. Common nonlinear functions are the sign function and the hyperbolic tangent. The architecture of neural networks is not limited to the feed-forward structure shown in Figure 5. Many other structures have been suggested, such as recurrent NN, where the output is fed back as an input to the net, networks


An Introduction to Pattern Classification 11
Fig. 5. A schematic diagram of a neural network. Each circle in the hidden and output layer is a computational element known as a neuron
with multiple outputs, and networks where each of the neurons activates only if the input pattern is in a certain region of the input space (an example of which are radial-basis function (RBF) networks). If a single neuron exists in a network, it is usually referred to as a perceptron. Perceptrons find a linear separating hyperplane and proof can be given to show that it will converge to a solution, if one exists. There are many algorithms for training (i.e. finding the weight vector for the perceptron): Batch and stochastic, on-line and off-line, with and without memory [15]. The perceptron is a good choice for an on-line linear classifier. It shares the same pros and cons as the LS classifier, with the additional drawback that it might not converge if no linear separation exists. However, for off-line applications it is usually simpler to use the LS algorithm. Multiple-layered NNs are far more difficult to train. Indeed this was a major obstacle in the development of NNs until an efficient algorithm for training was developed. This algorithm is known as the backpropagation algorithm, socalled because the errors that are the driving force in the training (if there is no error, there is no need to change the weights of the NN) are propagated from the output layer, through the hidden layers, to the input layer. This algorithm, whether in batch or stochastic mode, enables the network to be trained according to the need. Further advancement was attained through second-order methods for training, which achieve faster convergence. Among these we note the conjugate-gradient descent (CGD) algorithm [36] and Quickprop [18], both of which significantly accelerate network training. NNs have significant advantages in memory requirements and classification speed, and have shown excellent results on real-world problems [26]. Nevertheless, they suffer from major drawbacks. Among these are the difficulty in deciding


12 E. Yom-Tov
Algorithm Classification Computational Memory Difficulty to On-line Insight from error cost requirements Implement the classifier
Expectation- Low Medium Small Low No Yes Maximization (EM) Nearest Medium- High High Low No No neighbor low Decision Medium Medium Medium Low No Yes trees Parzen Low High High Low No No windows Linear least High Low Low Low Yes Yes squares (LS) Genetic Medium- Medium Low Low No Some programming low Neural Low Medium Low High Yes No Networks Ada-Boost Low Medium Medium Medium No No Support vector Low Medium Low Medium Yes Some machines (SVM)
on network architecture as well as several other network parameters, and that the resulting classifier is a ”black box”, where it is difficult to understand why the network training resulted in a certain set of weights. Finally, contrary to other classification algorithms, efficient training of NNs is also dependent on several ”tricks of the trade” such as normalizing the inputs, setting the initial weight values, etc. This makes it difficult for the novice to use NN effectively. An interesting and extremely useful approach to classification is to use simple classifiers as building blocks for constructing complicated decision regions. This approach is known as Boosting. Schematically, we first train a simple (or weak) classifier for the data. Then, those points of the train-set that are incorrectly classified are located and another weak classifier is trained so as to improve the classification of these incorrectly labeled points. This process is repeated until a sufficiently low training error is reached. The training of the weak classifiers can be performed by either drawing points from the training data with a probability inversely proportional to the distance of the points from the decision region or by selecting a cluster of the incorrectly trained points. In the first case, the algorithm is known as AdaBoost [19], which is the most popular boosting algorithm. In the second case, the algorithm is the Local Boosting algorithm [31]. Boosting has been shown to give very results on many data-sets. Its computational cost is reasonably low, as are its memory requirements. Thus, boosting is one of the most useful classification algorithms. The last type of classification algorithm we discuss in this introduction is the Support Vector Machine (SVM) classifier. This classifier is the result of seminal work by Boser, Guyon, and Vapnik [5] and later others. SVM draws on two main practical observations:
Table 1. Comparison of the reviewed classification algorithms


An Introduction to Pattern Classification 13
1. At a sufficiently high dimension, patterns are orthogonal to each other, and thus it is easier to find a separating hyperplane for data in a high dimension. 2. Not all patterns are necessary for finding a separating hyperplane. In fact, it is sufficient to use only those points that are near the boundary between groups for constructing the boundary.
An SVM classifier is a linear classifier which finds the hyperplane that separates the data with the largest margin(i.e. the distance between the hyperplane and the closest data point) possible, built after transforming the data into a high dimension (known as the feature space). Let us begin with the second part of the process - the separating hyperplane. A linear separating hyperplane is a decision function in the form
f (x) = sign (〈w, x〉 + b)
where x is the input pattern, w is the weight vector, b is a bias term, and 〈·, ·〉 denotes the inner product. If the data is to be classified correctly, this hyperplane should ensure that
yi · (〈w, xi〉 + b) > 0 f or all i = 1, . . . , m
assuming that y ∈ {−1, +1}. There is one separating hyperplane that maximizes the margin separating the data, which is attractive since this hyperplane gives good generalization
performance[46]. In order to find this hyperplane we need to minimize ‖w‖2. Thus the SVM problem can be written as:
minimize 1
2 ‖w‖2
subject to yi · (〈w, xi〉 + b) ≥ 1 f or all i = 1, . . . , m
(The right hand side of the bottom equation was changed to one instead of zero otherwise the minimum of w would be the trivial solution. In fact, any positive number would suffice) This constrained minimization problem is solved using Lagrange multipliers, which results in a dual optimization problem:
maximize W (α) = ∑m
i=1 αi − 1
2
∑m
i,j=1 αiαj yiyj 〈xi, xj 〉
s.t. αi ≥ 0, ∑m
i=1 αiyi = 0
The coefficients of a corresponding to input patterns that are not used for construction of the class boundary should be zero. The remaining coefficients are known as the support vectors. The above optimization problem can be solved in several ways, for example: Through a perceptron, which finds the largest margin hyperplane separating the data[15]; By use of quadratic programming optimization algorithms, which solve the optimization problem [15]; or through other efficient optimization algorithms such as the sequential minimal optimization (SMO) algorithm [40].


14 E. Yom-Tov
Classification of new samples is performed using the equation
y = sign
(m
∑
i=1
yiαi 〈x, xi〉 + b
)
As noted above, it is useful to map the input patterns into a high dimensional space. This could be done by mapping each input pattern through a function so that x ̃ = φ (x). However, in practice, if the function maps the data into a very high dimension, it would be problematic to compute and to store the results of the mapping. If the mapping is done into an infinite dimensional space this would be impossible. Fortunately, this problem can be avoided through a substitute known as the kernel trick[5]. Note that in the optimization problem above, the input patterns only appear in an inner product of pairs of patterns. Thus, instead of mapping each sample to a higher dimension and then performing the inner product, it is possible (for certain classes of kernels) to first compute the inner product between patterns and only then compute the mapping on a scalar. Thus, in the equations above we now replace the inner products 〈x, x′〉 with k(x, x′) where k is the kernel function. The kernel function used for mapping should conform to conditions known as the Mercer conditions [22]. Examples of such functions are polynomials, radial basis functions (Gaussian functions), and hyperbolic tangents. SVMs have been studied extensively[46]. They have been extended in many directions. Some notable examples include:
1. Cases where the optimal hyperplane does not exist, through the introduction of a penalty term which allows some training patterns to be incorrectly classified [10]. 2. Single class learning (outlier detection) [45]. 3. Online learning [17]. 4. Feature selection [20, 54]. 5. Incremental classification so as to reduce the computational cost of SVMs[7].
It is difficult to find thorough comparative studies of classification algorithms. Several such studies (for example [33, 46]) point to the conclusion that a few classification algorithms, namely SVM, AdaBoost, Kernel Fisher discriminant, and Neural networks achieve similar results with regard to error rates. Lately, the Relevance Vector Machine [49], a kernel method stemming from Bayesian learning, has also joined this group of algorithms. However, these algorithm differ greatly in the other factors outlined at the beginning of this chapter. Finally, we note several practical points of importance which one should take into account when designing classifiers:
1. In order to reduce the likelihood of over-fitting the classifier to the training data, the ratio of the number of training examples to the number of features should be at least 10:1. For the same reason the ratio of the number of training examples to the number of unknown parameters should be at least 10:1. 2. It is important to use proper error-estimation methods (see next section), especially when selecting parameters for the classifier.


An Introduction to Pattern Classification 15
3. Some algorithms require the input features to be scaled to similar ranges. This is especially evident those that use some kind of a weighted average of the inputs such as neural networks, SVM, etc. 4. There is no single best classification algorithm!
Thus far we have implicitly only discussed problems where there are two classes of data, i.e. the labels can take one of two values. In many applications it is necessary to distinguish between more than two classes. Some classifiers are suitable for such applications with only minor changes. Examples of such classifiers are the LS classifier, the Nearest Neighbor classifier, and decision trees. Neural networks require a minor modification to work with multiclass problems. Instead of having a single output neuron there should be as many output neurons as labels. Each of the output neurons is trained to respond to data of one class, and the strongest activated neuron is taken to be the predicted class label. SVMs have been modified to solve multiclass problems through a slight change in the objective function to the minimization procedure [46]. Not all classifiers are readily modifiable to multiclass applications. The strategy for solution of such cases is to train several classifiers and add a gating network that decides on the predicted label based on the output of these classifiers. The simplest example of such a strategy is to train as many classifiers as classes where each classifier is trained to respond to one class of data. The gating network then outputs the number of the classifier that responded to a given input. This type of solution is called a one-against-all solution. The main drawbacks of this solution are that it is heuristic, that the classifiers are solving problems that are very different in their difficulty, and that, if the output of the classifiers is binary, there might be more than one possible class for each output. A variation on the one-against-all solution is to train classifiers to distinguish between each pair of classes[21]. This solution has the advantage that the individual classifiers are trained on smaller datasets. The main drawback of this solution is the large number of classifiers that are needed to be trained ((N − 1) N/2). An elegant solution to multiclass problems was suggested in [13]. That article showed the parallel between multiclass problems and the study of errorcorrecting codes for communication applications. In the latter, bits of data are sent over a noisy channel. At the receiver, the data is reconstructed through thresholding of the received bit. In order to reduce the probability of error, additional bits of data are sent to the receiver. These bits are a function of the data bits, and are designed so that they can correct errors that occurred during transmission (if only a small numbers of error appeared). The functions by which these extra bits are computed are known as error-correcting codes. The application of error-correcting codes to multiclass problems is straightforward. Classifiers are trained according to an error-correcting code, and their output to test patterns is interpreted as though they were the received bits of information. This solution requires the addition of classifiers according to the specific error-correcting code (for example, the simple Hamming code requires 2N −N −1


16 E. Yom-Tov
classifiers for N classes of data), but if a few of the classifiers are in error, the total output would still be correct. In practice, the one-against-all method is usually not much worse than the more sophisticated approaches described above. When selecting the solution, one should consider training times and the available memory, in addition to the overall accuracy of the system. The last topic we address in the context of classification is one-class learning. This is an interesting philosophical subject, as well as an extremely practical one. We usually learn by observing different examples (Car vs. Plane, Cat vs. Dog, etc). Is it possible to learn by observing examples of only a single class? (e.g. would a child who only saw cats be able to say that a dog, first seen, is not a cat?). In the framework of classification, the object of single-class learning is to distinguish between objects of one kind (the target object) and all other possible objects)[48], where the latter are not seen during training. Single-class learning has been applied to problems such as image retrieval[9], typist identification[38], and character recognition[46]. The idea behind single-class learning is to identify areas where the data representing the target object is of high density. If a test sample appears close to (or inside) such a high-density area, it would be classified as a target object. If it is in a low-density area of the input space, it would be classified as a different object. The simplest type of single-class algorithm describes the data by a single Gaussian (with a mean and a covariance matrix). The probability estimate that a test sample is drawn from this Gaussian is computed, and this measure is reported to the user. A more sophisticated measure is the Parzen windows estimation of density, or through the use of a multi-Gaussian model, with EM used for training. Neural networks have also been used for this task training the network to form closed decision surfaces, and labeling points outside these surfaces as non-target-class data [34]. More recently, single-class SVMs were developed[47]. These are a modification of the two-class SVM described above, with the SVM attempting to enclose the data with a sphere in feature space. Any data falling outside this sphere is deemed not to be of the target class.
6 Error Estimation Techniques
As noted above, the most important factor in the performance of a classifier is its error rate. This measure is important for assessing if the classifier is useful, for tuning its parameters[35], and in order to compare it to other classifiers. It is often difficult to estimate the error rate of a given classifier even if there is full knowledge of the underlying distribution is available. In practice, it is desirable to estimate the error rate given a sample data set. This problem is aggravated if the dataset is small[23]. If the whole dataset is used to both training the classifier and for estimating its error, there is a serious danger of over-fitting the classifier to the training data (in the extreme case,


An Introduction to Pattern Classification 17
consider a 1-Nearest Neighbor classifier). Therefore, the data should be split into training data and testing data. There are three main methods for splitting the data:
1. Resubstitution: Here the whole dataset is used for both training and testing. As noted above this method is extremely optimistic. In practice, for small datasets error estimation obtained using this method is erroneous. 2. Holdout: Part of the data (for example, 80%) is used for training, and the remaining is used for testing. This method is pessimistically biased, and different splits of the data will result of different error rates. 3. Cross-validation: The data is divided into N equal sub-sets. The data is trained using (N-1) sub-sets, and tested on the N-th subset. The process is repeated until each of the N sub-sets is used as a test set. The error rate is the average of the N resulting errors. The resulting error rate has a lower bias than the holdout method. An extreme form of cross-validation is known as leave-one-out, where the sub-sets contain a single point. The estimate of leave-one-out is unbiased but it has a large variance and is computationally expensive to compute.
After computing the error rate of a classifier, we have an estimation of how well the algorithm will perform on new data. The algorithm should then be trained using the whole dataset, in preparation of new data. Although it is desirable to use the error rate as a way to compare the performance of different classification algorithms, this is (surprisingly) still an open issue for future research. Some researchers have used the Wilcoxon signed-rank tests for such comparison, although the underlying assumptions of this test are violated when it is used for such a comparison[14].
7 Summary
The purpose of pattern classification algorithms is to automatically construct methods for distinguishing between different exemplars, based on their differentiating patterns. The goal of completely automated learning algorithms is yet to be attained. Most pattern classification algorithms need some manual parameter tuning to achieve the best possible performance. More importantly, in most practical applications, domain knowledge remains crucial for the successful operation of Pattern Classification algorithms. Pattern classification has been an object of research for several decades. In the past decade this research resulted in a multitude of new algorithms, better theoretic understanding of previous ideas, as well as many successful practical applications. Pattern classification remains an exciting domain for theoretic research, as well as for application of its’ tools to practical problems. Some of the problems yet to be solved were outlined in previous paragraphs, and a more detailed list appears in [16].


18 E. Yom-Tov
References
1. A.R. Barron and T.M. Cover. Minimum complexity density estimation. IEEE Transactions on information theory, IT-37(4):1034–1054, 1991.
2. A. Ben-Hur, D. Horn, H.T. Siegelmann, and V. Vapnik. Support vector clustering. Journal of Machine Learning Research, 2:125–137, 2001. 3. J.C. Bezdek. Fuzzy mathematics in pattern classification. PhD thesis, Cornell University, Applied mathematics center, Ithaka, NY, 1973. 4. A.L. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artificial Intelligence, 97:245–271, 1997. 5. B.E. Boser, I.M. Guyon, and V. Vapnik. A training algorithm for optimal margin classifiers. In D. Haussler, editor, Proceedings of the 5th annual ACM workshop on computational learning theory, pages 144–152, Pittsburgh, PA, USA, 1992. ACM Press. 6. L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. Classification and regression trees. Chapman and Hall, New York, USA, 1993. 7. C.J.C. Burges and B. Sch ̈olkopf. Improving the accuracy and speed of support vector machines. In M.C. Mozer, M.I. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9, page 375, Cambridge, MA, USA, 1997. The MIT Press. 8. J-F Cardoso. Blind signal separation: Statistical principles. Proceedings of the IEEE, 9(10):2009–2025, 1998. 9. Y. Chen, X.S. Zhou, and T.S. Huang. One-class svm for learning in image retrieval. In Proceedings of the international conference on image processing, volume 1, pages 34–37. IEEE, 2001. 10. C. Cortes and V. Vapnik. Support vector networks. Machine learning, 20:273–297, 1995. 11. N. Cristianini, J. Shawe-Taylor, and J. Kandola. Spectral kernel methods for clustering. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems, volume 14, pages 649–655, Cambridge, MA, USA, 2002. The MIT Press. 12. A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum-likelihood from incomplete data via the em algorithm (with discussion). Journal of the royal statistical society, Series B, 39:1–38, 1977. 13. T.G. Dietrich and G. Bakiri. Solving multi-class learning problems via errorcorrecting output codes. Journal of artificial intelligence research, 2:263–286, 1995. 14. T.G. Dietterich. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7):1895–1923, 1998. 15. R.O. Duda, P.E. Hart, and D.G. Stork. Pattern classification. John Wiley and Sons, Inc, New-York, USA, 2001. 16. R.P.W. Duin, F. Roli, and D. de Ridder. A note on core research issues for statistical pattern recognition. Pattern recognition letters, 23:493–499, 2002. 17. Y. Engel, S. Mannor, and R. Meir. The kernel recursive least squares algorithm. Technion CCIT Report number 446, Technion, Haifa, Israel, 2003. 18. S.E. Fahlman. Faster-learning variations on back-propagation: An empirical study. In T.J. Sejnowski, G.E. Hinton, and D.S. Touretzky, editors, Connectionist Models Summer School, San Mateo, CA, USA, 1988. Morgan Kaufmann. 19. Y. Freund and R.E. Schapire. A decision-theoretic generalization of online learning and an application to boosting. Journal of Computer and System Sciences, 55:119139, 1995.


An Introduction to Pattern Classification 19
20. I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classification using support vector machines. Machine learning, 46(1-3):389–422, 2002. 21. T.J. Hastie and R.J. Tibshirani. Classification by pairwise coupling. In M.I. Jordan, M.J. Kearns, and S.A. Solla, editors, Advances in Neural Information Processing Systems, volume 10, Cambridge, MA, USA, 1998. The MIT Press.
22. S. Haykin. Neural Networks: A comprehensive foundation, 2nd Ed. Prentice-Hall, 1999. 23. A.K. Jain, R.P.W. Duin, and J. Mao. Statistical pattern recognition: A review. IEEE Transactions on pattern analysis and machine intelligence, 22(1):4–37, 1999. 24. T. Kohonen. Self-organization and associative memory. Biological Cybernetics, 43(1):59–69, 1982. 25. D. Koller and M. Sahami. Toward optimal feature selection. In Proceedings of the 13th International Conference on Machine Learning, pages 284–292, Bari, Italy, 1996. Morgan Kaufmann. 26. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of IEEE, 86(11):2278–2324, 1998. 27. D.D. Lewis. Feature selection and feature extraction for text categorization. In Proceedings of speech and natural language workshop, pages 212–217, San Francisco, USA, 1992. Morgan Kaufmann. 28. S.P. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, IT-2:129–137, 1982. 29. D.J. MacKay. Bayesian model comparison and backprop nets. In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, Neural Networks for Signal Processing 4, pages 839–846, San Mateo, CA, USA, 1992. Morgan Kaufmann. 30. M. Mehta, J. Rissanen, and R. Agrawal. Mdl-based decision tree pruning. In Proceedings of the first international conference on knowledge discovery and data mining, pages 216–221, 1995. 31. R. Meir, R. El-Yaniv, and S. Ben-David. Localized boosting. In Proceedings of the 13th Annual Conference on Computer Learning Theory, pages 190–199, San Francisco, 2000. Morgan Kaufmann. 32. S. Mika, G. R ̈atsch, J. Weston, B. Sch ̈olkopf, and K.-R. M ̈uller. Fisher discriminant analysis with kernels. In Y.-H. Hu, J. Larsen, E. Wilson, and S. Douglas, editors, Neural Networks for Signal Processing IX, pages 41–48. IEEE, 1999.
33. S. Mika, B. Sch ̈olkopf, A. J. Smola, K.-R. M ̈uller, M. Scholz, and G. R ̈atsch. Kernel pca and de–noising in feature spaces. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in Neural Information Processing Systems 11, Cambridge, MA, USA, 1999. MIT Press. 34. M.R. Moya, M.W. Koch, and L.D. Hostetler. One-class classifier networks for target recognition applications. In Proceedings of the world congress on neural networks, Portland, OR, USA, 1993. International neural networks society. 35. K.-R. M ̈uller, S. Mika, G. R ̈atsch, K. Tsuda, and B. Sch ̈olkopf. An introduction to kernel-based learning algorithms,” ieee transactions on neural networks. IEEE Transactions on Neural Networks, 12(2):181–201, 2001.
36. M. Muller. A scaled conjugate gradient algorithm for fast supervised learning. Neural Networks, 6:525–533, 1993. 37. A.Y. Ng, M.I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems, volume 14, pages 849–856, Cambridge, MA, USA, 2002. The MIT Press.


20 E. Yom-Tov
38. M. Nisenson, I. Yariv, R. El-Yaniv, and R. Meir. Towards behaviometric security systems: Learning to identify a typist. In Proceedings of the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD), 2003.
39. E. Parzen. On estimation of a probability density function and mode. Annals of mathematical statistics, 33(3):1065–1076, 1962.
40. J. Platt. Fast training of support vector machines using sequential minimal optimization. In A.J. Smola, P.L. Bartlett, B. Sch ̈olkopf, and D. Schuurmans, editors, Advances in kernel methods - Support vector learning, pages 185–208, Cambridge, MA, USA, 1999. MIT Press. 41. P. Pudil, J. Novovicova, and J. Kittler. Floating search methods in feature selection. Pattern recognition letters, 15(11):1119–1125, 1994. 42. J.R. Quinlan. Learning efficient classification procedures and their application to chess end games. In R.S. Michalski, J.G. Carbonell, and T.M. Mitchell, editors, Machine learning: An artificial intelligence approach, pages 463–482, San Francisco, CA, USA, 1983. Morgan Kaufmann. 43. J.R. Quinlan. C4.5: Programs for machine learning. Morgan Kaufmann, San Francisco, CA, USA, 1993. 44. D.E. Rumelhart and D. Zipser. Feature discovery by competitive learning. Parallel Distributed Processing, pages 151–193, 1986.
45. B. Sch ̈olkopf, J. Platt, J. Share-Taylor, A.J. Smola, and R.C. Williamson. Estimating the support of a high-dimensional distribution. TR87, Microsoft Research, Redmond, WA, USA, 1999.
46. B. Sch ̈olkopf and A.J. Smola. Leaning with kernels: Support vector machines, regularization, optimization, and beyond. MIT Press, Cambridge, MA, USA, 2002. 47. D.M.J. Tax and R.P.W. Duin. Data domain description by support vectors. In M. Verleysen, editor, Proceedings of the European symposium on artificial neural networks, pages 251–256, Brussel, 1999. 48. D.M.J. Tax and R.P.W. Duin. Combining one-class classifiers. In J. Kittler and F. Roli, editors, Proceedings of the Second International Workshop on Multiple Classifier Systems, MCS 2001, Heidelberg, Germany, 2001. Springer-Verlag. 49. M. Tipping. The relevance vector machine. Journal of machine learning research, 1:211–244, 2001. 50. G.V. Trunk. A problem of dimensionality: A simple example. IEEE Transactions on pattern analysis and machine intelligence, 1(3):306–307, 1979.
51. A.M. Turing. Intelligent machinery. In D.C. Ince, editor, Collected works of A.M. Turing: Mechanical Intelligence, Amsterdam, The Netherlands, 1992. Elsevier Science Publishers. 52. V.N. Vapnik. Personal communication, 2003.
53. W. Watanabe. Pattern recognition: Human and mechanical. Wiley, 1985.
54. J. Weston, A. Elisseeff, B. Sch ̈olkopf, and M. Tipping. Use of the zero-norm with linear models and kernel methods. Journal of machine learning research, 3:14391461, 2003. 55. D. Whitley. A genetic algorithm tutorial. Statistics and Computing, 4(2):65–85, 6 1994. 56. E. Yom-Tov and G.F. Inbar. Feature selection for the classification of movements from single movement-related potentials. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 10(3):170–177, 2001.


Some Notes on Applied Mathematics for
Machine Learning
Christopher J.C. Burges
Microsoft Research, One Microsoft Way, Redmond, WA 98052-6399, USA cburges@microsoft.com http://research.microsoft.com/~cburges
Abstract. This chapter describes Lagrange multipliers and some selected subtopics from matrix analysis from a machine learning perspective. The goal is to give a detailed description of a number of mathematical constructions that are widely used in applied machine learning.
1 Introduction
The topics discussed in this chapter are ones that I felt are often assumed in applied machine learning (and elsewhere), but that are seldom explained in detail. This work is aimed at the student who’s taken some coursework in linear methods and analysis, but who’d like to see some of the tricks used by researchers discussed in a little more detail. The mathematics described here is a small fraction of that used in machine learning in general (a treatment of machine learning theory would include the mathematics underlying generalization error bounds, for example)1, although it’s a largely self-contained selection, in that derived results are often used downstream. I include two kinds of homework, ‘exercises’ and ‘puzzles’. Exercises start out easy, and are otherwise as you’d expect; the puzzles are exercises with an added dose of mildly Machiavellian mischief. Notation: vectors appear in bold font, and vector components and matrices in normal font, so that for example v(a)
i denotes the i’th component of the
a’th vector v(a). The symbol A ≻ 0 ( ) means that the matrix A is positive (semi)definite. The transpose of the matrix A is denoted AT , while that of the vector x is denoted x′.
2 Lagrange Multipliers
Lagrange multipliers are a mathematical incarnation of one of the pillars of diplomacy (see the historical notes at the end of this section): sometimes an indirect approach will work beautifully when the direct approach fails.
1 My original lectures also contained material on functional analysis and convex optimization, which is not included here.
O. Bousquet et al. (Eds.): Machine Learning 2003, LNAI 3176, pp. 21–40, 2004. c© Springer-Verlag Berlin Heidelberg 2004


22 C.J.C. Burges
2.1 One Equality Constraint
Suppose that you wish to minimize some function f (x), x ∈ Rd, subject to the constraint c(x) = 0. A direct approach is to find a parameterization of the constraint such that f , expressed in terms of those parameters, becomes an unconstrained function. For example, if c(x) = x′Ax − 1, x ∈ Rd, and if A ≻ 0, you could rotate to a coordinate system and rescale to diagonalize the constraints to the form y′y = 1, and then substitute with a parameterization that encodes the constraint that y lives on the (d − 1)-sphere, for example
y1 = sin θ1 sin θ2 · · · sin θd−2 sin θd−1 y2 = sin θ1 sin θ2 · · · sin θd−2 cos θd−1 y3 = sin θ1 sin θ2 · · · cos θd−2 ···
Unfortunately, for general constraints (for example, when c is a general polynomial in the d variables) this is not possible, and even when it is, the above example shows that things can get complicated quickly. The geometry of the general situation is shown schematically in Figure 1.
Fig. 1. At the constrained optimum, the gradient of the constraint must be parallel to that of the function
On the left, the gradient of the constraint is not parallel to that of the function; it’s therefore possible to move along the constraint surface (thick arrow) so as to further reduce f . On the right, the two gradients are parallel, and any motion along c(x) = 0 will increase f , or leave it unchanged. Hence, at the solution, we must have ∇f = λ∇c for some constant λ; λ is called an (undetermined) Lagrange multiplier, where ‘undetermined’ arises from the fact that for some problems, the value of λ itself need never be computed.
2.2 Multiple Equality Constraints
How does this extend to multiple equality constraints, ci(x) = 0, i = 1, . . . , n? Let gi ≡ ∇ci. At any solution x∗, it must be true that the gradient of f has no components that are perpendicular to all of the gi, because otherwise you could


Some Notes on Applied Mathematics for Machine Learning 23
move x∗ a little in that direction (or in the opposite direction) to increase (decrease) f without changing any of the ci, i.e. without violating any constraints. Hence for multiple equality constraints, it must be true that at the solution x∗, the space spanned by the gi contains the vector ∇f , i.e. there are some constants λi such that ∇f (x∗) = ∑
i λigi(x∗). Note that this is not sufficient, however 
we also need to impose that the solution is on the correct constraint surface (i.e. ci = 0 ∀i). A neat way to encapsulate this is to introduce the Lagrangian L ≡ f (x) − ∑
i λici(x), whose gradient with respect to the x, and with respect
to all the λi, vanishes at the solution.
Puzzle 1: A single constraint gave us one Lagrangian; more constraints must give us more information about the solution; so why don’t multiple constraints give us multiple Lagrangians?
Exercise 1. Suppose you are given a parallelogram whose side lengths you can choose but whose perimeter is fixed. What shaped parallelogram gives the largest area? (This is a case where the Lagrange multiplier can remain undetermined.) Now, your enterprising uncle has a business proposition: to provide cheap storage in floating containers that are moored at sea. He wants to build a given storage facility out of a fixed area of sheet metal which he can shape as necessary. He wants to keep construction simple and so desires that the facility be a closed parallelepiped (it has to be protected from the rain and from the occasional wave). What dimensions should you choose in order to maximize the weight that can be stored without sinking?
Exercise 2. Prove that the distance between two points that are constrained to lie on the n-sphere is extremized when they are either antipodal, or equal.
2.3 Inequality Constraints
Suppose that instead of the constraint c(x) = 0 we have the single constraint c(x) ≤ 0. Now the entire region labeled c(x) < 0 in Figure 1 has become feasible. At the solution, if the constraint is active (c(x) = 0), we again must have that ∇f is parallel to ∇c, by the same argument. In fact we have a stronger condition, namely that if the Lagrangian is written L = f +λc, then since we are minimizing f , we must have λ ≥ 0, since the two gradients must point in opposite directions (otherwise a move away from the surface c = 0 and into the feasible region would further reduce f ). Thus for an inequality constraint, the sign of λ matters, and so here λ ≥ 0 itself becomes a constraint (it’s useful to remember that if you’re minimizing, and you write your Lagrangian with the multiplier appearing with a positive coefficient, then the constraint is λ ≥ 0). If the constraint is not active, then at the solution ∇f (x∗) = 0, and if ∇c(x∗) = 0, then in order that ∇L(x∗) = 0 we must set λ = 0 (and if in fact if ∇c(x∗) = 0, we can still set λ = 0). Thus in either case (active or inactive), we can find the solution by requiring that the gradients of the Lagrangian vanish, and we also have λc(x∗) = 0. This latter condition is one of the important Karush-Kuhn-Tucker conditions of convex optimization theory [15, 4], and can facilitate the search for the solution, as the next exercise shows.


24 C.J.C. Burges
For multiple inequality constraints, again at the solution ∇f must lie in the space spanned by the ∇ci, and again if the Lagrangian is L = f + ∑
i λici,
then we must in addition have λi ≥ 0 ∀i (since otherwise f could be reduced by moving into the feasible region); and for inactive constraints, again we (can, usually must, and so might as well) set λi = 0. Thus the above KKT condition generalizes to λici(x∗) = 0 ∀i. Finally, a simple and often useful trick is to solve ignoring one or more of the constraints, and then check that the solution satisfies those constraints, in which case you have solved the problem; we’ll call this the free constraint gambit below.
Exercise 3. Find the x ∈ Rd that minimizes ∑
i xi2 subject to ∑
i xi = 1. Find
the x ∈ Rd that maximizes ∑
i xi2 subject to ∑
i xi = 1 and xi ≥ 0 (hint: use
λix∗i = 0).
2.4 Cost Benefit Curves
Here’s an example from channel coding. Suppose that you are in charge of four fiber optic communications systems. As you pump more bits down a given channel, the error rate increases for that channel, but this behavior is slightly different for each channel. Figure 2 show a graph of the bit rate for each channel versus the ‘distortion’ (error rate). Your goal is to send the maximum possible number of bits per second at a given, fixed total distortion rate D. Let Di be the number
of errored bits sent down the i’th channel. Given a particular error rate, we’d like to find the maximum overall bit rate; that is, we must maximize the total rate R≡∑
i=1 Ri subject to the constraint D = ∑
i=1 Di. Introducing a Lagrange
multiplier λ, we wish to maximize the objective function
L=
4
∑
i=1
Ri(Di) + λ(D −
4
∑
i=1
Di) (1)
Setting ∂L/∂Di = 0 gives ∂Ri/∂Di = λ, that is, each fiber should be operated at a point on its rate/distortion curve such that its slope is the same for all fibers. Thus we’ve found the general rule for resource allocation, for benefit/cost
Fig. 2. Total bit rate versus distortion for each system


Some Notes on Applied Mathematics for Machine Learning 25
curves like those shown2 in Figure 2: whatever operating point is chosen for each system, in order to maximize the benefit at a given cost, the slope of the graph at that point should be the same for each curve. For the example shown, the slope of each graph decreases monotonically, and we can start by choosing a single large value of the slope λ for all curves, and decrease it until the condition
∑
i=1 Di = D is met, so in general for m fibers, an m dimensional search prob
lem has been reduced to a one dimensional search problem. We can get the same result informally as follows: suppose you had just two fibers, and were at an operating point where the slope s1 of the rate/distortion graph for fiber 1 was greater than the slope s2 for fiber 2. Suppose you then adjusted things so that fiber 1 sent one more errored bit every second, and fiber 2 sent one fewer. The extra number of bits you can now send down fiber 1 more than offsets the fewer number of bits you must send down fiber 2. This will hold whenever the slopes are different. For an arbitrary number of fibers, we can apply this argument to any pair of fibers, so the optimal point is for all fibers to be operating at the same slope.
Puzzle 2: Suppose that instead of fibers, you have four factories making widgets, that the y-axis in Figure 2 represents the total cost for making ni widgets, and that the x-axis represents the number ni of widgets made by the i’th factory. The curves have the same shape (they drop off at larger ni due to the economies of scale). Does the above argument mean that, to produce a total, fixed number of widgets, in order to minimize the cost, each factory should be operated at the same slope on its curve as all the other factories?
2.5 An Isoperimetric Problem
Isoperimetric problems - problems for which a quantity is extremized while a perimeter is held fixed - were considered in ancient times, but serious work on them began only towards the end of the seventeenth century, with a minor battle between the Bernoulli brothers [14]. It is a fitting example for us, since the general isoperimetric problem had been discussed for fifty years before Lagrange solved it in his first venture into mathematics [1], and it provides an introduction to functional derivatives, which we’ll need. Let’s consider a classic isoperimetric problem: to find the plane figure with maximum area, given fixed perimeter. Consider a curve with fixed endpoints {x = 0, y = 0} and {x = 1, y = 0}, and fixed length ρ. We will assume that the curve defines a function, that is, that for a given x ∈ [0, 1], there corresponds just one y. We wish to maximize the area between the curve and the x axis, A = ∫ 1
0 ydx, subject to the constraint that
the length, ρ = ∫ 1
0
√1 + y′2dx, is fixed (here, prime denotes differentiation with respect to x). The Lagrangian is therefore
L=
∫1
0
ydx + λ
(∫ 1
0
√1 + y′2dx − ρ
)
(2)
2 This seemingly innocuous statement is actually a hint for the puzzle that follows.


26 C.J.C. Burges
Two new properties of the problem appear here: first, integrals appear in the Lagrangian, and second, we are looking for a solution which is a function, not a point. To solve this we will use the calculus of variations, introduced by Lagrange and Euler. Denote a small variation of a function3 f by δf : that is, replace f (x) by f (x) + δf (x) everywhere, where δf is chosen to vanish at the boundaries, that is, δf (0) = δf (1) = 0 (note that δf is also a function of x). Here, y is the variable function, so the change in L is
δL =
∫1
0
δydx + λ
∫1
0
(1 + y′2)−1/2y′δy′dx
By using the facts that δy′ = δ dy
dx = d
dx δy and that the variation in y vanishes
at the endpoints, integrating by parts then gives:
δL =
∫1
0
(
1 − λy′′(1 + y′2)−3/2)
δydx
⇒ 1 −λy′′(1 + y′2)−3/2 ≡ 1 − λκ = 0
where κ is the local curvature, and where the second step results from our being able to choose δy arbitrarily on (0, 1), so the quantity multiplying δy in the integrand must vanish (imagine choosing δy to be zero everywhere except over an arbitrarily small interval around some point x ∈ [0, 1]). Since the only plane curves with constant curvature are the straight line and the arc of circle, we find the result (which holds even if the diameter of the circle is greater than one). Note that, as often happens in physical problems, λ here has a physical interpretation (as the inverse curvature); λ is always the ratio of the norms of ∇f and ∇c at the solution, and in this sense the size of λ measures the influence of the constraint on the solution.
2.6 Which Univariate Distribution has Maximum Entropy?
Here we use differential entropy, with the understanding that the bin width is sufficiently small that the usual sums can be approximated by integrals, but fixed, so that comparing the differential entropy of two distributions is equivalent to comparing their entropies. We wish to find the function f that minimizes
∫∞
−∞
f (x) log2 f (x)dx, x ∈ R (3)
subject to the four constraints
f (x) ≥ 0 ∀x,
∫∞
−∞
f (x) = 1,
∫∞
−∞
xf (x) = c1
∫∞
−∞
x2f (x) = c2
3 In fact Lagrange first suggested the use of the symbol δ to denote the variation of a whole function, rather than that at a point, in 1755 [14].


Some Notes on Applied Mathematics for Machine Learning 27
Note that the last two constraints, which specify the first and second moments, is equivalent to specifying the mean and variance. Our Lagrangian is therefore:
L=
∫∞
−∞
f (x) log2 f (x)dx + λ
(
1−
∫∞
−∞
f (x)
)
+ β1
(
c1 −
∫∞
−∞
xf (x)dx
)
+ β2
(
c2 −
∫∞
−∞
x2f (x)dx
)
where we’ll try the free constraint gambit and skip the positivity constraint. In this problem we again need the calculus of variations. In modern terms we use the functional derivative, which is just a shorthand for capturing the rules of the calculus of variations, one of which is:
δg(x)
δg(y) = δ(x − y) (4)
where the right hand side is the Dirac delta function. Taking the functional derivative of the Lagrangian with respect to f (y) and integrating with respect to x then gives log2 f (y) + log2(e) − λ − β1y − β2y2 = 0 (5)
which shows that f must have the functional form
f (y) = C exp(λ+β1y+β2y2) (6)
where C is a constant. The values for the Lagrange multipliers λ, β1 and β2 then follow from the three equality constraints above, giving the result that the Gaussian is the desired distribution. Finally, choosing C > 0 makes the result positive everywhere, so the free constraint gambit worked.
Puzzle 3: For a given event space, say with N possible outcomes, the maximum entropy is attained when pi = 1/N ∀i, that is, by the uniform distribution. That doesn’t look very Gaussian. What gives?
Exercise 4. What distribution maximizes the entropy for the class of univariate distributions whose argument is assumed to be positive, if only the mean is fixed? How about univariate distributions whose argument is arbitrary, but which have specified, finite support, and where no constraints are imposed on the mean or the variance?
Puzzle 4: The differential entropy for a uniform distribution with support in [−C, C] is
h(PU ) = −
∫C
−C
(1/2C) log2(1/2C)dx
= − log2(1/2C) (7)
This tends to ∞ as C → ∞. How should we interpret this? Find the variance for any fixed C, and show that the univariate Gaussian with that variance has differential entropy greater than h.


28 C.J.C. Burges
2.7 Maximum Entropy with Linear Constraints
Suppose that you have a discrete probability distribution Pi, ∑n
i Pi = 1, and
suppose further that the only information that you have about the distribution is that it must satisfy a set of linear constraints:
∑
i
αjiPi = Cj, j = 1, . . . , m (8)
The maximum entropy approach (see [5], for example) posits that, subject to the known constraints, our uncertainty about the set of events described by the distribution should be as large as possible, or specifically, that the mean number of bits required to describe an event generated from the constrained probability distribution be as large as possible. Maximum entropy provides a principled way to encode our uncertainty in a model, and it is the precursor to modern Bayesian techniques [13]. Since the mean number of bits is just the entropy of the distribution, we wish to find that distribution that maximizes4
−
∑
i
Pi log Pi + ∑
j
λj(Cj − ∑
i
αjiPi) + μ(∑
i
Pi − 1) − ∑
i
δiPi (9)
where the sum constraint on the Pi is imposed with μ, and the positivity of each
Pi with δi (so δi ≥ 0 and at the maximum, δiPi = 0 ∀i)5. Differentiating with respect to Pk gives
Pk = exp(−1 + μ − δk − ∑
j
λjαjk) (10)
Since this is guaranteed to be positive we have δk = 0 ∀k. Imposing the sum
constraint then gives Pk = 1
Z exp(− ∑
j λjαjk) where the “partition function” Z
is just a normalizing factor. Note that the Lagrange multipliers have shown us the form that the solution must take, but that form does not automatically satisfy the constraints - they must still be imposed as a condition on the solution. The problem of maximizing the entropy subject to linear constraints therefore gives the widely used logistic regression model, where the parameters of the model are the Lagrange multipliers λi, which are themselves constrained by Eq. (8). For an example from the document classification task of how imposing linear constraints on the probabilities can arise in practice, see [16].
2.8 Some Algorithm Examples
Lagrange multipliers are ubiquitous for imposing constraints in algorithms. Here we list their use in a few modern machine learning algorithms; in all of these applications, the free constraint gambit proves useful. For support vector machines, the Lagrange multipliers have a physical force interpretation, and can be used to
4 The factor log2 e can be absorbed into the Lagrange multipliers. 5 Actually the free constraint gambit would work here, too.


Some Notes on Applied Mathematics for Machine Learning 29
find the exact solution to the problem of separating points in a symmetric simplex in arbitrary dimensions [6]. For the remaining algorithms mentioned here, see [7] for details on the underlying mathematics. In showing that the principal PCA directions give minimal reconstruction error, one requires that the projection directions being sought after are orthogonal, and this can be imposed by introducing a matrix of multipliers. In locally linear embedding [17], the translation invariance constraint is imposed for each local patch by a multiplier, and the constraint that a solution matrix in the reconstruction algorithm be orthogonal is again imposed by a matrix of multipliers. In the Laplacian eigenmaps dimensional reduction algorithm [2], in order to prevent the collapse to trivial solutions, the dimension of the target space is enforced to be d > 0 by requiring that the rank of the projected data matrix be d, and again this imposed using a matrix of Lagrange multipliers.
Historical Notes. Joseph Louis Lagrange was born in 1736 in Turin. He was one of only two of eleven siblings to survive infancy; he spent most of his life in Turin, Berlin and Paris. He started teaching in Turin, where he organized a research society, and was apparently responsible for much fine mathematics that was published from that society under the names of other mathematicians [3, 1]. He ’believed that a mathematician has not thoroughly understood his own work till he has made it so clear that he can go out and explain it effectively to the first man he meets on the street’ [3]6. His contributions lay in the subjects of mechanics, calculus7, the calculus of variations8, astronomy, probability, group theory, and number theory [14]. Lagrange is at least partly responsible for the choice of base 10 for the metric system, rather than 12. He was supported academically by Euler and d’Alembert, financed by Frederick and Louis XIV, and was close to Lavoisier (who saved him from being arrested and having his property confiscated, as a foreigner living in Paris during the Revolution), Marie Antoinette and the Abbe ́ Marie. He survived the Revolution, although Lavoisier did not. His work continued to be fruitful until his death in 1813, in Paris.
3 Some Notes on Matrices
This section touches on some useful results in the theory of matrices that are rarely emphasized in coursework. For a complete treatment, see for example [12] and [11]. Following [12], the set of p by q matrices is denoted Mpq, the set of (square) p by p matrices by Mp, and the set of symmetric p by p matrices by Sp. We work only with real matrices - the generalization of the results to the complex field is straightforward. In this section only, we will use the notation in which repeated indices are assumed to be summed over, so that for example
6 Sadly, at that time there were very few female mathematicians. 7 For example he was the first to state Taylor’s theorem with a remainder [14]. 8 . . . with which he started his career, in a letter to Euler, who then generously delayed publication of some similar work so that Lagrange could have time to finish his work [1].


30 C.J.C. Burges
AijBjkCkl is written as shorthand for ∑
j,k AijBjkCkl. Let’s warm up with some
basic facts.
3.1 A Dual Basis
Suppose you are given a basis of d orthonormal vectors e(a) ∈ Rd, a = 1, . . . , d, and you construct a matrix E ∈ Md whose columns are those vectors. It is a striking fact that the rows of E then also always form an orthonormal basis. We can see this as follows. Let the e(a) have components e(a)
i , i = 1, . . . , d. Let’s
write the vectors constructed from the rows of E as eˆ so that eˆ(a)
i ≡ e(ai). Then
orthonormality of the columns can be encapsulated as ET E = 1. However since E has full rank, it has an inverse, and ET EE−1 = E−1 = ET , so EET = 1 (using the fundamental fact that the left and right inverses of any square matrix are the same) which shows that the rows of E are also orthonormal. The vectors ˆe(a) are called the dual basis to the e(a). This result is sometimes useful in simplifying expressions: for example ∑
a e(a)
i e(a)
j Λ(i, j), where Λ is some function, can be
replaced by Λ(i, i)δij.
3.2 Other Ways to Think About Matrix Multiplication
Suppose you have matrices X ∈ Mmn and Y ∈ Mnp so that XY ∈ Mmp. The
familiar way to represent matrix multiplication is (XY )ab = ∑n
i=1 XaiYib, where
the summands are just products of numbers. However an alternative represen
tation is XY = ∑n
i=1 xiy′i, where xi (y′i) is the i’th column (row) of X (Y ), and
where the summands are outer products of matrices. For example, we can write the product of a 2 × 3 and a 3 × 2 matrix as
[a b c def
]
⎡
⎣
gh ij kl
⎤
⎦=
[a
d
] [g h] +
[b
e
] [i j] +
[c
f
] [k l]
One immediate consequence (which we’ll use in our description of singular value decomposition below) is that you can always add columns at the right of X, and rows at the bottom of Y , and get the same product XY , provided either the extra columns, or the extra rows, contain only zeros. To see why this expansion works it’s helpful to expand the outer products into standard matrix form: the matrix multiplication is just
{( a 0 0 d00
)
+
(0 b 0 0e0
)
+
(0 0 c 00f
)
+
}
×
⎧
⎨
⎩
⎛
⎝
gh 00 00
⎞
⎠+
⎛
⎝
00 ij 00
⎞
⎠+
⎛
⎝
00 00 kl
⎞
⎠
⎫
⎬
⎭
Along a similar vein, the usual way to view matrix-vector multiplication is as an operation that maps a vector z ∈ Rn to another vector z′ ∈ Rm: z′ = Xz. However you can also view the product as a linear combination of the columns
of X: z′ = ∑n
i=1 zixi. With this view it’s easy to see why the result must lie in
the span of the columns of X.


Some Notes on Applied Mathematics for Machine Learning 31
3.3 The Levi-Civita Symbol
The Levi-Civita symbol9 in d dimensions is denoted ǫij···k and takes the value 1 if its d indices are an even permutation of 1, 2, 3, · · · , d, the value -1 if an odd permutation, and 0 otherwise. The 3-dimensional version of this is the fastest way I know to derive vector identities in three dimensions, using the identity ǫijkǫimn = δjmδkn − δjnδkm (recall that repeated indices are summed).
Exercise 5. Use the fact that a = b ∧ c can be written in component form as ai = ǫijkbjck to derive, in one satisfying line, the vector identity in three dimensions: (a ∧ b) · (c ∧ d) = (a · c)(b · d) − (a · d)(b · c).
3.4 Characterizing the Determinant and Inverse
The determinant of a matrix A ∈ Mn can be defined as
|A| ≡ 1
n! ǫα1α2···αn ǫβ1β2···βn Aα1β1 Aα2β2 · · · Aαnβn (11)
Exercise 6. Show that also,
|A| = ǫα1α2···αn A1α1 A2α2 · · · Anαn (12)
We can use this to prove an interesting theorem linking the determinant, derivatives, and the inverse:
Lemma 1. For any square nonsingular matrix A,
∂|A|
∂Aij
= A−1
ji (13)
Proof.
∂|A|
∂Aij
= ǫjα2···αn δi1A2α2 · · · Anαn + ǫα1j···αn A1α1 δi2A3α3 · · · Anαn + · · ·
so
Akj
∂|A|
∂Aij
= ǫα1α2···αn (Akα1 δi1A2α2 · · · Anαn + A1α1 Akα2 δi2A3α3 · · · + · · · )
For any value of i, one and only one term in the sum on the right survives, and for that term, we must have k = i by antisymmetry of the ǫ. Thus the right hand side is just |A|δki. Multiplying both sides on the right by (AT )−1 gives the result. ⊓⊔
9 The name ‘tensor’ is sometimes incorrectly applied to arbitrary objects with more than one index. In factor a tensor is a generalization of the notion of a vector and is a geometrical object (has meaning independent of the choice of coordinate system); ǫ is a pseudo-tensor (transforms as a tensor, but changes sign upon inversion).


32 C.J.C. Burges
We can also use this to write the following closed form for the inverse:
A−1
ij = 1
|A|(n − 1)! ǫjα1α2···αn−1 ǫiβ1β2···βn−1 Aα1β1 Aα2β2 · · · Aαn−1βn−1 (14)
Exercise 7. Prove this, using Eqs. (11) and (13).
Exercise 8. Show that, for an arbitrary non-singular square matrix A,
∂ A−1
ij
∂Aαβ = −A−1
iα A−1
βj . (Hint: take derivatives of A−1A = 1).
Exercise 9. The density p(x) for a multivariate Gaussian is proportional to |Σ|−1/2 exp (− 1
2 (x − μ)′Σ−1(x − μ)). For n independent and identically dis
tributed points, the density is p(x1, x2, · · · , xn|μ, Σ) = ∏
i p(xi|μ, Σ). By taking
derivatives with respect to μ and Σ and using the above results, show that the maximum likelihood values for the mean and covariance matrix are just their sample estimates.
Puzzle 5: Suppose that in Exercise 9, n = 2, and that x1 = −x2, so that the maximum likelihood estimate for the mean is zero. Suppose that Σ is chosen to have positive determinant but such that x is an eigenvector with negative eigenvalue. Then the likelihood can be made as large as you like by just scaling Σ with a positive scale factor, which appears to contradict the results of Exercise 9. What’s going on?
3.5 SVD in Seven Steps
Singular value decomposition is a generalization of eigenvalue decomposition. While eigenvalue decomposition applies only to square matrices, SVD applies to rectangular; and while not all square matrices are diagonalizable, every matrix has an SVD. SVD is perhaps less familiar, but it plays important roles in everything from theorem proving to algorithm design (for example, for a classic result on applying SVD to document categorization, see [10]). The key observation is that, given A ∈ Mmn, although we cannot perform an eigendecomposition of A,
we can do so for the two matrices AAT ∈ Sm and AT A ∈ Sn. Since both of
these are positive semidefinite, their eigenvalues are non-negative; if AAT has rank k, define the ‘singular values’ σi2 to be its k positive eigenvalues. Below we will use ‘nonzero eigenvector’ to mean an eigenvector with nonzero eigenvalue, will denote the diagonal matrix whose i’th diagonal component is σi by diag(σi), and will assume without loss of generality that m ≤ n. Note that we repeatedly use the tricks mentioned in Section (3.2). Let’s derive the SVD.
1. AAT has the same nonzero eigenvalues as AT A. Let xi ∈ Rm be an eigenvec
tor of AAT with positive eigenvalue σi2, and let yi ≡ (1/σi)(AT xi), y ∈ Rn.
Then AT Ayi = (1/σi)AT AAT xi = σiAT xi = σi2yi. Similarly let yi ∈ Rn be
an eigenvector of AT A with eigenvalue σi′2, and let zi ≡ (1/σi′)(Ayi). Then
AAT zi = (1/σi′)AAT Ayi = σi′Ayi = σi′2zi. Thus there is a 1-1 correspon
dence between nonzero eigenvectors for the matrices AT A and AAT , and the corresponding eigenvalues are shared.


Some Notes on Applied Mathematics for Machine Learning 33
2. The xi can be chosen to be orthonormal, in which case so also are the yi. The xi are orthonormal, or can be so chosen, since they are eigenvectors of
a symmetric matrix. Then yi · yj ∝ x′iAAT xj ∝ xi · xj ∝ δij.
3. rank(A) = rank(AT ) = rank(AAT ) = rank(AT A) ≡ k [12]. 4. Let the xi be the nonzero eigenvectors of AAT and the yi those of AT A. Let X ∈ Mmk (Y ∈ Mnk) be the matrix whose columns are the xi (yi). Then
Y = AT Xdiag(1/σi) ⇒ diag(σi)Y T = XT A. Note that m ≥ k; if m = k,
then A = Xdiag(σi)Y T . 5. If m > k, add m − k rows of orthonormal null vectors of AT to the bottom of XT , and add m − k zero rows to the bottom of diag(σi); defining the latter
to be diag(σi, 0), then X is orthogonal and A = Xdiag(σi, 0)Y T . Note that here, X ∈ Mm, diag(σi, 0) ∈ Mmk and Y ∈ Mnk. 6. To get something that looks more like an eigendecomposition, add n − k rows of vectors that, together with the yi form an orthonormal set, to the
bottom of Y T , and add n − k columns of zeros to the right of diag(σi, 0); defining the latter to be diag(σi, 0, 0), then the Y are also orthogonal and
A = Xdiag(σi, 0, 0)Y T . Note that here, X ∈ Mm, diag(σi, 0, 0) ∈ Mmn, and Y ∈ Mn.
7. To get something that looks more like a sum of outer products, just write A
in step (4) as A = ∑k
i=1 σixiy′i.
Let’s put the singular value decomposition to work.
3.6 The Moore-Penrose Generalized Inverse
Suppose B ∈ Sm has eigendecomposition B = EΛET , where Λ is diagonal and E is the orthogonal matrix of column eigenvectors. Suppose further that B is nonsingular, so that B−1 = EΛ−1ET = ∑
i(1/λi)eie′i. This suggests that, since SVD
generalizes eigendecomposition, perhaps we can also use SVD to generalize the notion of matrix inverse to non-square matrices A ∈ Mmn. The Moore-Penrose
generalized inverse (often called just the generalized inverse) does exactly this10. In outer product form, it’s the SVD analog of the ordinary inverse, with the latter
written in terms of outer products of eigenvectors: A† = ∑k
i=1(1/σi)yix′i ∈ Mnm.
The generalized inverse has several special properties:
1. AA† and A†A are Hermitian; 2. AA†A = A; 3. A†AA† = A†.
In fact, A† is uniquely determined by conditions (1), (2) and (3). Also, if A is square and nonsingular, then A† = A−1, and more generally, if (AT A)−1 exists, then A† = (AT A)−1AT , and if (AAT )−1 exists, then A† = AT (AAT )−1. The generalized inverse comes in handy, for example, in characterizing the general solution to linear equations, as we’ll now see.
10 The Moore-Penrose generalized inverse is one of many pseudo inverses.


34 C.J.C. Burges
3.7 SVD, Linear Maps, Range and Null Space
If A ∈ Mmn, the range of A, R(A), is defined as that subspace spanned by
y = Ax for all x ∈ Rn. A’s null space N (A), on the other hand, is that subspace spanned by those x ∈ Rn for which Ax = 0. Letting A|i denote the columns of A, recall that Ax = x1A|1 + x2A|2 + · · · + xnA|n, so that the dimension of R(A) is the rank k of A, and R(A) is spanned by the columns of A. Also, N (AT ) is spanned by those vectors which are orthogonal to every row of AT (or every column of A), so R(A) is the orthogonal complement of N (AT ). The notions of range and null space are simply expressed in terms of the SVD, A =
∑k
i=1 σixiy′i, x ∈ Rm, y ∈ Rn. The null space of A is the subspace orthogonal
to the k yi, so dim(N (A)) = n − k. The range of A is spanned by the xi, so dim(R(A)) = k. Thus in particular, we have dim(R(A)) + dim(N (A)) = n. The SVD provides a handy way to characterize the solutions to linear systems of equations. In general the system Az = b, A ∈ Mmn, z ∈ Rn, b ∈ Rm has 0, 1 or ∞ solutions (if z1 and z2 are solutions, then so is αz1 + βz2, α, β ∈ R). When does a solution exist? Since Az is a linear combination of the columns of A, b must lie in the span of those columns. In fact, if b ∈ R(A), then z0 = A†b is
a solution, since Az0 = ∑k
i=1 σixiy′i
∑k
j=1(1/σi)yj x′j b = ∑k
i=1 xix′ib = b, and the general solution is therefore z = A†b + N (A).
Puzzle 6: How does this argument break down if b ∈/ R(A)?
What if b ∈/ R(A), i.e. Az = b has no solution? One reasonable step would be to find that z that minimizes the Euclidean norm ‖Az − b‖. However, adding any vector in N (A) to a solution z would also give a solution, so a reasonable second step is to require in addition that ‖z‖ is minimized. The general solution to this is again z = A†b. This is closely related to the following unconstrained quadratic programming problem: minimize f (z) = 1
2 z′Az + bz, x ∈ Rn, A 0.
(We need the extra condition on A since otherwise f can be made arbitrarily negative). The solution to this is at ∇f = 0 → Az + b = 0, so the general solution is again z = A†b + N (A).
Puzzle 7: If b ∈/ R(A), there is again no solution, even though A 0. What happens if you go ahead and try to minimize f anyway?
3.8 Matrix Norms
A function ‖·‖ : Mmn → R is a matrix norm over a field F if for all A, B ∈ Mmn,
1. ‖A‖ ≥ 0
2. ‖A‖ = 0 ⇔ A = 0 3. ‖cA‖ = |c|‖A‖ for all scalars c ∈ F 4. ‖A + B‖ ≤ ‖A‖ + ‖B‖
The Frobenius norm, ‖A‖F =
√ ∑
ij |Aij|2, is often used to represent the
distance between matrices A and B as ‖A − B‖2F , when for example one is
searching for that matrix which is as close as possible to a given matrix, given


Some Notes on Applied Mathematics for Machine Learning 35
some constraints. For example, the closest positive semidefinite matrix, in Frobe
nius norm, to a given symmetric matrix A, is Aˆ ≡ ∑
i:λi>0 λie(i)e′(i) where the
λi, e(i) are the eigenvalues and eigenvectors of A, respectively. The Minkowski vector p-norm also has a matrix analog: ‖A‖p ≡ max‖x‖=1 ‖Ax‖p. There are three interesting special cases of this which are easy to compute: the maximum absolute column norm, ‖A‖1 ≡ maxj
∑n
i |Aij|, the maximum absolute row norm,
‖A‖∞ ≡ maxi
∑n
j |Aij|, and the spectral norm, ‖A‖2. Both the Frobenius and
spectral norms can be written in terms of the singular values: assuming the
ordering σ1 ≥ σ2 · · · ≥ σk, then ‖A‖2 = σ1 and ‖A‖F =
√
∑k
i=1 σi2.
Exercise 10. Let U and W be orthogonal matrices. Show that ‖U AW ‖F = ‖A‖F .
Exercise 11. The submultiplicative property, ‖AB‖ ≤ ‖A‖‖B‖, is an additional property that some matrix norms satisfy [11]11. Prove that, if A ∈ Mm
and if a submultiplicative norm exists for which ‖A‖ < 1, then (1 + A)−1 = 1 − A + A2 − A3 + · · · , and if A is nonsingular and a submultiplicative norm exists for which ‖A−1‖ < 1, then (1+A)−1 = A−1(1−A−1+A−2−A−3+· · · ). Show that for any rectangular matrix W , W (1 + W ′W )−1W ′ = (1 + W W ′)−1W W ′ = W W ′(1 + W W ′)−1. (This is used, for example, in the derivation of the conditional distribution of the latent variables given the observed variables, in probabilistic PCA [19].)
The Minkowski p norm has the important property that ‖Ax‖p ≤ ‖A‖p ‖x‖p.
Let’s use this, and the L1 and L∞ matrix norms, to prove a basic fact about stochastic matrices. A matrix P is stochastic if its elements can be interpreted as probabilities, that is, if all elements are real and non-negative, and each row sums to one (row-stochastic), or each column sums to one (column-stochastic), or both (doubly stochastic).
Theorem 1. If P is a square stochastic matrix, then P has eigenvalues whose absolute values lie in the range [0, 1].
Proof. For any p ≥ 1, and x any eigenvector of P , ‖P x‖p = |λ| ‖x‖p ≤ ‖P ‖p ‖x‖p
so |λ| ≤ ‖P ‖p. Suppose that P is row-stochastic; then choose the L∞ norm, which
is the maximum absolute row norm ‖P ‖∞ = maxi
∑
j |Pij| = 1; so |λ| ≤ 1. If
P is column-stochastic, choosing the 1-norm (the maximum absolute column norm) gives the same result. ⊓⊔
Note that stochastic matrices, if not symmetric, can have complex eigenvalues, so in this case F is the field of complex numbers.
3.9 Positive Semidefinite Matrices
Positive semidefinite matrices are ubiquitous in machine learning theory and algorithms (for example, every kernel matrix is positive semidefinite, for Mercer
11 Some authors include this in the definition of matrix norm [12].


36 C.J.C. Burges
kernels). Again we restrict ourselves to real matrices. A matrix A ∈ Sn is positive
definite iff for every x ∈ Rn, x′Ax > 0; it is positive semidefinite iff for every x ∈ Rn, x′Ax ≥ 0, and some x exists for which the equality is met. Recall that we denote the property of positive definiteness of a matrix A by A ≻ 0, and positive semidefiniteness by A 0. Let’s start by listing a few properties, the first of which relate to what positive semidefinite matrices look like (here, repeated indices are not summed):
1. If A ≻ 0, then Aii > 0 ∀i; 2. If A 0, then Aii ≥ 0 ∀i; 3. If A 0, then Aii = 1 ∀i ⇒ |Aij| ≤ 1 ∀i, j; 4. If A ∈ Sn is strictly diagonally dominant, that is, Aii > ∑
j=i |Aij | ∀i, then
it is also positive definite; 5. If A 0 and Aii = 0 for some i, then Aij = Aji = 0 ∀j;
6. If A 0 then AiiAjj ≥ |Aij|2 ∀i, j; 7. If A ∈ Sn 0 and B ∈ Sn 0 then AB 0;
8. A ∈ Sn is positive semidefinite and of rank one iff A = xx′ for some x ∈ Rn; 9. A ≻ 0 ⇔ A all of the leading minors of A are positive.
A very useful way to think of positive semidefinite matrices is in terms of Gram matrices. Let V be a vector space over some field F, with inner product 〈·, ·〉. The Gram matrix G of a set of vectors vi ∈ V is defined by Gij ≡ 〈vi, vj〉. Now let V be Euclidean space and let F be the reals. The key result is the following: let A ∈ Sn. Then A is positive semidefinite with rank r if and only if there exists a set of vectors {v1, . . . , vn}, vi ∈ V , containing exactly r linearly independent vectors, such that Aij = vi · vj.
Note in particular that the vectors v can always be chosen to have dimension r ≤ n.
Puzzle 8: A kernel matrix K ∈ Sn is a matrix whose elements take the form
Kij ≡ k(xi, xj) for some xi, xj ∈ Rd, i, j = 1, . . . , n for some d, where k is a symmetric function which satisfies Mercer’s condition (see e.g. [6]). For any such function k, there exists an inner product space H and a map Φ : Rd → H such that k(xi, xj) = Φ(xi) · Φ(xj). The dimension of H can be large, or even infinite (an example of the latter is k(xi, xj) = exp−(1/σ2)‖xi−xj‖2 ). In particular, the dimension of the dot product space can be larger than n. How does this square with the claim just made about the maximum necessary dimension of the Gram vectors?
Some properties of positive semidefinite matrices that might otherwise seem mysterious become obvious, when they are viewed as Gram matrices, as I hope the following exercise helps demonstrate.
Exercise 12. Use the fact that every positive semidefinite matrix is a Gram matrix to prove items (2), (3), (5), and (6) in the list above. Use the definition of a positive (semi)definite matrix to prove (1), (4), (7) and (8).
If the Gram representation is so useful, the question naturally arises: given a positive semidefinite matrix, how can you extract a set of Gram vectors for


Some Notes on Applied Mathematics for Machine Learning 37
it? (Note that the set of Gram vectors is never unique; for example, globally rotating them gives the same matrix). Let A ∈ Sn 0 and write the eigen
decomposition of A in outer product form: A = ∑n
a=1 λae(a)e′(a) or Aij =
∑n
a=1 λae(a)
i e(a)
j . Written in terms of the dual eigenvectors (see Section 3.1):
Aij = ∑n
a=1 λaeˆ(ai)eˆ(aj), the summand has become a weighted dot product; we
can therefore take the set of Gram vectors to be v(ai) = √λaeˆ(ai). The Gram vectors therefore are the dual basis to the scaled eigenvectors.
3.10 Distance Matrices
One well-known use of the Gram vector decomposition of positive semidefinite matrices is the following. Define a ‘distance matrix’ to be any matrix of the form Dij ∈ Sn ≡ ‖xi − xj‖2, where ‖ · ‖ is the Euclidean norm (note that the entries are actually squared distances). A central goal of multidimensional scaling is the following: given a matrix which is a distance matrix, or which is approximately a distance matrix, or which can be mapped to an approximate distance matrix, find the underlying vectors xi ∈ Rd, where d is chosen to be as small as possible, given the constraint that the distance matrix reconstructed from the xi approximates D with acceptable accuracy [8]. d is chosen to be small essentially to remove unimportant variance from the problem (or, if sufficiently small, for data visualization). Now let e be the column vector of n ones, and introduce the ‘centering’ projection matrix P e ≡ 1 − 1
n ee′.
Exercise 13. Prove the following: (1) for any x ∈ Rn, P ex subtracts the mean value of the components of x from each component of x, (2) P ee = 0, (3) e is the only eigenvector of P e with eigenvalue zero, and (4) for any dot product matrix Aij ∈ Sm ≡ xi·xj, i, j = 1, . . . , m, xi ∈ Rn, then (P eAP e)ij = (xi−μ)·(xj −μ), where μ is the mean of the xi.
The earliest form of the following theorem is due to Schoenberg [18]. For a proof of this version, see [7].
Theorem 2. Consider the class of symmetric matrices A ∈ Sn such that Aij ≥
0 and Aii = 0 ∀i, j. Then A ̄ ≡ −P eAP e is positive semidefinite if and only if
A is a distance matrix, with embedding space Rd for some d. Given that A is a
distance matrix, the minimal embedding dimension d is the rank of A ̄, and the
embedding vectors are any set of Gram vectors of A ̄, scaled by a factor of √12 .
3.11 Computing the Inverse of an Enlarged Matrix
We end our excursion with a look at a trick for efficiently computing inverses. Suppose you have a symmetric matrix K ∈ Sn−1, and suppose you form a new symmetric matrix by adding a number u ≡ Knn and a column v, vi ≡ Kin (and a corresponding row Kni ≡ Kin). Denote the enlarged matrix by
K+ =
(K v v′ u
)
(15)


38 C.J.C. Burges
Now consider the inverse
K+−1 ≡
(A b b′ c
)
(16)
where again b is a column vector and c is a scalar. It turns out that it is straightforward to compute A, b and c in terms of K−1, v and u. Why is this useful? In any machine learning algorithm where the dependence on all the data is captured by a symmetric matrix K(xi, xj), then in test phase, when a prediction is being made for a single point x, the dependence on all the data is captured by K+, where vi = K(xi, x) and u = K(x, x). If that algorithm in addition requires that the quantities b and c be computed, it’s much more efficient to compute them by using the following simple lemma (and computing K−1 just once, for
the training data), rather than by computing K+−1 for each x. This is used, for example, in Gaussian process regression and Gaussian process classification, where in Gaussian process regression, c is needed to compute the variance in the estimate of the function value f (x) at the test point x, and b and c are needed to compute the mean of f (x) [9, 20].
Lemma 2. Given K ∈ Mn−1 and K+ ∈ Mn as defined above, then the elements of K+ are given by:
c= 1
u − v′K−1v (17)
b=− 1
u − v′K−1v v′K−1 (18)
Aij = K−1
ij + 1
u − v′K−1v (v′K−1)i(v′K−1)j (19)
and furthermore,
det(K )
det(K+) = 1
u − v′K−1v = c (20)
Proof. Since the inverse of a symmetric matrix is symmetric, K+−1 can be written
in the form (16). Then requiring that K+−1K+ = 1 gives (repeated indices are summed):
i < n, j < n : AimKmj + bivj = δij (21) i = n, j < n : bmKmj + cvj = 0 (22) i < n, j = n : Aimvm + biu = 0 (23) i = n, j = n : bmvm + cu = 1 (24)
Eq. (22) gives b = −cv′K−1. Substituting this in (24) gives Eq. (17), and substituting it in (21) gives Eq. (19). Finally the expression for the ratio of determinants follows from the expression for the elements of an inverse matrix in terms of ratios of its cofactors. ⊓⊔


Some Notes on Applied Mathematics for Machine Learning 39
Exercise 14. Verify formulae (17), (18), (19) and (20) for a matrix K+ ∈ S2 of your choice.
Puzzle 9: Why not use this result iteratively (starting at n = 2) to compute the inverse of an arbitrary symmetric matrix A ∈ Sn? How does the number of operations needed to do this compare with the number of operations needed by Gaussian elimination (as a function of n)? If, due to numerical problems, the first (top left) element of the first matrix is off by a factor 1 + ǫ, ǫ ≪ 1, what is the error (roughly) in the estimated value of the final (bottom right) element of Sn?
References
1. W.W. Rouse Ball. A Short Account of the History of Mathematics. Dover, 4 edition, 1908. 2. M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. In Advances in Neural Information Processing Systems 14. MIT Press, 2002. 3. E.T. Bell. Men of Mathematics. Simon and Schuster, Touchstone edition, 1986; first published 1937. 4. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. 5. B. Buck and V. Macaualay (editors). Maximum Entropy in Action. Clarendon Press, 1991. 6. C.J.C. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2):121–167, 1998.
7. C.J.C. Burges. Geometric Methods for Feature Extraction and Dimensional Reduction. In L. Rokach and O. Maimon, editors, Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers. Kluwer Academic, 2004, to appear. 8. T.F. Cox and M.A.A. Cox. Multidimensional Scaling. Chapman and Hall, 2001. 9. Noel A.C. Cressie. Statistics for spatial data. Wiley, revised edition, 1993. 10. S. Deerwester, S.T. Dumais, T.K. Landauer, G.W. Furnas, and R.A. Harshman. Indexing by Latent Semantic Analysis. Journal of the Society for Information Science, 41(6):391–407, 1990. 11. G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins, third edition, 1996. 12. R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, 1985. 13. E.T. Jaynes. Bayesian methods: General background. In J.H. Justice, editor, Maximum Entropy and Bayesian Methods in Applied Statistics, pages 1–25. Cambridge University Press, 1985.
14. Morris Kline. Mathematical Thought from Ancient to Modern Times, Vols. 1,2,3. Oxford University Press, 1972. 15. O.L. Mangasarian. Nonlinear Programming. McGraw Hill, New York, 1969. 16. K. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classification. In IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61–67, 1999. 17. S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(22):2323–2326, 2000.


40 C.J.C. Burges
18. I.J. Schoenberg. Remarks to maurice frechet’s article sur la de ́finition axiomatique d’une classe d’espace distancie ́s vectoriellement applicable sur l’espace de Hilbert. Annals of Mathematics, 36:724–732, 1935.
19. M.E. Tipping and C.M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, 61(3):611, 1999.
20. C.K.I. Williams. Prediction with gaussian processes: from linear regression to linear prediction and beyond. In Michael I. Jordan, editor, Learning in Graphical Models, pages 599–621. MIT Press, 1999.


Bayesian Inference: An Introduction to
Principles and Practice in Machine Learning
Michael E. Tipping
Microsoft Research, 7 J J Thomson Avenue, Cambridge CB3 0FB, U.K. mtipping@microsoft.com http://www.research.microsoft.com/users/mtipping
Abstract. This article gives a basic introduction to the principles of Bayesian inference in a machine learning context, with an emphasis on the importance of marginalisation for dealing with uncertainty. We begin by illustrating concepts via a simple regression task before relating ideas to practical, contemporary, techniques with a description of ‘sparse Bayesian’ models and the ‘relevance vector machine’.
1 Introduction
What is meant by “Bayesian inference” in the context of machine learning? To assist in answering that question, let’s start by proposing a conceptual task: we wish to learn, from some given number of example instances of them, a model of the relationship between pairs of variables A and B. Indeed, many machine learning problems are of the type “given A, what is B?”.1 Verbalising what we typically treat as a mathematical task raises an interesting question in itself. How do we answer “what is B?”? Within the appealingly well-defined and axiomatic framework of propositional logic, we ‘answer’ the question with complete certainty, but this logic is clearly too rigid to cope with the realities of real-world modelling, where uncertainty over ‘truth’ is ubiquitous. Our measurements of both the dependent (B) and independent (A) variables are inherently noisy and inexact, and the relationships between the two are invariably non-deterministic. This is where probability theory comes to our aid, as it furnishes us with a principled and consistent framework for meaningful reasoning in the presence of uncertainty. We might think of probability theory, and in particular Bayes’ rule, as providing us with a “logic of uncertainty” [1]. In our example, given A we would ‘reason’ about the likelihood of the truth of B (let’s say B is binary for example) via its conditional probability P (B|A): that is, “what is the probability of B given that A takes a particular value?”. An appropriate answer might be “B is true with probability 0.6”. One of the primary tasks of ‘machine learning’ is
1 In this article we will focus exclusively on such ‘supervised learning’ tasks, although of course there are other modelling applications which are equally amenable to Bayesian inferential techniques.
O. Bousquet et al. (Eds.): Machine Learning 2003, LNAI 3176, pp. 41–62, 2004. c© Springer-Verlag Berlin Heidelberg 2004


42 M.E. Tipping
then to approximate P (B|A) with some appropriately specified model based on a given set of corresponding examples of A and B.2 It is in the modelling procedure where Bayesian inference comes to the fore. We typically (though not exclusively) deploy some form of parameterised model for our conditional probability:
P (B|A) = f (A; w), (1)
where w denotes a vector of all the ‘adjustable’ parameters in the model. Then, given a set D of N examples of our variables, D = {An, Bn}nN=1, a conventional approach would involve the maximisation of some measure of ‘accuracy’ (or minimisation of some measure of ‘loss’) of our model for D with respect to the adjustable parameters. We then can make predictions, given A, for unknown B by evaluating f (A; w) with parameters w set to their optimal values. Of course, if our model f is made too complex — perhaps there are many adjustable parameters w — we risk over-specialising to the observed data D, and consequently realising a poor model of the true underlying distribution P (B|A). The first key element of the Bayesian inference paradigm is to treat parameters such as w as random variables, exactly the same as A and B. So the conditional probability now becomes P (B|A, w), and the dependency of the probability of B on the parameter settings, as well as A, is made explicit. Rather than ‘learning’ comprising the optimisation of some quality measure, a distribution over the parameters w is inferred from Bayes’ rule. We will demonstrate this concept by means of a simple example regression task in Section 2. To obtain this ‘posterior’ distribution over w alluded to above, it is necessary to specify a ‘prior’ distribution p(w) before we observe the data. This may be considered an inconvenience, but Bayesian inference treats all sources of uncertainty in the modelling process in a unified and consistent manner, and forces us to be explicit as regards our assumptions and constraints; this in itself is arguably a philosophically appealing feature of the paradigm. However, the most attractive facet of a Bayesian approach is the manner in which “Ockham’s Razor” is automatically implemented by ‘integrating out’ all irrelevant variables. That is, under the Bayesian framework there is an automatic preference for simple models that sufficiently explain the data without unnecessary complexity. We demonstrate this key feature in Section 3, and in particular underline the point that this property holds even if the prior p(w) is completely uninformative. We show that, in practical terms, the concept of Ockham’s Razor enables us to ‘set’ regularisation parameters and ‘select’ models without the need for any additional validation procedure. The practical disadvantage of the Bayesian approach is that it requires us to perform integrations over variables, and many of these computations are analytically intractable. As a result, much contemporary research in Bayesian
2 In many learning methods, this conditional probability approximation is not made explicit, though such an interpretation may exist. However, one might consider it a significant limitation if a particular machine learning procedure cannot be expressed coherently within a probabilistic framework.


Bayesian Inference: Principles and Practice in Machine Learning 43
approaches to machine learning relies on, or is directly concerned with, approximation techniques. However, we show in Section 4, where we describe the “sparse Bayesian” model, that a combination of analytic calculation and straightforward, practically efficient, approximation can offer state-of-the-art results.
2 From Least-Squares to Bayesian Inference
We introduce the methodology of Bayesian inference by considering an example prediction (regression) problem. Let us assume we are given a very simple data set (illustrated later within Figure 1) comprising N = 15 samples artificially generated from the function y = sin(x) with added Gaussian noise of variance 0.2. We will denote the ‘input’ variables in our example by xn, n = 1 . . . N . For each such xn, there is an associated real-valued ‘target’ tn, n = 1 . . . N , and from these input-target pairs, we wish to ‘learn’ the underlying functional mapping.
2.1 Linear Models
We will model this data with some parameterised function y(x; w), where w = (w1, w2, . . . , wM ) is the vector of adjustable model parameters. Here, we consider linear models (strictly, “linear-in-the-parameter”) models which are a linearlyweighted sum of M fixed (but potentially nonlinear) basis functions φm(x):
y(x; w) =
M ∑
m=1
wmφm(x). (2)
For our purposes here, we make the common choice to utilise Gaussian data
centred basis functions φm(x) = exp {−(x − xm)2/r2}, which gives us a ‘radial basis function’ (RBF) type model.
“Least-Squares” Approximation. Our objective is to find values for w such that y(x; w) makes good predictions for new data: i.e. it models the underlying generative function. A classic approach to estimating y(x; w) is “least-squares”, minimising the error measure:
ED(w) = 1
2
N ∑
n=1
[
tn −
M ∑
m=1
wmφm(xn)
]2
. (3)
If t = (t1, . . . , tN )T and Φ is the ‘design matrix’ such that Φnm = φm(xn), then the minimiser of (3) is obtained in closed-form via linear algebra:
wLS = (ΦTΦ)−1ΦTt. (4)
However, with M = 15 basis functions and only N = 15 examples here, we know that minimisation of squared-error leads to a model which exactly interpolates the data samples, as shown in Figure 1. Now, we may look at Figure 1 and exclaim “the function on the right is clearly over-fitting!”. But, without prior knowledge of the ‘truth’, can we really


44 M.E. Tipping
0246
−1.5
−1
−0.5
0
0.5
1
1.5
Ideal fit Least−squares RBF fit
Fig. 1. Overfitting? The ‘ideal fit’ is shown on the left, while the least-squares fit using 15 basis functions is shown on the right and perfectly interpolates all the data points
judge which model is genuinely better? The answer is that we can’t — in a realworld problem, the data could quite possibly have been generated by a complex function such as shown on the right. The only way that we can proceed to meaningfully learn from data such as this is by imposing some a priori prejudice on the nature of the complexity of functions we expect to elucidate. A common way of doing this is via ‘regularisation’.
2.2 Complexity Control: Regularisation
A common, and generally very reasonable, assumption is that we typically expect that data is generated from smooth, rather than complex, functions. In a linear model framework, smoother functions typically have smaller weight magnitudes, so we can penalise complex functions by adding an appropriate penalty term to the cost function that we minimise:
̂E(w) = ED(w) + λEW (w). (5)
A standard choice is the squared-weight penalty, EW (w) = 1
2
∑M
m=1 w2m,
which conveniently gives the “penalised least-squares” (PLS) estimate for w:
wP LS = (ΦTΦ + λI)−1ΦTt. (6)
The hyperparameter λ balances the trade-off between ED(w) and EW (w) i.e. between how well the function fits the data and how smooth it is. Given that we can compute the weights directly for a given λ, the learning problem is now transformed into one of finding an appropriate value for that hyperparameter. A very common approach is to assess potential values of λ according to the error calculated on a set of ‘validation’ data (i.e. data which is not used to estimate w), and examples of fits for different values of λ and their associated validation errors are given in Figure 2. In practice, we might evaluate a large number of models with different hyperparameter values and select the model with lowest validation error, as demonstrated in Figure 3. We would then hope that this would give us a model which


Bayesian Inference: Principles and Practice in Machine Learning 45
0246
−1.5
−1
−0.5
0
0.5
1
1.5
All data Validation error: E = 2.11
Validation error: E = 0.52 Validation error: E = 0.70
Fig. 2. Function estimates (solid line) and validation error for three different values of regularisation hyperparameter λ (the true function is shown dashed). The training data is plotted in black, and the validation set in green (gray)
was close to ‘the truth’. In this artificial case where we know the generative function, the deviation from ‘truth’ is illustrated in the figure with the measurement of ‘test error’, the error on noise-free samples of sin(x). We can see that the minimum validation error does not quite localise the best test error, but it is arguably satisfactorily close. We’ll come back to this graph in Section 3 when we look at marginalisation and how Bayesian inference can be exploited in order to estimate λ. For now, we look at how this regularisation approach can be initially reformulated within a Bayesian probabilistic framework.
2.3 A Probabilistic Regression Framework
We assume as before that the data is a noisy realisation of an underlying functional model: tn = y(xn; w) + ǫn. Applying least-squares resulted in us min
imising ∑
n ǫ2n, but here we first define an explicit probabilistic model over the
noise component ǫn, chosen to be a Gaussian distribution with mean zero and
variance σ2. That is, p(ǫn|σ2) = N (0, σ2). Since tn = y(xn; w) + ǫn it fol
lows that p(tn|xn,w, σ2) = N (y(xn; w), σ2). Assuming that each example from the the data set has been generated independently (an often realistic assumption, although not always true), the likelihood 3 of all the data is given by the product:
3 Although ‘probability’ and ‘likelihood’ functions may be identical, a common convention is to refer to “probability” when it is primarily interpreted as a function of the random variable t, and “likelihood” when interpreted as a function of the parameters w.


46 M.E. Tipping
−14 −12 −10 −8 −6 −4 −2 0 2 4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Training
Validation
Test
log λ
Normalised error
Fig. 3. Plots of error computed on the separate 15-example training and validation sets, along with ‘test’ error measured on a third noise-free set. The minimum test and validation errors are marked with a triangle, and the intersection of the best λ computed via validation is shown
p(t|x,w, σ2) =
N ∏
n=1
p(tn|xn,w, σ2), (7)
=
N ∏
n=1
(2πσ2)−1/2 exp
[
− {tn − y(xn; w)}2
2σ2
]
. (8)
Note that, from now on, we will write terms such as p(t|x,w, σ2) as p(t|w, σ2), since we never seek to model the given input data x. Omitting to include such conditioning variables is purely for notational convenience (it implies no further model assumptions) and is common practice.
2.4 Maximum Likelihood and Least-Squares
The ‘maximum-likelihood’ estimate for w is that value which maximises p(t|w, σ2). In fact, this is identical to the ‘least-squares’ solution, which we can see by noting that minimising squared-error is equivalent to minimising the negative logarithm of the likelihood which here is:
− log p(t|w, σ2) = N
2 log(2πσ2) + 1
2σ2
N ∑
n=1
{tn − y(xn; w)}2 . (9)
Since the first term on the right in (9) is independent of w, this leaves only the second term which is proportional to the squared error.
2.5 Specifying a Bayesian Prior
Of course, giving an identical solution for w as least-squares, maximum likelihood estimation will also result in overfitting. To control the model complexity,


Bayesian Inference: Principles and Practice in Machine Learning 47
instead of the earlier regularisation weight penalty EW (w), we now define a prior distribution which expresses our ‘degree of belief’ over values that w might take:
p(w|α) =
M ∏
m=1
(α
2π
)1/2 exp
{
−α
2 w2m
}
. (10)
This (common) choice of a zero-mean Gaussian prior, expresses a preference for smoother models by declaring smaller weights to be a priori more probable. Though the prior is independent for each weight, there is a shared inverse variance hyperparameter α, analogous to λ earlier, which moderates the strength of our ‘belief’.
2.6 Posterior Inference
Previously, given our error measure and regulariser, we computed a single point estimate wLS for the weights. Now, given the likelihood and the prior, we compute the posterior distribution over w via Bayes’ rule:
p(w|t, α, σ2) = likelihood × prior
normalising factor = p(t|w, σ2)p(w|α)
p(t|α, σ2) . (11)
As a consequence of combining a Gaussian prior and a linear model within a Gaussian likelihood, the posterior is also conveniently Gaussian: p(w|t, α, σ2) = N (μ, Σ) with
μ = (ΦTΦ + σ2αI)−1ΦTt, (12)
Σ = σ2(ΦTΦ + σ2αI)−1. (13)
So instead of ‘learning’ a single value for w, we have inferred a distribution over all possible values. In effect, we have updated our prior ‘belief’ in the parameter values in light of the information provided by the data t, with more posterior probability assigned to values which are both probable under the prior and which ‘explain the data’.
MAP Estimation: A ‘Bayesian’ Short-Cut. The “maximum a posteriori ” (MAP) estimate for w is the single most probable value under the posterior distribution p(w|t, α, σ2). Since the denominator in Bayes’ rule (11) earlier is independent of w, this is equivalent to maximising the numerator, or equivalently minimising EMAP (w) = − log p(t|w, σ2) − log p(w|α). Retaining only those terms dependent on w gives:
EMAP (w) = 1
2σ2
N ∑
n=1
{tn − y(xn; w)}2 + α
2
M ∑
m=1
w2m. (14)
The MAP estimate is therefore identical to the PLS estimate with λ = σ2α.


48 M.E. Tipping
Illustration of Sequential Bayesian Inference. For our example problem, we’ll end this section by looking at how the posterior p(w|t, α, σ2) evolves as we observe increasingly more data points tn. Before proceeding, we note that we can compute the posterior incrementally since here the data are assumed independent (conditioned on w). e.g. for t = (t1, t2, t3):
p(w|t1, t2, t3) ∝ p(t1, t2, t3|w) p(w),
= p(t2, t3|w) p(t1|w) p(w), = Likelihood of (t2, t3) × posterior having observed t1.
So, more generally, we can treat the posterior having observed (t1, . . . , tK) as the ‘prior’ for the remaining data (tK+1, . . . , tN ) and obtain the equivalent result to seeing all the data at once. We exploit this result in Figure 4 where we illustrate how the posterior distribution updates with increasing amounts of data. The second row in Figure 4 illustrates some relevant points. First, because the data observed up to that point are not generally near the centres of the two basis functions visualised, those values of x are relatively uninformative regarding the associated weights and the posterior thereover has not deviated far from the prior. Second, on the far right in the second row, we can see that the function is fairly well determined in the vicinity of the observations, but at higher values of x, where data are yet to be observed, the MAP estimate of the function is not accurate and the posterior samples there exhibit high variance. On the third row we have observed all data, and notice that although the MAP predictor appears subjectively good, the posterior still seems quite diffuse and the variance in the samples is noticeable. We emphasise this point in the bottom row, where we have generated and observed an extra 200 data points and it can be seen how the posterior is now much more concentrated, and samples from it are now quite closely concentrated about the MAP value. Note that this facility to sample from the prior or posterior is a very informative feature of the Bayesian paradigm. For the posterior, it is a helpful way of visualising the remaining uncertainty in parameter estimates in cases where the posterior distribution itself cannot be visualised. Furthermore, the ability to visualise samples from the prior alone is very advantageous, as it offers us evidence to judge the appropriateness of our prior assumptions. No equivalent facility exists within the regularisation or penalty function framework.
3 Marginalisation and Ockham’s Razor
Since we have just seen that the maximum a posteriori (MAP) and penalised least-squares (PLS) estimates are equivalent, it might be tempting to assume that the Bayesian framework is simply a probabilistic re-interpretation of classical methods. This is certainly not the case! It is sometimes overlooked that the distinguishing element of Bayesian methods is really marginalisation, where instead of seeking to ‘estimate’ all ‘nuisance’ variables in our models, we attempt to integrate them out. As we will now see, this is a powerful component of the Bayesian framework.


Bayesian Inference: Principles and Practice in Machine Learning 49
−1.5
−1
−0.5
0
0.5
1
1.5 Data and Basis Functions
basis functions −1 0 1
−1
−0.5
0
0.5
1
Prior over w10, w11
−2
−1
0
1
2 Samples from Prior
−1.5
−1
−0.5
0
0.5
1
1.5
basis functions −1 0 1
−1
−0.5
0
0.5
1 Posterior Distribution
0246
−1.5
−1
−0.5
0
0.5
1
1.5 Samples from Posterior
−1.5
−1
−0.5
0
0.5
1
1.5
basis functions −1 0 1
−1
−0.5
0
0.5
1
wMAP
0246
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
basis functions −1 0 1
−1
−0.5
0
0.5
1
0246
−1.5
−1
−0.5
0
0.5
1
1.5
Fig. 4. Illustration of the evolution of the posterior distribution as data is sequentially ‘absorbed’. The left column shows the data, with those points which have been observed so far crossed, along with a plot of the basis functions. The contour plots in the middle column show the prior/posterior over just two (for visualisation purposes) of the weights, w10 and w11, corresponding to the highlighted basis functions on the left. The right hand column plots y(x; w) from a number of samples of w from the full prior/posterior, along with the posterior mean, or MAP, estimator (in thicker green/gray). From top to bottom, the number of data is increasing. Row 1 shows the a priori case for no data, row 2 shows the model after 8 examples, and row 3 shows the model after all 15 data points have been observed. Finally, the bottom row shows the case when an additional 200 data points have been generated and absorbed in the posterior model


50 M.E. Tipping
3.1 Making Predictions
First lets reiterate some of the previous section and consider how, having ‘learned’ from the training values t, we make a prediction for the value of t∗ given a new input datum x∗:
Framework Learned Quantity Prediction
Classical wP LS y(x∗; wP LS) MAP Bayesian p(w|t, α, σ2) p(t∗|wMAP , σ2) True Bayesian p(w|t, α, σ2) p(t∗|t, α, σ2)
The first two approaches result in similar predictions, although the MAP Bayesian model does give a probability distribution for t∗ (which can be sampled from, e.g. see Figure 4). The mean of this distribution is the same as that of the classical predictor y(x∗; wP LS), since wMAP = wP LS. However, the ‘true Bayesian’ way is to integrate out, or marginalise over, the uncertain variables w in order to obtain the predictive distribution:
p(t∗|t, α, σ2) =
∫
p(t∗|w, σ2) p(w|t, α, σ2) dw. (15)
This distribution p(t∗|t, α, σ2) incorporates our uncertainty over the weights having seen t, by averaging the model probability for t∗ over all possible values of w. If we are unsure about the parameter settings, for example if there were very few data points, then p(w|t, α, σ2) and similarly p(t∗|t, α, σ2) will be appropriately diffuse. The classical, and even MAP Bayesian, predictions take no account of how well-determined our parameters w really are.
3.2 The General Bayesian Predictive Framework
You way well find the presence of α and σ2 as conditioning variables in the predictive distribution, p(t∗|t, α, σ2), in (15) rather disconcerting, and indeed, for any general model, if we wish to predict t∗ given some training data t, what we really, really want is p(t∗|t). That is, we wish to integrate out all variables not directly related to the task at hand. So far, we’ve only placed a prior over the weights w — to be truly, truly Bayesian, we should define p(α), a so-called hyperprior, along with a prior over the noise level p(σ2). Then the full posterior over ‘nuisance’ variables becomes:
p(w, α, σ2|t) = p(t|w, σ2)p(w|α)p(α)p(σ2)
p(t) . (16)
The denominator, or normalising factor, in (16) is the marginalised probability of the data:
p(t) =
∫
p(t|w, σ2)p(w|α)p(α)p(σ2) dw dα dσ2, (17)


Bayesian Inference: Principles and Practice in Machine Learning 51
and is nearly always analytically intractable to compute! Nevertheless, as we’ll soon see, p(t) is a very useful quantity and can be amenable to effective approximation.
3.3 Practical Bayesian Prediction
Given the full posterior (16), Bayesian inference in our example regression model would proceed with:
p(t∗|t) =
∫
p(t∗|w, σ2) p(w, α, σ2|t) dw dα dσ2, (18)
but as we indicated, we can’t compute either p(w, α, σ2|t) or p(t∗|t) analytically. If we wish to proceed, we must turn to some approximation strategy (and it is here that much of the Bayesian “voodoo” resides). A sensible approach might be to perform those integrations that are analytically computable, and then approximate remaining integrations, perhaps using one of a number of established methods:
– Type-II maximum likelihood (discussed shortly) – Laplace’s method (see, e.g., [2]) – Variational techniques (see, e.g., [3, 4]) – Sampling (e.g. [2, 5])
Much research in Bayesian inference has gone, and continues to go, into the development and assessment of approximation techniques, including those listed above. For the purposes of this article, we will primarily exploit the first of them.
3.4 A Type-II Maximum Likelihood Approximation
Here, using the product rule of probability, we can rewrite the ideal full posterior p(w, α, σ2|t) as:
p(w, α, σ2|t) ≡ p(w|t, α, σ2) p(α, σ2|t). (19)
The first term is our earlier weight posterior which we have already computed: p(w|t, α, σ2) ∼ N (μ, Σ). The second term p(α, σ2|t) we will approximate, admittedly crudely, by a δ-function at its mode. i.e. we find “most probable” values αMP and σ2MP which maximise:
p(α, σ2|t) = p(t|α, σ2) p(α) p(σ2)
p(t) . (20)
Since the denominator is independent of α and σ2, we only need maximise the numerator p(t|α, σ2)p(α)p(σ2). Furthermore, if we assume flat, uninformative, priors over log α and log σ, then we equivalently just need to find the maximum of p(t|α, σ2). Assuming a flat prior here may seem to be a computational convenience, but in fact it is arguably our prior of choice since our model will be invariant to the scale of the target data (and basis set), which is almost always


52 M.E. Tipping
an advantageous feature4. For example, our results won’t change if we measure t in metres instead of miles. We’ll return to the task of maximising p(t|α, σ2) in Section 3.6.
3.5 The Approximate Predictive Distribution
Having found αMP and σ2MP, our approximation to the predictive distribution
would be:
p(t∗|t) =
∫
p(t∗|w, σ2) p(w|t, α, σ2) p(α, σ2|t) dw dα dσ2,
≈
∫
p(t∗|w, σ2) p(w|t, α, σ2) δ(αMP, σ2MP) dw dα dσ2,
=
∫
p(t∗|w, σ2MP) p(w|t, αMP, σ2MP) dw. (21)
In our example earlier, recall that p(w|t, αMP, σ2MP) ∼ N (μ, Σ), from which
the approximate predictive distribution can be finally written as:
p(t∗|t) ≈
∫
p(t∗|w, σ2MP) p(w|t, αMP, σ2MP) dw. (22)
This is now computable and is Gaussian: N (μ∗, σ∗2), with:
μ∗ = y(x∗; μ),
σ∗2 = σ2MP + f TΣf ,
where f = [φ1(x∗), . . . , φM (x∗)]T. Intuitively, we see that
– the mean predictor μ∗ is the model function evaluated with the posterior mean weights (the same as the MAP prediction), – the predictive variance σ∗2 is the sum of variances associated with both the noise process and the uncertainty of the weight estimates. In particular, it can be clearly seen that when the posterior over w is more diffuse, and Σ is larger, σ∗2 is also increased.
3.6 Marginal Likelihood
Returning now to the question of finding αMP and σ2MP, as noted earlier we find
the maximising values of the ‘marginal likelihood’ p(t|α, σ2). This is given by:
4 Note that for scale parameters such as α and σ2, it can be shown that it is appropriate to define uniformative priors uniformly over a logarithmic scale [6]. While for brevity we will continue to denote parameters “α” and “σ”, from now on we will work with the logarithms thereof, and in particular, will maximise distributions with respect to log α and log σ. In this respect, one must note with caution that finding the maximum of a distribution with respect to parameters is not invariant to transformations of those parameters, whereas the result of integration with respect to transformed distributions is invariant.


Bayesian Inference: Principles and Practice in Machine Learning 53
p(t|α, σ2) =
∫
p(t|w, σ2) p(w|α) dw,
= (2π)−N/2|σ2I + α−1ΦΦT|−1/2 exp
{
−1
2 tT(σ2I + α−1ΦΦT)−1t
} .
(23)
This is a Gaussian distribution over the single N -dimensional dataset vector t, and (23) is readily evaluated for arbitrary values of α (and σ2). Note that here we can use all the data to directly determine αMP and σ2MP — we don’t
need to reserve a separate data set to validate their values. We can use gradientbased techniques to maximise (23) (and we will do so for a similar quantity in Section 4), but here we choose to repeat the earlier experiment for the regularised linear model. While we fix σ2 (though we could also experimentally evaluate it), in Figure 5 we have computed the marginal likelihood (in fact, its negative logarithm) at a number of different values of α (for just the 15-example training set, though we could also have made use of the validation set too) and compared with the training, validation and test errors of Figure 3 earlier.
It is quite striking that using only 15 examples and no validation data, the Bayesian approach for setting α (giving test error 1.66) finds a closer model to the ‘truth’ than the classical model with its validated value of λ (test error 2.33). It is also interesting to see, and is it not immediately obvious why, that the marginal likelihood measure, although only measured on the training data, is not monotonic (unlike training error) and exhibits a maximum at some intermediate complexity level. The marginal likelihood criterion appears to be successfully penalising both models that are too simple and too complex — this is “Ockham’s Razor” at work.
−14 −12 −10 −8 −6 −4 −2 0 2 4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Training
Validation
Test
Marginal likelihood
log α
Normalised error
Fig. 5. Plots of the training, validation and test errors of the model as shown in Figure 3 (with the horizontal scale adjusted appropriately to convert from λ to α) along with the negative log marginal likelihood evaluated on the training data alone for that same model. The values of α and test error achieved by the model with highest marginal likelihood (smallest negative log) are indicated


54 M.E. Tipping
3.7 Ockham’s Razor
In the fourteenth century, William of Ockham proposed:
“Pluralitas non est ponenda sine neccesitate”
which literally translates as “entities should not be multiplied unnecessarily”. Its original historic context was theological, but the concept remains relevant for machine learning today, where it might be translated as “models should be no more complex than is sufficient to explain the data”. The Bayesian procedure is effectively implementing “Ockham’s Razor” by assigning lower probability both to models that are too simple and too complex. We might ask: why is an intermediate value of α preferred? The schematic of Figure 6 shows how this can be the case, as a result of the marginal likelihood p(t|α) being a normalised distribution over the space of all possible data sets t. Models with high α only fit (assign significant marginal probability to) data from smooth functions. Models with low values of α can fit data generated from functions that are both smooth and complex. However, because of normalisation, the low-α model must generally assign lower probability to data from smooth functions, so the marginal likelihood naturally prefers the simpler model if the data is smooth, which is precisely the meaning of Ockham’s Razor. Furthermore, one can see from Figure 6 that for a data set of ‘intermediate’ complexity, a ‘medium’ value of α can be preferred. This is qualitatively analogous to the case of our example set, where we indeed find that an intermediate value of α is optimal. Note, crucially, that this is achieved without any prior preference for any particular value of α as we originally assumed
Fig. 6. A schematic plot of three marginal probability distributions for ‘high’, ‘medium’ and ‘low’ values of α. The figure is a simplification of the case for the actual distribution p(t|α), where for illustrative purposes the N -dimensional space of t has been compressed onto a single axis and where, notionally, data sets (instances of t) arising from simpler (smoother) functions lie towards the left-hand end of the horizontal scale, and data from complex functions to the right


Bayesian Inference: Principles and Practice in Machine Learning 55
a uniform hyperprior over its logarithm. The effect of Ockham’s Razor is an automatic and pleasing consequence of applying the Bayesian framework.
3.8 Model Selection
While we have concentrated so far on the search for an appropriate value of hyperparameter α (and, to an extent, σ2), our model is also conditioned on other variables we have up to now overlooked: the choice of basis set Φ and, for our Gaussian basis, its width parameter r (as defined in Section 2.1). Ideally, we should define priors P (Φ) and p(r), and integrate out those variables when making predictions. More practically, we could use p(t|Φ, r) as a criterion for model selection with the expectation that Ockham’s Razor will assist us in selecting a model that is sufficient to explain the data but is not over-complex. In our example model, we previously optimised the marginal likelihood to find a value for α. In fact, as there are only two nuisance parameters here, it is feasible to integrate out α and σ2 numerically. In Figure 7 we evaluate several basis sets Φ and width values r by computing the integral
p(t|Φ, r) =
∫
p(t|α, σ2, Φ, r) p(α) p(σ2) dα dσ2, (24)
≈1
S
S ∑
s=1
p(t|αs, σs2, Φ, r), (25)
with a Monte-Carlo average where we obtain S samples log-uniformly from α ∈ [10−12, 1012] and σ ∈ [10−4, 100]. The results of Figure 7 are quite compelling: with uniform priors over all nuisance variables —i.e. we have imposed absolutely no prior knowledge — we observe that test error appears very closely related to marginal likelihood. The qualitative shapes of the curves, and the relative merits, of Gaussian and Laplacian basis functions are also captured. For the Gaussian basis we are very close to obtaining the optimal value of r, in terms of test error, from just 15 examples and no validation data. Reassuringly, the simplest model that contains the ‘truth’, y = w1 sin(x), is the most probable model here. We also show in the figure the model y = w1 sin(x)+w2 cos(x) which is also an ideal fit for the data, but it is penalised in marginal probability terms since the addition of the w2 cos(x) term allows it to explain more data sets, and normalisation thus requires it to assign less probability to our particular set. Nevertheless, it is still some orders of magnitude more probable than the Gaussian basis model.
3.9 Summary So Far. . .
Marginalisation is the key element of Bayesian inference, and hopefully some of the examples above have persuaded the reader that it can be an exceedingly powerful one. Problematically though, ideal Bayesian inference proceeds by integrating out all irrelevant variables, and we must concede that


56 M.E. Tipping
sin+cos sin 0 1 2 3 4 5
−2
0
2
4
6
8
10
12
Gaussian
Laplacian
−log p( t|Φ,r)
basis width r
sin+cos sin 0 1 2 3 4 5
0
1
2
3
4
5
Gaussian
Laplacian
basis width r
Test Error
Fig. 7. Top: negative log model probability − log p(t|Φ, r) for various basis sets, evaluated by analytic integration over w and Monte-Carlo averaging over α and σ2. Bottom: corresponding test error for the posterior mean predictor. Basis sets examined were ‘Gaussian’, exp {−|x − xm|2/r2}, ‘Laplacian’, exp {−|x − xm|/r}, sin(x), sin(x) with cos(x). For the Gaussian and Laplacian basis, the horizontal axis denotes varying ‘width’ parameter r shown. For the sine/cosine bases, the horizontal axis has no significance and the values are placed to the left for convenience
– for practical purposes, it may be appropriate to require point estimates of some ‘nuisance’ variables, since it could easily be impractical to average over many parameters and particularly models every time we wish to make a prediction (imagine, for example, running a handwriting recogniser on a portable computing device), – many of the desired integrations necessitate some form of approximation.
Nevertheless, regarding these points, we can still leverage Bayesian techniques to considerable benefit exploiting carefully-applied approximations. In particular, marginalised likelihoods within the Bayesian framework allow us to estimate fixed values of hyperparameters where desired and, most beneficially, choose between models and their varying parameterisations. This can all be done without the need to use validation data. Furthermore:
– it is straightforward to estimate other parameters in the model that may be of interest, e.g. the noise variance, – we can sample from both prior and posterior models of the data, – the exact parameterisation of the model is irrelevant when integrating out, – we can incorporate other priors of interest in a principled manner.


Bayesian Inference: Principles and Practice in Machine Learning 57
We now further demonstrate these points, notably the last one, in the next section where we present a practical framework for the inference of ‘sparse’ models.
4 Sparse Bayesian Models
4.1 Bayes and Contemporary Machine Learning
In the previous section we saw that marginalisation is a valuable component of the Bayesian paradigm which offers a number of advantageous features applicable to many data modelling tasks. Disadvantageously, we also saw that the integrations required for full Bayesian inference can often be analytically intractable, although approximations for simple linear models could be very effective. Historically, interest in Bayesian “machine learning” (but not statistics!) has focused on approximations for non-linear models, e.g. for neural networks, the “evidence procedure” [7] and “hybrid Monte Carlo” sampling [5]. More recently, flexible (i.e. many-parameter) linear kernel methods have attracted much renewed interest, thanks mainly to the popularity of the “support vector machine”. These kind of models, of course, are particularly amenable to Bayesian techniques.
Linear Models and Sparsity. Much interest in linear models has focused on sparse learning algorithms, which set many weights wm to zero in the estimated
predictor function y(x) = ∑
m wmφm(x). Sparsity is an attractive concept; it
offers elegant complexity control, feature extraction, the potential for elucidation of meaningful input variables along with the practical benefits of computational speed and compactness. How do we impose a preference for sparsity in a model? The most common approach is via an appropriate regularisation term or prior. The most common
regularisation term that we have already met, EW (w) = ∑M
m=1 |wm|2, of course
corresponds to a Gaussian prior and is easy to work with, but while it is an effective way to control complexity, it does not promote sparsity. In the regularisation
sense, the ‘correct’ term would be EW (w) = ∑
m |wm|0, but this, being discon
tinuous in wm, is very difficult to work with. Instead, EW (w) = ∑
m |wm|1 is a
workable compromise which gives reasonable sparsity and reasonable tractability, and is exploited in a number of methods, including as a Laplacian prior
p(w) ∝ exp(− ∑
m |wm|) [8]. However, there is an arguably more elegant way
of obtaining sparsity within a Bayesian framework that builds effectively on the ideas outlined in the previous section and we conclude this article with a brief outline thereof.
4.2 A Sparse Bayesian Prior
In fact, we can obtain sparsity by retaining the traditional Gaussian prior, which is great news for tractability. The modification to our earlier Gaussian prior (10) is subtle:
p(w|α1, . . . , αM ) =
M ∏
m=1
[
(2π)−1/2αm1/2 exp
{
−1
2 αmw2m
}]
. (26)


58 M.E. Tipping
In contrast to the model in Section 2, we now have M hyperparameters α = (α1, . . . , αM ), one αm independently controlling the (inverse) variance of each weight wm.
A Hierarchical Prior. The prior p(w|α) is nevertheless still Gaussian, and superficially seems to have little preference for sparsity. However, it remains conditioned on α, so for full Bayesian consistency we should now define hyperpriors over all αm. Previously, we utilised a log-uniform hyperprior — this is a special case of a Gamma hyperprior, which we introduce for greater generality here. This combination of the prior over αm controlling the prior over wm gives us what is often referred to as a hierarchical prior. Now, if we have p(wm|αm) and p(αm) and we want to know the ‘true’ p(wm) we already know what to do — we must marginalise:
p(wm) =
∫
p(wm|αm) p(αm) dαm. (27)
For a Gamma p(αm), this integral is computable and we find that p(wm) is a Student-t distribution illustrated as a function of two parameters in Figure 8;
its equivalent as a regularising penalty function would be ∑
m log |wm|.
4.3 A Sparse Bayesian Model for Regression
We can develop a sparse regression model by following an identical methodology to the previous sections. Again, we assume independent Gaussian noise: tn ∼
N (y(xn; w), σ2), which gives a corresponding likelihood:
p(t|w, σ2) = (2πσ2)–N/2 exp
{
−1
2σ2 ‖t − Φw‖2
}
, (28)
where as before we denote t = (t1 . . . tN )T, w = (w1 . . . wM )T, and Φ is the N × M ‘design’ matrix with Φnm = φm(xn).
Gaussian prior Marginal prior: single α Independent α
Fig. 8. Contour plots of Gaussian and Student-t prior distributions over two parameters. While the marginal prior p(w1, w2) for the ‘single’ hyperparameter model of Section 2 has a much sharper peak than the Gaussian at zero, it can be seen that it is not sparse unlike the multiple ‘independent’ hyperparameter prior, which as well as having a sharp peak at zero, places most of its probability mass along axial ridges where the magnitude of one of the two parameters is small


Bayesian Inference: Principles and Practice in Machine Learning 59
Following the Bayesian framework, we desire the posterior distribution over all unknowns:
p(w, α, σ2|t) = p(t|w, α, σ2)p(w, α, σ2)
p(t) , (29)
which we can’t compute analytically. So as previously, we decompose this as:
p(w, α, σ2|t) ≡ p(w|t, α, σ2) p(α, σ2|t) (30)
where p(w|t, α, σ2) is the ‘weight posterior’ distribution, and is tractable. This leaves p(α, σ2|t) which must be approximated.
The Weight Posterior Term. Given the data, the posterior distribution over weights is Gaussian:
p(w|t, α, σ2) = p(t|w, σ2) p(w|α)
p(t|α, σ2) ,
= (2π)–(N+1)/2|Σ|–1/2 exp
{
−1
2 (w − μ)TΣ–1(w − μ)
}
, (31)
with
Σ = (σ–2ΦTΦ + A)–1, (32)
μ = σ–2ΣΦTt, (33)
and where we collect all the hyperparameters into a diagonal matrix: A = diag(α1, α2, . . . , αM ). A key point to note from (31–33) is that if any αm = ∞, the corresponding μm = 0.
The Hyperparameter Posterior Term. Again we will adopt the “type-II maximum likelihood” approximation where we maximise p(t|α, σ2) to find αMP
and σ2MP. As before, for uniform hyperpriors over log α and log σ, p(α, σ2|t) ∝
p(t|α, σ2), where the marginal likelihood p(t|α, σ2) is obtained by integrating out the weights:
p(t|α, σ2) =
∫
p(t|w, σ2) p(w|α) dw,
= (2π)–N/2|σ2I + ΦA−1ΦT|–1/2 exp
{
−1
2 tT(σ2I + ΦA–1ΦT)–1t
} .
(34)
In Section 2, we found the single αMP empirically but here for multiple (in practice, perhaps thousands of) hyperparameters, we cannot experimentally explore the space of possible α so we instead optimise p(t|α, σ2) directly, via a gradient-based approach.


60 M.E. Tipping
Hyperparameter Re-estimation. Differentiating log p(t|α, σ2) with respect to α and σ2, setting to zero and rearranging (see [9]) ultimately gives iterative re-estimation formulae:
αinew = γi
μi2
, (35)
(σ2)new = ‖t − Φμ‖2
N − ∑M
i=1 γi
. (36)
For convenience we have defined
γi = 1 − αiΣii, (37)
where γi ∈ [0, 1] is a measure of ‘well-determinedness’ of parameter wi. This quantity effectively captures the influence of the likelihood (total when γ → 1) and the prior (total when γ → 0) on the value of each wi. Note that the quantities on the right-hand-side of equations (35–37) are computed using the ‘old’ values of α and σ2.
Summary of Inference Procedure. We’re now in a position to define a ‘learning algorithm’ for approximate Bayesian inference in this model:
1. Initialise all {αi} and σ2 (or fix latter if known) 2. Compute weight posterior sufficient statistics μ and Σ 3. Compute all {γi}, then re-estimate {αi} (and σ2 if desired) 4. Repeat from 2. until convergence 5. ‘Delete’ weights (and basis functions) for which optimal αi = ∞, since this implies μi = 0 6. Make predictions for new data via the predictive distribution computed with the converged αMP and σ2MP:
p(t∗|t) =
∫
p(t∗|w, σ2MP) p(w|t, αMP, σ2MP) dw (38)
the mean of which is y(x∗; μ)
Step 5. rather ideally assumes that we can reliably estimate such large values of α, whereas in reality limited computational precision implies that in this algorithm we have to place some finite upper limit on α (e.g. 1012 times the value of the smallest α). In many real-world tasks, we do indeed find that many αi do tend towards infinity, and we converge toward a model that is very sparse, even if M is very large.
4.4 The “Relevance Vector Machine” (RVM)
To give an example of the potential of the above model, we briefly introduce here the “Relevance Vector Machine” (RVM), which is simply a specialisation


Bayesian Inference: Principles and Practice in Machine Learning 61
of a sparse Bayesian model which utilises the same data-dependent kernel basis as the popular “support vector machine” (SVM):
y(x; w) =
N ∑
n=1
wnK(x, xn) + w0 (39)
This model is described, with a number of examples, in much more detail elsewhere [9]. For now, Figure 9 provides an illustration, on some noise-polluted synthetic data, of the potential of this Bayesian framework for effectively combining sparsity with predictive accuracy.
Max error: 0.0664 RMS error: 0.0322 RV’s: 7
Relevance Vector Regression
Noise: 0.100 Estimate: 0.107
Max error: 0.0896 RMS error: 0.0420 SV’s: 29
Support Vector Regression
Noise: 0.100
C and ε found by cross−validation
Fig. 9. The relevance vector and support vector machines applied to a regression problem using a Gaussian kernel, which demonstrates some of the advantages of the Bayesian approach. Of particular note is the sparsity of the final Bayesian model, which qualitatively appears near-optimal. It is also worth underlining that the ‘nuisance’ parameters C and ǫ for the SVM had to be found by a separate cross-validation procedure, whereas the RVM algorithm estimates them automatically, and arguably quite accurately in the case of the noise variance
5 Summary
While the tone of the first three sections of this article has been introductory and the models considered therein have been quite simplistic, the brief example of the ‘sparse Bayesian’ learning procedure given in Section 4 is intended to demonstrate that ‘practical’ Bayesian inference procedures have the potential to be highly effective in the context of modern machine learning. Readers who find this demonstration sufficiently convincing and who are interested specifically in the sparse Bayesian model framework can find further information (including some implementation code), and details of related approaches, at a web-page maintained by the author: http://www.research.microsoft.com/mlp/RVM. In


62 M.E. Tipping
particular, note that the algorithm for hyperparameter estimation of Section 4.3 was presented here as it has a certain intuitive simplicity, but in fact there is a much more efficient and practical approach to optimising log p(t|α, σ2) which is detailed in [10]. We summarised some of the features, advantages and limitations of the general Bayesian framework earlier in Section 3.9, and so will not repeat them here. The reader interested in investigating further and in more depth on this general topic may find much helpful further material in the references [1, 5, 11, 12, 13, 14].
References
1. Jaynes, E.T.: Probability theory: the logic of science. Cambridge University Press (2003) 2. Evans, M., Swartz, T.B.: Methods for approximating integrals in statistics with special emphasis on Bayesian integration. Statistical Science 10 (1995) 254–272 3. Beal, M., Ghahramani, Z.: The Variational Bayes web site at http://www.variational-bayes.org/ (2003)
4. Bishop, C.M., Tipping, M.E.: Variational relevance vector machines. In Boutilier, C., Goldszmidt, M., eds.: Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann (2000) 46–53 5. Neal, R.M.: Bayesian Learning for Neural Networks. Springer (1996) 6. Berger, J.O.: Statistical decision theory and Bayesian analysis. Second edn. Springer (1985) 7. MacKay, D.J.C.: The evidence framework applied to classification networks. Neural Computation 4 (1992) 720–736 8. Williams, P.M.: Bayesian regularisation and pruning using a Laplace prior. Neural Computation 7 (1995) 117–143 9. Tipping, M.E.: Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learing Research 1 (2001) 211–244 10. Tipping, M.E., Faul, A.C.: Fast marginal likelihood maximisation for sparse Bayesian models. In Bishop, C.M., Frey, B.J., eds.: Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics, Key West, FL, Jan 3-6. (2003) 11. MacKay, D.J.C.: Bayesian interpolation. Neural Computation 4 (1992) 415–447 12. Bishop, C.M.: Neural Networks for Pattern Recognition. Oxford University Press (1995) 13. Gelman, A., Carlin, J.B., Stern, H.S., Rubin, D.B.: Bayesian Data Analysis. Chapman & Hall (1995) 14. MacKay, D.J.C.: Information Theory, Inference and Learning Algorithms. Cambridge University Press (2003)


Gaussian Processes in Machine Learning
Carl Edward Rasmussen
Max Planck Institute for Biological Cybernetics, 72076 T ̈ubingen, Germany carl@tuebingen.mpg.de http://www.tuebingen.mpg.de/∼carl
Abstract. We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.
Supervised learning in the form of regression (for continuous outputs) and classification (for discrete outputs) is an important constituent of statistics and machine learning, either for analysis of data sets, or as a subgoal of a more complex problem. Traditionally parametric1 models have been used for this purpose. These have a possible advantage in ease of interpretability, but for complex data sets, simple parametric models may lack expressive power, and their more complex counterparts (such as feed forward neural networks) may not be easy to work with in practice. The advent of kernel machines, such as Support Vector Machines and Gaussian Processes has opened the possibility of flexible models which are practical to work with. In this short tutorial we present the basic idea on how Gaussian Process models can be used to formulate a Bayesian framework for regression. We will focus on understanding the stochastic process and how it is used in supervised learning. Secondly, we will discuss practical matters regarding the role of hyperparameters in the covariance function, the marginal likelihood and the automatic Occam’s razor. For broader introductions to Gaussian processes, consult [1], [2].
1 Gaussian Processes
In this section we define Gaussian Processes and show how they can very naturally be used to define distributions over functions. In the following section we continue to show how this distribution is updated in the light of training examples.
1 By a parametric model, we here mean a model which during training “absorbs” the information from the training data into the parameters; after training the data can be discarded.
O. Bousquet et al. (Eds.): Machine Learning 2003, LNAI 3176, pp. 63–71, 2004. c© Springer-Verlag Berlin Heidelberg 2004


64 C.E. Rasmussen
Definition 1. A Gaussian Process is a collection of random variables, any finite number of which have (consistent) joint Gaussian distributions.
A Gaussian process is fully specified by its mean function m(x) and covariance function k(x, x′). This is a natural generalization of the Gaussian distribution whose mean and covariance is a vector and matrix, respectively. The Gaussian distribution is over vectors, whereas the Gaussian process is over functions. We will write:
f ∼ GP(m, k), (1)
meaning: “the function f is distributed as a GP with mean function m and covariance function k”. Although the generalization from distribution to process is straight forward, we will be a bit more explicit about the details, because it may be unfamiliar to some readers. The individual random variables in a vector from a Gaussian distribution are indexed by their position in the vector. For the Gaussian process it is the argument x (of the random function f (x)) which plays the role of index set: for every input x there is an associated random variable f (x), which is the value of the (stochastic) function f at that location. For reasons of notational convenience, we will enumerate the x values of interest by the natural numbers, and use these indexes as if they were the indexes of the process – don’t let yourself be confused by this: the index to the process is xi, which we have chosen to index by i. Although working with infinite dimensional objects may seem unwieldy at first, it turns out that the quantities that we are interested in computing, require only working with finite dimensional objects. In fact, answering questions about the process reduces to computing with the related distribution. This is the key to why Gaussian processes are feasible. Let us look at an example. Consider the Gaussian process given by:
f ∼ GP(m, k), where m(x) = 1
4 x2, and k(x, x′) = exp(− 1
2 (x − x′)2). (2)
In order to understand this process we can draw samples from the function f . In order to work only with finite quantities, we request only the value of f at a distinct finite number n of locations. How do we generate such samples? Given the x-values we can evaluate the vector of means and a covariance matrix using Eq. (2), which defines a regular Gaussian distribution:
μi = m(xi) = 1
4 xi2, i = 1, . . . , n and
Σij = k(xi, xj) = exp(− 1
2 (xi − xj)2), i, j = 1, . . . , n, (3)
where to clarify the distinction between process and distribution we use m and k for the former and μ and Σ for the latter. We can now generate a random vector from this distribution. This vector will have as coordinates the function values f (x) for the corresponding x’s:
f ∼ N (μ, Σ). (4)


Gaussian Processes in Machine Learning 65
−5 −4 −3 −2 −1 0 1 2 3 4 5
−2
0
2
4
6
8
Fig. 1. Function values from three functions drawn at random from a GP as specified in Eq. (2). The dots are the values generated from Eq. (4), the two other curves have (less correctly) been drawn by connecting sampled points. The function values suggest a smooth underlying function; this is in fact a property of GPs with the squared exponential covariance function. The shaded grey area represent the 95% confidence intervals
We could now plot the values of f as a function of x, see Figure 1. How can we do this in practice? Below are a few lines of Matlab2 used to create the plot:
xs = (-5:0.2:5)’; ns = size(xs,1); keps = 1e-9; m = inline(’0.25*x.^2’); K = inline(’exp(-0.5*(repmat(p’’,size(q))-repmat(q,size(p’’))).^2)’); fs = m(xs) + chol(K(xs,xs)+keps*eye(ns))’*randn(ns,1); plot(xs,fs,’.’)
In the above example, m and k are mean and covariances; chol is a function to compute the Cholesky decomposition3 of a matrix. This example has illustrated how we move from process to distribution and also shown that the Gaussian process defines a distribution over functions. Up until now, we have only been concerned with random functions – in the next section we will see how to use the GP framework in a very simple way to make inferences about functions given some training examples.
2 Posterior Gaussian Process
In the previous section we saw how to define distributions over functions using GPs. This GP will be used as a prior for Bayesian inference. The prior does not depend on the training data, but specifies some properties of the functions; for
2 Matlab is a trademark of The MathWorks Inc. 3 We’ve also added a tiny keps multiple of the identity to the covariance matrix for numerical stability (to bound the eigenvalues numerically away from zero); see comments around Eq. (8) for a interpretation of this term as a tiny amount of noise.


66 C.E. Rasmussen
example, in Figure 1 the function is smooth, and close to a quadratic. The goal of this section is to derive the simple rules of how to update this prior in the light of the training data. The goal of the next section is to attempt to learn about some properties of the prior4 in the the light of the data. One of the primary goals computing the posterior is that it can be used to make predictions for unseen test cases. Let f be the known function values of the training cases, and let f∗ be a set of function values corresponding to the test set inputs, X∗. Again, we write out the joint distribution of everything we are interested in: [f
f∗
]
∼N
([μ
μ∗
]
,
[Σ Σ∗ Σ∗⊤ Σ∗∗
])
, (5)
where we’ve introduced the following shorthand: μ = m(xi), i = 1, . . . , n for the training means and analogously for the test means μ∗; for the covariance we use Σ for training set covariances, Σ∗ for training-test set covariances and Σ∗∗ for test set covariances. Since we know the values for the training set f we are interested in the conditional distribution of f∗ given f which is expressed as5:
f∗|f ∼ N (μ∗ + Σ⊤
∗ Σ−1(f − μ), Σ∗∗ − Σ⊤
∗ Σ−1Σ∗
). (6)
This is the posterior distribution for a specific set of test cases. It is easy to verify (by inspection) that the corresponding posterior process is:
f |D ∼ GP(mD, kD),
mD(x) = m(x) + Σ(X, x)⊤Σ−1(f − m)
kD(x, x′) = k(x, x′) − Σ(X, x)⊤Σ−1Σ(X, x′),
(7)
where Σ(X, x) is a vector of covariances between every training case and x. These are the central equations for Gaussian process predictions. Let’s examine these equations for the posterior mean and covariance. Notice that the posterior variance kD(x, x) is equal to the prior variance k(x, x) minus a positive term, which depends on the training inputs; thus the posterior variance is always smaller than the prior variance, since the data has given us some additional information. We need to address one final issue: noise in the training outputs. It is common to many applications of regression that there is noise in the observations6. The most common assumption is that of additive i.i.d. Gaussian noise in the outputs.
4 By definition, the prior is independent of the data; here we’ll be using a hierarchical prior with free parameters, and make inference about the parameters. 5 the formula for conditioning a joint Gaussian distribution is:
[x
y
]
∼N
([a
b
]
,
[A C C⊤ B
])
=⇒ x|y ∼ N (a + CB−1(y − b), A − CB−1C⊤).
6 However, it is perhaps interesting that the GP model works also in the noise-free case – this is in contrast to most parametric methods, since they often cannot model the data exactly.


Gaussian Processes in Machine Learning 67
−5 −4 −3 −2 −1 0 1 2 3 4 5
−2
0
2
4
6
8
Fig. 2. Three functions drawn at random from the posterior, given 20 training data points, the GP as specified in Eq. (3) and a noise level of σn = 0.7. The shaded area gives the 95% confidence region. Compare with Figure 1 and note that the uncertainty goes down close to the observations
In the Gaussian process models, such noise is easily taken into account; the effect is that every f (x) has a extra covariance with itself only (since the noise is assumed independent), with a magnitude equal to the noise variance:
y(x) = f (x) + ε, ε ∼ N (0, σ2n),
f ∼ GP(m, k), y ∼ GP(m, k + σ2nδii′ ), (8)
where δii′ = 1 iff i = i′ is the Kronecker’s delta. Notice, that the indexes to the Kronecker’s delta is the identify of the cases, i, and not the inputs xi; you may have several cases with identical inputs, but the noise on these cases is assumed to be independent. Thus, the covariance function for a noisy process is the sum of the signal covariance and the noise covariance. Now, we can plug in the posterior covariance function into the little Matlab example on page 65 to draw samples from the posterior process, see Figure 2. In this section we have shown how simple manipulations with mean and covariance functions allow updates of the prior to the posterior in the light of the training data. However, we left some questions unanswered: How do we come up with mean and covariance functions in the first place? How could we estimate the noise level? This is the topic of the next section.
3 Training a Gaussian Process
In the previous section we saw how to update the prior Gaussian process in the light of training data. This is useful if we have enough prior information about a dataset at hand to confidently specify prior mean and covariance functions. However, the availability of such detailed prior information is not the typical case in machine learning applications. In order for the GP techniques to be of value in practice, we must be able to chose between different mean and covariance


68 C.E. Rasmussen
functions in the light of the data. This process will be referred to as training7 the GP model. In the light of typically vague prior information, we use a hierarchical prior, where the mean and covariance functions are parameterized in terms of hyperparameters. For example, we could use a generalization of Eq. (2):
f ∼ GP(m, k),
m(x) = ax2 + bx + c, and k(x, x′) = σy2 exp ( − (x − x′)2
2l2
) + σ2nδii′ , (9)
where we have introduced hyperparameters θ = {a, b, c, σy, σn, l}. The purpose of this hierarchical specification is that it allows us to specify vague prior information in a simple way. For example, we’ve stated that we believe the function to be close to a second order polynomial, but we haven’t said exactly what the polynomial is, or exactly what is meant by “close”. In fact the discrepancy between the polynomial and the data is a smooth function plus independent Gaussian noise, but again we’re don’t need exactly to specify the characteristic length scale l or the magnitudes of the two contributions. We want to be able to make inferences about all of the hyperparameters in the light of the data. In order to do this we compute the probability of the data given the hyperparameters. Fortunately, this is not difficult, since by assumption the distribution of the data is Gaussian:
L = log p(y|x, θ) = − 1
2 log |Σ| − 1
2 (y − μ)⊤Σ−1(y − μ) − n
2 log(2π). (10)
We will call this quantity the log marginal likelihood. We use the term “marginal” to emphasize that we are dealing with a non-parametric model. See e.g. [1] for the weight-space view of Gaussian processes which equivalently leads to Eq. (10) after marginalization over the weights. We can now find the values of the hyperparameters which optimizes the marginal likelihood based on its partial derivatives which are easily evaluated:
∂L ∂θm
= − (y − μ)⊤Σ−1 ∂m
∂θm
,
∂L ∂θk
=1
2 trace (Σ−1 ∂Σ
∂θk
)+ 1
2 (y − μ)⊤ ∂Σ
∂θk
Σ−1 ∂Σ
∂θk
(y − μ),
(11)
where θm and θk are used to indicate hyperparameters of the mean and covariance functions respectively. Eq. (11) can conveniently be used in conjunction
7 Training the GP model involves both model selection, or the discrete choice between different functional forms for mean and covariance functions as well as adaptation of the hyperparameters of these functions; for brevity we will only consider the latter here – the generalization is straightforward, in that marginal likelihoods can be compared.


Gaussian Processes in Machine Learning 69
−5 −4 −3 −2 −1 0 1 2 3 4 5
−2
0
2
4
6
8
Fig. 3. Mean and 95% posterior confidence region with parameters learned by maximizing marginal likelihood, Eq. (10), for the Gaussian process specification in Eq. (9), for the same data as in Figure 2. The hyperparameters found were a = 0.3, b = 0.03, c = −0.7, l = 0.7, σy = 1.1, σn = 0.25. This example was constructed so that the approach without optimization of hyperparameters worked reasonably well (Figure 2), but there is of course no guarantee of this in a typical application
with a numerical optimization routine such as conjugate gradients to find good8 hyperparameter settings. Due to the fact that the Gaussian process is a non-parametric model, the marginal likelihood behaves somewhat differently to what one might expect from experience with parametric models. Note first, that it is in fact very easy for the model to fit the training data exactly: simply set the noise level σ2n to zero, and the model produce a mean predictive function which agrees exactly with the training points. However, this is not the typical behavior when optimizing the marginal likelihood. Indeed, the log marginal likelihood from Eq. (10) consists of three terms: The first term, − 1
2 log |Σ| is a complexity penalty term, which
measures and penalizes the complexity of the model. The second term a negative quadratic, and plays the role of a data-fit measure (it is the only term which depends on the training set output values y). The third term is a log normalization term, independent of the data, and not very interesting. Figure 3 illustrates the predictions of a model trained by maximizing the marginal likelihood. Note that the tradeoff between penalty and data-fit in the GP model is automatic. There is no weighting parameter which needs to be set by some external method such as cross validation. This is a feature of great practical importance, since it simplifies training. Figure 4 illustrates how the automatic tradeoff comes about. We’ve seen in this section how we, via a hierarchical specification of the prior, can express prior knowledge in a convenient way, and how we can learn values of hyperparameters via optimization of the marginal likelihood. This can be done using some gradient based optimization. Also, we’ve seen how the marginal
8 Note, that for most non-trivial Gaussian processes, optimization over hyperparameters is not a convex problem, so the usual precautions against bad local minima should be taken.


70 C.E. Rasmussen
too simple
too complex
"just right"
All possible data sets
P(Y|Mi)
Y
Fig. 4. Occam’s razor is automatic. On the x-axis is an abstract representation of all possible datasets (of a particular size). On the y-axis the probability of the data given the model. Three different models are shown. A more complex model can account for many more data sets than a simple model, but since the probabilities have to integrate to unity, this means more complex models are automatically penalized more
likelihood automatically incorporates Occam’s razor; this property of of great practical importance, since it simplifies training a lot.
4 Conclusions and Future Directions
We’ve seen how Gaussian processes can conveniently be used to specify very flexible non-linear regression. We only mentioned in passing one type of covariance function, but in fact any positive definite function9 can be used as covariance function. Many such functions are known, and understanding the properties of functions drawn from GPs with particular covariance functions is an important ongoing research goal. When the properties of these functions are known, one will be able to chose covariance functions reflecting prior information, or alternatively, one will be able to interpret the covariance functions chosen by maximizing marginal likelihood, to get a better understanding of the data. In this short tutorial, we have only treated the simplest possible case of regression with Gaussian noise. In the case of non-Gaussian likelihoods (such as e.g. needed for classification) training becomes more complicated. One can resort to approximations, such as the Laplace approximation [3], or approximations based on projecting the non-Gaussian posterior onto the closest Gaussian (in a KL sense) [4] or sampling techniques [5].
9 The covariance function must be positive definite to ensure that the resulting covariance matrix is positive definite.


Gaussian Processes in Machine Learning 71
Another issue is the computational limitations. A straightforward implementation of the simple techniques explained here, requires inversion of the covariance matrix Σ, with a memory complexity of O(n2) and a computational complexity of O(n3). This is feasible on a desktop computer for dataset sizes of n up to a few thousands. Although there are many interesting machine learning problems with such relatively small datasets, a lot of current work is going into the development of approximate methods for larger datasets. A number of these methods rely on sparse approximations.
Acknowledgements
The author was supported by the German Research Council (DFG) through grant RA 1030/1.
References
1. Williams, C.K.I.: Prediction with Gaussian processes: From linear regression to linear prediction and beyond. In Jordan, M.I., ed.: Learning in Graphical Models. Kluwer Academic (1998) 599–621 2. MacKay, D.J.C.: Gaussian processes — a replacement for supervised neural networks? Tutorial lecture notes for NIPS 1997 (1997) 3. Williams, C.K.I., Barber, D.: Bayesian classification with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence 20(12) (1998) 13421351 4. Csat ́o, L., Opper, M.: Sparse on-line Gaussian processes. Neural Computation 14 (2002) 641–668 5. Neal, R.M.: Regression and classification using Gaussian process priors (with discussion). In Bernardo, J.M., et al., eds.: Bayesian statistics 6. Oxford University Press (1998) 475–501


Unsupervised Learning
Zoubin Ghahramani⋆
Gatsby Computational Neuroscience Unit, University College London, UK zoubin@gatsby.ucl.ac.uk http://www.gatsby.ucl.ac.uk/~zoubin
Abstract. We give a tutorial and overview of the field of unsupervised learning from the perspective of statistical modeling. Unsupervised learning can be motivated from information theoretic and Bayesian principles. We briefly review basic models in unsupervised learning, including factor analysis, PCA, mixtures of Gaussians, ICA, hidden Markov models, state-space models, and many variants and extensions. We derive the EM algorithm and give an overview of fundamental concepts in graphical models, and inference algorithms on graphs. This is followed by a quick tour of approximate Bayesian inference, including Markov chain Monte Carlo (MCMC), Laplace approximation, BIC, variational approximations, and expectation propagation (EP). The aim of this chapter is to provide a high-level view of the field. Along the way, many state-ofthe-art ideas and future directions are also reviewed.
1 Introduction
Machine learning is the field of research devoted to the formal study of learning systems. This is a highly interdisciplinary field which borrows and builds upon ideas from statistics, computer science, engineering, cognitive science, optimization theory and many other disciplines of science and mathematics. The purpose of this chapter is to introduce in a fairly concise manner the key ideas underlying the sub-field of machine learning known as unsupervised learning. This introduction is necessarily incomplete given the enormous range of topics under the rubric of unsupervised learning. The hope is that interested readers can delve more deeply into the many topics covered here by following some of the cited references. The chapter starts at a highly tutorial level but will touch upon state-of-the-art research in later sections. It is assumed that the reader is familiar with elementary linear algebra, probability theory, and calculus, but not much else.
1.1 What Is Unsupervised Learning?
Consider a machine (or living organism) which receives some sequence of inputs x1, x2, x3, . . ., where xt is the sensory input at time t. This input, which we will
⋆ The author is also at the Center for Automated Learning and Discovery, Carnegie Mellon University, USA.
O. Bousquet et al. (Eds.): Machine Learning 2003, LNAI 3176, pp. 72–112, 2004. c© Springer-Verlag Berlin Heidelberg 2004


Unsupervised Learning 73
often call the data, could correspond to an image on the retina, the pixels in a camera, or a sound waveform. It could also correspond to less obviously sensory data, for example the words in a news story, or the list of items in a supermarket shopping basket. One can distinguish between four different kinds of machine learning. In supervised learning the machine1 is also given a sequence of desired outputs y1, y2, . . . , and the goal of the machine is to learn to produce the correct output given a new input. This output could be a class label (in classification) or a real number (in regression). In reinforcement learning the machine interacts with its environment by producing actions a1, a2, . . .. These actions affect the state of the environment, which in turn results in the machine receiving some scalar rewards (or punishments) r1, r2, . . .. The goal of the machine is to learn to act in a way that maximizes the future rewards it receives (or minimizes the punishments) over its lifetime. Reinforcement learning is closely related to the fields of decision theory (in statistics and management science), and control theory (in engineering). The fundamental problems studied in these fields are often formally equivalent, and the solutions are the same, although different aspects of problem and solution are usually emphasized. A third kind of machine learning is closely related to game theory and generalizes reinforcement learning. Here again the machine gets inputs, produces actions, and receives rewards. However, the environment the machine interacts with is not some static world, but rather it can contain other machines which can also sense, act, receive rewards, and learn. Thus the goal of the machine is to act so as to maximize rewards in light of the other machines’ current and future actions. Although there is a great deal of work in game theory for simple systems, the dynamic case with multiple adapting machines remains an active and challenging area of research. Finally, in unsupervised learning the machine simply receives inputs x1, x2,. . ., but obtains neither supervised target outputs, nor rewards from its environment. It may seem somewhat mysterious to imagine what the machine could possibly learn given that it doesn’t get any feedback from its environment. However, it is possible to develop of formal framework for unsupervised learning based on the notion that the machine’s goal is to build representations of the input that can be used for decision making, predicting future inputs, efficiently communicating the inputs to another machine, etc. In a sense, unsupervised learning can be thought of as finding patterns in the data above and beyond what would be considered pure unstructured noise. Two very simple classic examples of unsupervised learning are clustering and dimensionality reduction. We discuss these in Section 2. The remainder of this chapter focuses on unsupervised learning,
1 Henceforth, for succinctness I’ll use the term machine to refer both to machines and living organisms. Some people prefer to call this a system or agent. The same mathematical theory of learning applies regardless of what we choose to call the learner, whether it is artificial or biological.


74 Z. Ghahramani
although many of the concepts discussed can be applied to supervised learning as well. But first, let us consider how unsupervised learning relates to statistics and information theory.
1.2 Machine Learning, Statistics, and Information Theory
Almost all work in unsupervised learning can be viewed in terms of learning a probabilistic model of the data. Even when the machine is given no supervision or reward, it may make sense for the machine to estimate a model that represents the probability distribution for a new input xt given previous inputs x1, . . . , xt−1 (consider the obviously useful examples of stock prices, or the weather). That is, the learner models P (xt|x1, . . . , xt−1). In simpler cases where the order in which the inputs arrive is irrelevant or unknown, the machine can build a model of the data which assumes that the data points x1, x2, . . . are
independently and identically drawn from some distribution P (x)2. Such a model can be used for outlier detection or monitoring. Let x represent patterns of sensor readings from a nuclear power plant and assume that P (x) is learned from data collected from a normally functioning plant. This model can be used to evaluate the probability of a new sensor reading; if this probability is abnormally low, then either the model is poor or the plant is behaving abnormally, in which case one may want to shut it down. A probabilistic model can also be used for classification. Assume P1(x) is a model of the attributes of credit card holders who paid on time, and P2(x) is a model learned from credit card holders who defaulted on their payments. By evaluating the relative probabilities P1(x′) and P2(x′) on a new applicant x′, the machine can decide to classify her into one of these two categories. With a probabilistic model one can also achieve efficient communication and data compression. Imagine that we want to transmit, over a digital communication line, symbols x randomly drawn from P (x). For example, x may be letters of the alphabet, or images, and the communication line may be the Internet. Intuitively, we should encode our data so that symbols which occur more frequently have code words with fewer bits in them, otherwise we are wasting bandwidth. Shannon’s source coding theorem quantifies this by telling us that the optimal number of bits to use to encode a symbol with probability P (x) is − log2 P (x). Using these number of bits for each symbol, the expected coding cost is the entropy of the distribution P .
H(P ) d=ef − ∑
x
P (x) log2 P (x) (1)
In general, the true distribution of the data is unknown, but we can learn a model of this distribution. Let’s call this model Q(x). The optimal code with
2 We will use both P and p to denote probability distributions and probability densities. The meaning should be clear depending on whether the argument is discrete or continuous.


Unsupervised Learning 75
respect to this model would use − log2 Q(x) bits for each symbol x. The expected coding cost, taking expectations with respect to the true distribution, is
−∑
x
P (x) log2 Q(x) (2)
The difference between these two coding costs is called the Kullback-Leibler (KL) divergence
KL(P ‖Q) d=ef ∑
x
P (x) log P (x)
Q(x) (3)
The KL divergence is non-negative and zero if and only if P=Q. It measures the coding inefficiency in bits from using a model Q to compress data when the true data distribution is P . Therefore, the better our model of the data, the more efficiently we can compress and communicate new data. This is an important link between machine learning, statistics, and information theory. An excellent text which elaborates on these relationships and many of the topics in this chapter is [1].
1.3 Bayes Rule
Bayes rule,
P (y|x) = P (x|y)P (y)
P (x) (4)
which follows from the equality P (x, y) = P (x)P (y|x) = P (y)P (x|y), can be used to motivate a coherent statistical framework for machine learning. The basic idea is the following. Imagine we wish to design a machine which has beliefs about the world, and updates these beliefs on the basis of observed data. The machine must somehow represent the strengths of its beliefs numerically. It has been shown that if you accept certain axioms of coherent inference, known as the Cox axioms, then a remarkable result follows [2]: If the machine is to represent the strength of its beliefs by real numbers, then the only reasonable and coherent way of manipulating these beliefs is to have them satisfy the rules of probability, such as Bayes rule. Therefore, P (X = x) can be used not only to represent the frequency with which the variable X takes on the value x (as in so-called frequentist statistics) but it can also be used to represent the degree of belief that X = x. Similarly, P (X = x|Y = y) can be used to represent the degree of belief that X = x given that one knows Y = y.3
3 Another way to motivate the use of the rules of probability to encode degrees of belief comes from game-theoretic arguments in the form of the Dutch Book Theorem. This theorem states that if you are willing to accept bets with odds based on your degrees of beliefs, then unless your beliefs are coherent in the sense that they satisfy the rules of probability theory, there exists a set of simultaneous bets (called a “Dutch Book”) which you will accept and which is guaranteed to lose you money, no matter what the outcome. The only way to ensure that Dutch Books don’t exist against you, is to have degrees of belief that satisfy Bayes rule and the other rules of probability theory.


76 Z. Ghahramani
From Bayes rule we derive the following simple framework for machine learning. Assume a universe of models Ω; let Ω = {1, . . . , M } although it need not be finite or even countable. The machines starts with some prior beliefs over models
m ∈ Ω (we will see many examples of models later), such that ∑M
m=1 P (m) = 1.
A model is simply some probability distribution over data points, i.e. P (x|m). For simplicity, let us further assume that in all the models the data is taken to be independently and identically distributed (i.i.d.). After observing a data set D = {x1, . . . , xN }, the beliefs over models is given by:
P (m|D) = P (m)P (D|m)
P (D) ∝ P (m)
N
∏
n=1
P (xn|m) (5)
which we read as the posterior over models is the prior multiplied by the likelihood, normalized. The predictive distribution over new data, which would be used to encode new data efficiently, is
P (x|D) =
M
∑
m=1
P (x|m)P (m|D) (6)
Again this follows from the rules of probability theory, and the fact that the models are assumed to produce i.i.d. data. Often models are defined by writing down a parametric probability distribution (again, we’ll see many examples below). Thus, the model m might have parameters θ, which are assumed to be unknown (this could in general be a vector of parameters). To be a well-defined model from the perspective of Bayesian learning, one has to define a prior over these model parameters P (θ|m) which naturally has to satisfy the following equality
P (x|m) =
∫
P (x|θ, m)P (θ|m)dθ (7)
Given the model m it is also possible to infer the posterior over the parameters of the model, i.e. P (θ|D, m), and to compute the predictive distribution, P (x|D, m). These quantities are derived in exact analogy to equations (5) and (6), except that instead of summing over possible models, we integrate over parameters of a particular model. All the key quantities in Bayesian machine learning follow directly from the basic rules of probability theory. Certain approximate forms of Bayesian learning are worth mentioning. Let’s focus on a particular model m with parameters θ, and an observed data set D. The predictive distribution averages over all possible parameters weighted by the posterior
P (x|D, m) =
∫
P (x|θ)P (θ|D, m)dθ. (8)
In certain cases, it may be cumbersome to represent the entire posterior distribution over parameters, so instead we will choose to find a point-estimate


Unsupervised Learning 77
of the parameters θˆ. A natural choice is to pick the most probable parameter value given the data, which is known as the maximum a posteriori or MAP parameter estimate
θˆMAP = arg mθax P (θ|D, m) = arg mθax
[
log P (θ|m) + ∑
n
log P (xn|θ, m)
]
(9)
Another natural choice is the maximum likelihood or ML parameter estimate
θˆML = arg mθax P (D|θ, m) = arg mθax
∑
n
log P (xn|θ, m) (10)
Many learning algorithms can be seen as finding ML parameter estimates. The ML parameter estimate is also acceptable from a frequentist statistical modeling perspective since it does not require deciding on a prior over parameters. However, ML estimation does not protect against overfitting—more complex models will generally have higher maxima of the likelihood. In order to avoid problems with overfitting, frequentist procedures often maximize a penalized or regularized log likelihood (e.g. [3]). If the penalty or regularization term is interpreted as a log prior, then maximizing penalized likelihood appears identical to maximizing a posterior. However, there are subtle issues that make a Bayesian MAP procedure and maximum penalized likelihood different [4]. One difference is that the MAP estimate is not invariant to reparameterization, while the maximum of the penalized likelihood is invariant. The penalized likelihood is a function, not a density, and therefore does not increase or decrease depending on the Jacobian of the reparameterization.
2 Latent Variable Models
The framework described above can be applied to a wide range of models. No singe model is appropriate for all data sets. The art in machine learning is to develop models which are appropriate for the data set being analyzed, and which have certain desired properties. For example, for high dimensional data sets it might be necessary to use models that perform dimensionality reduction. Of course, ultimately, the machine should be able to decide on the appropriate model without any human intervention, but to achieve this in full generality requires significant advances in artificial intelligence. In this section, we will consider probabilistic models that are defined in terms of some latent or hidden variables. These models can be used to do dimensionality reduction and clustering, the two cornerstones of unsupervised learning.
2.1 Factor Analysis
Let the data set D consist of D-dimensional real valued vectors,D = {y1, . . . , yN}. In factor analysis, the data is assumed to be generated from the following model
y = Λx + ǫ (11)


78 Z. Ghahramani
where x is a K-dimensional zero-mean unit-variance multivariate Gaussian vector with elements corresponding to hidden (or latent) factors, Λ is a D × K matrix of parameters, known as the factor loading matrix, and ǫ is a D-dimensional zero-mean multivariate Gaussian noise vector with diagonal covariance matrix Ψ . Defining the parameters of the model to be θ = (Ψ, Λ), by integrating out the factors, one can readily derive that
p(y|θ) =
∫
p(x|θ)p(y|x, θ)dx = N (0, ΛΛ⊤ + Ψ ) (12)
where N (μ, Σ) refers to a multivariate Gaussian density with mean μ and covariance matrix Σ. For more details refer to [5]. Factor analysis is an interesting model for several reasons. If the data is very high dimensional (D is large) then even a simple model like the full-covariance multivariate Gaussian will have too many parameters to reliably estimate or infer from the data. By choosing K < D, factor analysis makes it possible to model a Gaussian density for high dimensional data without requiring O(D2) parameters. Moreover, given a new data point, one can compute the posterior over the hidden factors, p(x|y, θ); since x is lower dimensional than y this provides a low-dimensional representation of the data (for example, one could pick the mean of p(x|y, θ) as the representation for y).
2.2 Principal Components Analysis (PCA)
Principal components analysis (PCA) is an important limiting case of factor analysis (FA). One can derive PCA by making two modifications to FA. First, the noise is assumed to be isotropic, in other words each element of ǫ has equal variance: Ψ = σ2I, where I is a D ×D identity matrix. This model is called probabilistic PCA [6, 7]. Second, if we take the limit of σ → 0 in probabilistic PCA, we obtain standard PCA (which also goes by the names Karhunen-Loe`ve expansion, and singular value decomposition; SVD). Given a data set with covariance matrix Σ, for maximum likelihood factor analysis the goal is to find parameters Λ, and Ψ for which the model ΛΛ⊤ + Ψ has highest likelihood. In PCA, the goal is to find Λ so that the likelihood is highest for ΛΛ⊤. Note that this matrix is singular unless K = D, so the standard PCA model is not a sensible model. However, taking the limiting case, and further constraining the columns of Λ to be orthogonal, it can be derived that the principal components correspond to the K eigenvectors with largest eigenvalue of Σ. PCA is thus attractive because the solution can be found immediately after eigendecomposition of the covariance. Taking the limit σ → 0 of p(x|y, Λ, σ) we find that it is a delta-function at x = Λ⊤y, which is the projection of y onto the principal components.
2.3 Independent Components Analysis (ICA)
Independent components analysis (ICA) extends factor analysis to the case where the factors are non-Gaussian. This is an interesting extension because


Unsupervised Learning 79
many real-world data sets have structure which can be modeled as linear combinations of sparse sources. This includes auditory data, images, biological signals such as EEG, etc. Sparsity simply corresponds to the assumption that the factors have distributions with higher kurtosis that the Gaussian. For example, p(x) = λ
2 exp{−λ|x|} has a higher peak at zero and heavier tails than a Gaus
sian with corresponding mean and variance, so it would be considered sparse (strictly speaking, one would like a distribution which had non-zero probability mass at 0 to get true sparsity). Models like PCA, FA and ICA can all be implemented using neural networks (multi-layer perceptrons) trained using various cost functions. It is not clear what advantage this implementation/interpretation has from a machine learning perspective, although it provides interesting ties to biological information processing. Rather than ML estimation, one can also do Bayesian inference for the parameters of probabilistic PCA, FA, and ICA.
2.4 Mixture of Gaussians
The densities modeled by PCA, FA and ICA are all relatively simple in that they are unimodal and have fairly restricted parametric forms (Gaussian, in the case of PCA and FA). To model data with more complex structure such as clusters, it is very useful to consider mixture models. Although it is straightforward to consider mixtures of arbitrary densities, we will focus on Gaussians as a common special case. The density of each data point in a mixture model can be written:
p(y|θ) =
K
∑
k=1
πk p(y|θk) (13)
where each of the K components of the mixture is, for example, a Gaussian with differing means and covariances θk = (μk, Σk) and πk is the mixing proportion
for component k, such that ∑K
k=1 πk = 1 and πk > 0, ∀k.
A different way to think about mixture models is to consider them as latent variable models, where associated with each data point is a K-ary discrete latent (i.e. hidden) variable s which has the interpretation that s = k if the data point was generated by component k. This can be written
p(y|θ) =
K
∑
k=1
P (s = k|π)p(y|s = k, θ) (14)
where P (s = k|π) = πk is the prior for the latent variable taking on value k, and p(y|s = k, θ) = p(y|θk) is the density under component k, recovering Equation (13).
2.5 K-Means
The mixture of Gaussians model is closely related to an unsupervised clustering algorithm known as k-means as follows: Consider the special case where all the


80 Z. Ghahramani
Gaussians have common covariance matrix proportional to the identity matrix: Σk = σ2I, ∀k, and let πk = 1/K, ∀k. We can estimate the maximum likelihood parameters of this model using the iterative algorithm which we are about to describe, known as EM. The resulting algorithm, as we take the limit σ2 → 0, becomes exactly the k-means algorithm. Clearly the model underlying k-means has only singular Gaussians and is therefore an unreasonable model of the data; however, k-means is usually justified from the point of view of clustering to minimize a distortion measure, rather than fitting a probabilistic models.
3 The EM Algorithm
The EM algorithm is an algorithm for estimating ML parameters of a model with latent variables. Consider a model with observed variables y, hidden/latent variables x, and parameters θ. We can lower bound the log likelihood for any data point as follows
L(θ) = log p(y|θ) = log
∫
p(x, y|θ) dx (15)
= log
∫
q(x) p(x, y|θ)
q(x) dx (16)
≥
∫
q(x) log p(x, y|θ)
q(x) dx d=ef F (q, θ) (17)
where q(x) is some arbitrary density over the hidden variables, and the lower bound holds due to the concavity of the log function (this inequality is known as Jensen’s inequality). The lower bound F is a functional of both the density q(x) and the model parameters θ. For a data set of N data points y(1), . . . , y(N), this lower bound is formed for the log likelihood term corresponding to each data point, thus there is a separate density q(n)(x) for each point and F (q, θ) =
∑
n F (n)(q(n), θ).
The basic idea of the Expectation-Maximization (EM) algorithm is to iterate between optimizing this lower bound as a function of q and as a function of θ. We can prove that this will never decrease the log likelihood. After initializing the parameters somehow, the kth iteration of the algorithm consists of the following two steps:
E Step: Optimize F with respect to the distribution q while holding the parameters fixed
qk(x) = arg max
q(x)
∫
q(x) log p(x, y|θk−1)
q(x) (18)
qk(x) = p(x|y, θk−1) (19)
M Step: Optimize F with respect to the parameters θ while holding the distribution over hidden variables fixed


Unsupervised Learning 81
θk = arg mθax
∫
qk(x) log p(x, y|θ)
qk(x) dx (20)
θk = arg mθax
∫
qk(x) log p(x, y|θ) dx (21)
Let us be absolutely clear what happens for a data set of N data points: In the E step, for each data point, the distribution over the hidden variables is set to the posterior for that data point q(n)
k (x) = p(x|y(n), θk−1), ∀n. In the M
step the single set of parameters is re-estimated by maximizing the sum of the expected log likelihoods: θk = arg maxθ
∑
n
∫ q(n)
k (x) log p(x, y(n)|θ) dx.
Two things are still unclear: how does (19) follow from (18), and how is this algorithm guaranteed to increase the likelihood? The optimization in (18) can be written as follows since p(x, y|θk−1) = p(y|θk−1)p(x|y, θk−1):
qk(x) = arg max
q(x)
[
log p(y|θk−1) +
∫
q(x) log p(x|y, θk−1)
q(x) dx
]
(22)
Now, the first term is a constant w.r.t. q(x) and the second term is the negative of the Kullback-Leibler divergence
KL(q(x)‖p(x|y, θk−1)) =
∫
q(x) log q(x)
p(x|y, θk−1) dx (23)
which we have seen in Equation (3) in its discrete form. This is minimized at q(x) = p(x|y, θk−1), where the KL divergence is zero. Intuitively, the interpretation of this is that in the E step of EM, the goal is to find the posterior distribution of the hidden variables given the observed variables and the current settings of the parameters. We also see that since the KL divergence is zero, at the end of the E step, F (qk, θk−1) = L(θk−1). In the M step, F is increased with respect to θ. Therefore, F (qk, θk) ≥ F (qk, θk−1). Moreover, L(θk) = F (qk+1, θk) ≥ F (qk, θk) after the next E step. We can put these steps together to establish that L(θk) ≥ L(θk−1), establishing that the algorithm is guaranteed to increase the likelihood or keep it fixed (at convergence). The EM algorithm can be applied to all the latent variable models described above, i.e. FA, probabilistic PCA, mixture models, and ICA. In the case of mixture models, the hidden variable is the discrete assignment s of data points to clusters; consequently the integrals turn into sums where appropriate. EM has wide applicability to latent variable models, although it is not always the fastest optimization method [8]. Moreover, we should note that the likelihood often has many local optima and EM will converge some local optimum which may not be the global one. EM can also be used to estimate MAP parameters of a model, and as we will see in Section 11.4 there is a Bayesian generalization of EM as well.


82 Z. Ghahramani
4 Modeling Time Series and Other Structured Data
So far we have assumed that the data is unstructured, that is, the observations are assumed to be independent and identically distributed. This assumption is unreasonable for many data sets in which the observations arrive in a sequence and subsequent observations are correlated. Sequential data can occur in time series modeling (as in financial data or the weather) and also in situations where the sequential nature of the data is not necessarily tied to time (as in protein data which consist of sequences of amino acids). As the most basic level, time series modeling consists of building a probabilistic model of the present observation given all past observations p(yt|yt−1, yt−2 . . .). Because the history of observations grows arbitrarily large it is necessary to limit the complexity of such a model. There are essentially two ways of doing this. The first approach is to limit the window of past observations. Thus one can simply model p(yt|yt−1) and assume that this relation holds for all t. This is known as a first-order Markov model. A second-order Markov model would be p(yt|yt−1, yt−2), and so on. Such Markov models have two limitations: First, the influence of past observations on present observations vanishes outside this window, which can be unrealistic. Second, it may be unnatural and unwieldy to model directly the relationship between raw observations at one time step and raw observations at a subsequent time step. For example, if the observations are noisy images, it would make more sense to de-noise them, extract some description of the objects, motions, illuminations, and then try to predict from that. The second approach is to make use of latent or hidden variables. Instead of modeling directly the effect of yt−1 on yt, we assume that the observations were generated from some underlying hidden variable xt which captures the dynamics of the system. For example, y might be noisy sonar readings of objects in a room, while x might be the actual locations and sizes of these objects. We usually call this hidden variable x the state variable since it is meant to capture all the aspects of the system relevant to predicting the future dynamical behavior of the system. In order to understand more complex time series models, it is essential that one be familiar with state-space models (SSMs) and hidden Markov models (HMMs). These two classes of models have played a historically important role in control engineering, visual tracking, speech recognition, protein sequence modeling, and error decoding. They form the simplest building blocks from which other richer time-series models can be developed, in a manner completely analogous to the role that FA and mixture models play in building more complex models for i.i.d. data.
4.1 State-Space Models (SSMs)
In a state-space model, the sequence of observed data y1, y2, y3, . . . is assumed to have been generated from some sequence of hidden state variables x1, x2, x3, . . ..


Unsupervised Learning 83
Letting x1:T denote the sequence x1, . . . , xT , the basic assumption in an SSM is that the joint probability of the hidden states and observations factors in the following way:
p(x1:T , y1:T |θ) =
T
∏
t=1
p(xt|xt−1, θ)p(yt|xt, θ) (24)
In order words, the observations are assumed to have been generated from the hidden states via p(yt|xt, θ), and the hidden states are assumed to have first-order Markov dynamics captured by p(xt|xt−1, θ). We can consider the first term p(x1|x0, θ) to be a prior on the initial state of the system x1. The simplest kind of state-space model assumes that all variables are multivariate Gaussian distributed and all the relationships are linear. In such linearGaussian state-space models, we can write
yt = Cxt + vt (25) xt = Axt−1 + wt (26)
where the matrices C and A define the linear relationships and v and w are zeromean Gaussian noise vectors with covariance matrices R and Q respectively. If we assume that the prior on the initial state p(x1) is also Gaussian, then all subsequent xs and ys are also Gaussian due the the fact that Gaussian densities are closed under linear transformations. This model can be generalized in many ways, for example by augmenting it to include a sequence of observed inputs u1, . . . , uT as well as the observed model outputs y1, . . . , yT , but we will not discuss generalizations further. By comparing equations (11) and (25) we see that linear-Gaussian SSMs can be thought of as a time-series generalization of factor analysis where the factors are assumed to have linear-Gaussian dynamics over time. The parameters of this model are θ = (A, C, Q, R). To learn ML settings of these parameters one can make use of the EM algorithm [9]. The E step of the algorithm involves computing q(x1:T ) = p(x1:T |y1:T , θ) which is the posterior over hidden state sequences. In fact, this whole posterior does not have to be computed or represented, all that is required are the marginals q(xt) and pairwise marginals q(xt, xt+1). These can be computed via the Kalman smoothing algorithm, which is an efficient algorithm for inferring the distribution over the hidden states of a linear-Gaussian SSM. Since the model is linear, the M step of the algorithm requires solving a pair of weighted linear regression problems to re-estimate A and C, while Q and R are estimated from the residuals of those regressions. This is analogous to the M step of factor analysis, which also involves solving a linear regression problem.
4.2 Hidden Markov Models (HMMs)
Hidden Markov models are similar to state-space models in that the sequence of observations is assumed to have been generated from a sequence of underlying


84 Z. Ghahramani
hidden states. The key difference is that in HMMs the state is assumed to be discrete rather than a continuous random vector. Let st denote the hidden state of an HMM at time t. We assume that st can take discrete values in {1, . . . , K}. The model can again be written as in (24):
P (s1:T , y1:T |θ) =
T
∏
t=1
P (st|st−1, θ)P (yt|st, θ) (27)
where P (s1|s0, θ) is simply some initial distribution over the K settings of the first hidden state; we can call this discrete distribution π, represented by a K ×1 vector. The state-transition probabilities P (st|st−1, θ) are captured by a K × K transition matrix A, with elements Aij = P (st = i|st−1 = j, θ). The observations in an HMM can be either continuous or discrete. For continuous observations yt one can for example choose a Gaussian density; thus p(yt|st = i, θ) would be a different Gaussian for each choice of i ∈ {1, . . . , K}. This model is the dynamical generalization of a mixture of Gaussians. The marginal probability at each point in time is exactly a mixture of K Gaussians—the difference is that which component generates data point yt and which component generated yt−1 are not independent random variables, but certain combinations are more and less probable depending on the entries in A. For yt a discrete observation, let us assume that it can take on values {1, . . . , L}. In that case the output probabilities P (yt|st, θ) can be captured by an L × K emission matrix, E. The model parameters for a discrete-observation HMM are θ = (π, A, E). Maximum likelihood learning of the model parameters can be approached using the EM algorithm, which in the case of HMMs is known as the BaumWelch algorithm. The E step involves computing Q(st) and Q(st, st+1) which are marginals of Q(s1:T ) = P (s1:T |y1:T , θ). These marginals are computed as part of the forward–backward algorithm which as the name suggests sweeps forward and backward through the time series, and applies Bayes rule efficiently using the Markov conditional independence properties of the HMM, to compute the required marginals. The M step of HMM learning involves re-estimating π, A, and E by adding up and normalizing expected counts for transitions and emissions that were computed in the E step.
4.3 Modeling Other Structured Data
We have considered the case of i.i.d. data and time series data. The observations in real world data sets can have many other possible structures as well. Let us mention a few examples, although it is not possible to strive for completeness. In spatial data, the points are assumed to live in some metric, often Euclidean, space. Three examples of spatial data include epidemiological data which can be modeled as a function of the spatial location of the measurement; data from computer vision where the observations are measurements of features on a 2D input to the camera; and functional neuroimaging where the data can be physiological measurements related to neural activity located in 3D voxels defining coordinates in the brain. Generalizing HMMs, one can define Markov random


Unsupervised Learning 85
field models where there are a set of hidden variables correlated to neighbors in some lattice, and related to the observed variables. Hierarchical or tree-structured data contains known or unknown tree-like correlation structure between the data points or measured features. For example, the data points may be features of animals related through an evolutionary tree. A very different form of structured data is if each data point itself is treestructured, for example if each point is a parse tree of a sentence in the English language. Finally, one can take the structured dependencies between variables and consider the structure itself as an unknown part of the model. Such models are known as probabilistic relational models and are closely related to graphical models which we will discuss in Section 7.
5 Nonlinear, Factorial, and Hierarchical Models
The models we have described so far are attractive because they are relatively simple to understand and learn. However, their simplicity is also a limitation, since the intricacies of real-world data are unlikely to be well-captured by a simple statistical model. This motivates us to seek to describe and study learning in much more flexible models. A simple combination of two of the ideas we have described for i.i.d. data is the mixture of factor analyzers [10, 11, 12]. This model performs simultaneous clustering and dimensionality reduction on the data, by assuming that the covariance in each Gaussian cluster can be modeled by an FA model. Thus, it becomes possible to apply a mixture model to very high dimensional data while allowing each cluster to span a different sub-space of the data. As their name implies linear-Gaussian SSMs are limited by assumptions of linearity and Gaussian noise. In many realistic dynamical systems there are significant nonlinear effects, which make it necessary to consider learning in nonlinear state-space models. Such models can also be learned using the EM algorithm, but the E step must deal with inference in non-Gaussian and potentially very complicated densities (since non-linearities will turn Gaussians into non-Gaussians), and the M step is nonlinear regression, rather than linear regression [13]. There are many methods of dealing with inference in non-linear SSMs, including methods such as particle filtering [14, 15, 16, 17, 18, 19], linearization [20], the unscented filter [21, 22], the EP algorithm [23], and embedded HMMs [24]. Non-linear models are also important if we are to consider generalizing simple dimensionality reduction models such as PCA and FA. These models are limited in that they can only find a linear subspace of the data to capture the correlations between the observed variables. There are many interesting and important nonlinear dimensionality reduction models, including generative topographic mappings (GTM) [25] (a probabilistic alternative to Kohonen maps), multi-dimensional scaling (MDS) [26, 27], principal curves [28], Isomap [29], and locally linear embedding (LLE) [30].


86 Z. Ghahramani
Hidden Markov models also have their limitations. Even though they can model nonlinear dynamics by discretizing the hidden state space, an HMM with K hidden states can only capture log2 K bits of information in its state variable about the past of the sequence. HMMs can be extended by allowing a vector of discrete state variables, in an architecture known as a factorial HMM [31]. Thus a vector of M variables, each of which can take K states, can capture KM possible states in total, and M log2 K bits of information about the past of the sequence. The problem is that such a model, if dealt with naively as an HMM would have exponentially many parameters and would take exponentially long to do inference in. Both the complexity in time and number of parameters can be alleviated by restricting the interactions between the hidden variables at one time step and at the next time step. A generalization of these ideas is the notion of a dynamical Bayesian network (DBN) [32].
A relatively old but still quite powerful class of models for binary data is the Boltzmann machine (BM) [33]. This is a simple model inspired from Ising models in statistical physics. A BM is a multivariate model for capturing correlations and higher order statistics in vectors of binary data. Consider data consisting of vectors of M binary variables (t he elements of the vector may, for example, be pixels in a black-and-white image). Clearly, each data point can be an instance of one of 2M possible patterns. An arbitrary distribution over such patterns would require a table with 2M − 1 entries, again intractable in number of parameters, storage, and computation time. A BM allows one to define flexible distributions over the 2M entries of this table by using O(M 2) parameters defining a symmetric matrix of weights connecting the variables. This can be augmented with hidden variables in order to enrich the model class, without adding exponentially many parameters. These hidden variables can be organized into layers of a hierarchy as in the Helmholtz machine [34]. Other hierarchical models include recent generalizations of ICA designed to capture higher order statistics in images [35].
6 Intractability
The problem with the models described in the previous section is that learning their parameters is in general computationally intractable. In a model with exponentially many settings for the hidden states, doing the E step of an EM algorithm would require computing appropriate marginals of a distribution over exponentially many possibilities. Let us consider a simple example. Imagine we have a vector of N binary random variables s = (s1, . . . , sN ), where si ∈ {0, 1} and a vector of N known
integers (r1, . . . , rN ) where ri ∈ {1, 2, 3, . . . , 10}. Let the variable Y = ∑N
i=1 risi.
Assume that the binary variables are all independent and identically distributed with P (si = 1) = 1/2, ∀i. Let N be 100. Now imagine that we are told Y = 430. How do we compute P (si = 1|Y = 430)? The problem is that even though the si were independent before we observed the value of Y , now that we know the value of Y , not all settings of s are possible anymore. To figure out for some si


Unsupervised Learning 87
the probability of P (si = 1|Y = 430) requires that we enumerate all potentially exponentially many ways of achieving Y = 430 and counting how many of those had si = 1 vs si = 0. This example illustrates the following ideas: Even if the prior is simple, the posterior can be very complicated. Whether two random variables are independent or not is a function of one’s state of knowledge. Thus si and sj may be independent if we are not told the value of Y but are certainly dependent given the value of Y . These type of phenomena are related to “explaining-away” which refers to the fact that if there are multiple potential causes for some effect, observing one, explains away the need for the others [36]. Intractability can thus occur if we have a model with discrete hidden variables which can take on exponentially many combinations. Intractability can also occur with continuous hidden variables if their density is not simply described, or if they interact with discrete hidden variables. Moreover, even for simple models, such as a mixture of Gaussians, intractability occurs when we consider the parameters to be unknown as well, and we attempt to do Bayesian inference on them. To deal with intractability it is essential to have good tools for representing multivariate distributions, such as graphical models.
7 Graphical Models
Graphical models are an important tool for representing the dependencies between random variables in a probabilistic model. They are important for two reasons. First, graphs are an intuitive way of visualizing dependencies. We are used to graphical depictions of dependency, for example in circuit diagrams and in phylogenetic trees. Second, by exploiting the structure of the graph it is possible to devise efficient message passing algorithms for computing marginal and conditional probabilities in a complicated model. We discuss message passing algorithms for inference in Section 8. The main statistical property represented explicitly by the graph is conditional independence between variables. We say that X and Y are conditionally independent given Z, if P (X, Y |Z) = P (X|Z)P (Y |Z) for all values of the variables X,Y , and Z where these quantities are defined (i.e. excepting settings z where P (Z = z) = 0). We use the notation X⊥⊥Y |Z to denote the above conditional independence relation. Conditional independence generalists to sets of variables in the obvious way, and it is different from marginal independence which states that P (X, Y ) = P (X)P (Y ), and is denoted X⊥⊥Y . There are several different graphical formalisms for depicting conditional independence relationships. We focus on three of the main ones: undirected, factor, and directed graphs.
7.1 Undirected Graphs
In an undirected graphical model each random variable is represented by a node, and the edges of the graph indicate conditional independence relationships.


88 Z. Ghahramani
A
C
B
D
E
A
C
B
D
E
A
C
B
D
E
Fig. 1. Three kinds of probabilistic graphical model: undirected graphs, factor graphs and directed graphs
Specifically, let X , Y, and Z be sets of random variables. Then X ⊥⊥Y|Z if every path on the graph from a node in X to a node in Y has to go through a node in Z. Thus a variable X is conditionally independent of all other variables given the neighbors of X, and we say that the neighbors separate X from the rest of the graph. An example of an undirected graph is shown in Figure 1. In this graph A⊥⊥B|C and B⊥⊥E|{C, D}, for example, and the neighbors of D are B, C, E. A clique is a fully connected subgraph of a graph. A maximal clique is not contained in any other clique of the graph. It turns out that the set of conditional independence relations implied by the separation properties in the graph are satisfied by probability distributions which can be written as a normalized product of non-negative functions over the variables in the maximal cliques of the graph (this is known as the Hammersley-Clifford Theorem [37]). In the example in Figure 1, this implies that the probability distribution over (A, B, C, D, E) can be written as:
P (A, B, C, D, E) = c g1(A, C)g2(B, C, D)g3(C, D, E) (28)
Here, c is the constant that ensures that the probability distribution sums to 1, and g1, g2 and g3 are non-negative functions of their arguments. For example, if all the variables are binary the function g2 is a table with a non-negative number for each of the 8 = 2 × 2 × 2 possible settings of the variables B, C, D. These non-negative functions are supposed to represent how compatible these settings are with each other, with a 0 encoding logical incompatibility. For this reason, the g’s are sometimes referred to as compatibility functions, other times as potential functions. Undirected graphical models are also sometimes referred to as Markov networks.
7.2 Factor Graphs
In a factor graph there are two kinds of nodes, variable nodes and factor nodes, usually denoted as open circles and filled dots (Figure 1). Like an undirected model, the factor graph represents a factorization of the joint probability distribution: each factor is a non-negative function of the variables connected to the corresponding factor node. Thus for the factor graph in Figure 1 we have:


Unsupervised Learning 89
P (A, B, C, D, E) = cg1(A, C)g2(B, C)g3(B, D), g4(C, D)g5(C, E)g6(D, E)
(29)
Factor nodes are also sometimes called function nodes. Again, as in an undirected graphical model, the variables in a set X are conditionally independent of the variables in a set Y given Z if all paths from X to Y go through variables in Z. Note that the factor graph is Figure 1 has exactly the same conditional independence relations as the undirected graph, even though the factors in the former are contained in the factors in the latter. Factor graphs are particularly elegant and simple when it comes to implementing message passing algorithms for inference (Section 8).
7.3 Directed Graphs
In directed graphical models, also known as probabilistic directed acyclic graphs (DAGs), belief networks, and Bayesian networks, the nodes represent random variables and the directed edges represent statistical dependencies. If there exists an edge from A to B we say that A is a parent of B, and conversely B is a child of A. A directed graph corresponds to the factorization of the joint probability into a product of the conditional probabilities of each node given its parents. For the example in Figure 1 we write:
P (A, B, C, D, E) = P (A)P (B)P (C|A, B)P (D|B, C)P (E|C, D) (30)
In general we would write:
P (X1, . . . , XN ) =
N
∏
i=1
P (Xi|Xpai ) (31)
where Xpai denotes the variables that are parents of Xi in the graph.
Assessing the conditional independence relations in a directed graph is slightly less trivial than in undirected and factor graphs. Rather than simply looking at separation between sets of variables, one has to consider the directions of the edges. The graphical test for two sets of variables being conditionally independent given a third is called d-separation [36]. D-separation takes into account the following fact about v-structures of the graph, which consist of two (or more) parents of a child, as in the A → C ← B subgraph in Figure 1. In such a v-structure A⊥⊥B, but it is not true that A⊥⊥B|C. That is, A and B are marginally independent, but conditionally dependent given C. This can be easily checked by writing out P (A, B, C) = P (A)P (B)P (C|A, B). Summing out C leads to P (A, B) = P (A)P (B). However, given the value of C, P (A, B|C) = P (A)P (B)P (C|A, B)/P (C) which does not factor into separate functions of A and B. As a consequence of this property of v-structures, in a directed graph a variable X is independent of all other variables given the parents of X, the children of X, and the parents of the children of X. This is the minimal set that d-separates X from the rest of the graph and is known as the Markov boundary for X.


90 Z. Ghahramani
It is possible, though not always appropriate, to interpret a directed graphical model as a causal generative model of the data. The following procedure would generate data from the probability distribution defined by a directed graph: draw a random value from the marginal distribution of all variables which do not have any parents (e.g. a ∼ P (A), b ∼ P (B)), then sample from the conditional distribution of the children of these variables (e.g. c ∼ P (C|A = a, B = a)), and continue this procedure until all variables are assigned values. In the model, P (C|A, B) can capture the causal relationship between the causes A and B and the effect C. Such causal interpretations are much less natural for undirected and factor graphs, since even generating a sample from such models cannot easily be done in a hierarchical manner starting from “parents” to “children” except in special cases. Moreover, the potential functions capture mutual compatibilities, rather than cause-effect relations. A useful property of directed graphical models is that there is no global normalization constant c. This global constant can be computationally intractable to compute in undirected and factor graphs. In directed graphs, each term is a conditional probability and is therefore already normalized ∑
x P (Xi = x|Xpai ) = 1.
7.4 Expressive Power
Directed, undirected and factor graphs are complementary in their ability to express conditional independence relationships. Consider the directed graph consisting of a single v-structure A → C ← B. This graph encodes A⊥⊥B but not A⊥⊥B|C. There exists no undirected graph or factor graph over these three variables which captures exactly these independencies. For example, in A − C − B it is not true that A⊥⊥B but it is true that A⊥⊥B|C. Conversely, if we consider the undirected graph in Figure 2, we see that some independence relationships are better captured by undirected models (and factor graphs).
Fig. 2. No directed graph over 4 variables can represent the set of conditional independence relationships represented by this undirected graph
8 Exact Inference in Graphs
Probabilistic inference in a graph usually refers to the problem of computing the conditional probability of some variable Xi given the observed values of some other variables Xobs = xobs while marginalizing out all other variables. Starting from a joint distribution P (X1, . . . , XN ), we can divide the set of all variables into three exhaustive and mutually exclusive sets {X1, . . . XN } = {Xi} ∪ Xobs ∪ Xother. We wish to compute


Unsupervised Learning 91
P (Xi|Xobs = xobs) =
∑
x P (Xi, Xother = x, Xobs = xobs)
∑
x′
∑
x P (Xi = x′, Xother = x, Xobs = xobs) (32)
The problem is that the sum over x is exponential in the number of variables in Xother. For example. if there are M variables in Xother and each is binary,
then there are 2M possible values for x. If the variables are continuous, then the desired conditional probability is the ratio of two high-dimensional integrals, which could be intractable to compute. Probabilistic inference is essentially a problem of computing large sums and integrals. There are several algorithms for computing these sums and integrals which exploit the structure of the graph to get the solution efficiently for certain graph structures (namely trees and related graphs). For general graphs the problem is fundamentally hard [38].
8.1 Elimination
The simplest algorithm conceptually is variable elimination. It is easiest to explain with an example. Consider computing P (A = a|D = d) in the directed graph in Figure 1. This can be written
P (A = a|D = d) ∝ ∑
c
∑
b
∑
e
P (A = a, B = b, C = c, D = d, E = e)
=
∑
c
∑
b
∑
e
P (A = a)P (B = b)P (C = c|A = a, B = b)
P (D = d|C = c, B = b)P (E = e|C = c, D = d)
=
∑
c
∑
b
P (A = a)P (B = b)P (C = c|A = a, B = b)
P (D = d|C = c, B = b) ∑
e
P (E = e|C = c, D = d)
=
∑
c
∑
b
P (A = a)P (B = b)P (C = c|A = a, B = b)
P (D = d|C = c, B = b)
What we did was (1) exploit the factorization, (2) rearrange the sums, and (3) eliminate a variable, E. We could repeat this procedure and eliminate the variable C. When we do this we will need to compute a new function φ(A =
a, B = b, D = d) d=ef ∑
c P (C = c|A = a, B = b)P (D = d|C = c, B = b),
resulting in:
P (A = a|D = d) ∝ ∑
b
P (A = a)P (B = b)φ(A = a, B = b, D = d)
Finally, we eliminate B by computing φ′(A = a, D = d) d=ef ∑
b P (B =
b)φ(A = a, B = b, D = d) to get our final answer which can be written
P (A = a|D = d) ∝ P (A = a)φ′(A = a, D = d) = P (A = a)φ′(A = a, D = d)
∑
a P (A = a)φ′(A = a, D = d)


92 Z. Ghahramani
The functions we get when we eliminate variables can be thought of as messages sent by that variable to its neighbors. Eliminating transforms the graph by removing the eliminated node and drawing (undirected) edges between all the nodes in the Markov boundary of the eliminated node. The same answer is obtained no matter what order we eliminate variables in; however, the computational complexity can depend dramatically on the ordering used.
8.2 Belief Propagation
The belief propagation (BP) algorithm is a message passing algorithm for computing conditional probabilities of any variable given the values of some set of other variables in a singly-connected directed acyclic graph [36]. The algorithm itself follows from the rules of probability and the conditional independence properties of the graph. Whereas variable elimination focuses on finding the conditional probability of a single variable Xi given Xobs = xobs, belief propagation can compute at once all the conditionals p(Xi|Xobs = xobs) for all i not observed. We first need to define singly-connected directed graphs. A directed graph is singly connected if between every pair of nodes there is only one undirected path. An undirected path is a path along the edges of the graph ignoring the direction of the edges: in other words the path can traverse edges both upstream and downstream. If there is more than one undirected path between any pair of nodes then the graph is said to be multiply connected, or loopy (since it has loops). Singly connected graphs have an important property which BP exploits. Let us call the set of observed variables the evidence, e = Xobs. Every node in
the graph divides the evidence into upstream e+
X and downstream e−
X parts.
For example, in Figure 3 the variables U1 . . . Un their parents, ancestors, and children and descendents (not including X, its children and descendents) and anything else connected to X via an edge directed toward X are all considered to be upstream of X; anything connected to X via an edge away from X is considered downstream of X (e.g. Y1, its children, the parents of its children, etc). Similarly, every edge X → Y in a singly connected graph divides the
X
Y
UU
Y
1
1
n
......
......
m
Fig. 3. Belief propagation in a directed graph


Unsupervised Learning 93
evidence into upstream and downstream parts. This separation of the evidence into upstream and downstream components does not generally occur in multiplyconnected graphs. Belief propagation uses three key ideas to compute the probability of some variable given the evidence p(X|e), which we can call the “belief” about X.4 First, the belief about X can be found by combining upstream and downstream evidence:
P (X|e) = P (X, e)
P (e) ∝ P (X, e+
X , e−
X ) ∝ P (X|e+
X )P (e−
X |X) (33)
The last proportionality results from the fact that given X the downstream
and upstream evidence are conditionally independent: P (e−
X |X, e+
X ) = P (e−
X |X).
Second, the effect of the upstream and downstream evidence on X can be computed via a local message passing algorithm between the nodes in the graph. Third, the message from X to Y has to be constructed carefully so that node X doesn’t send back to Y any information that Y sent to X, otherwise the message passing algorithm would reverberate information between nodes amplifying and distorting the final beliefs. Using these ideas and the basic rules of probability we can arrive at the following equations, where ch(X) and pa(X) are children and parents of X, respectively:
λ(X) d=ef P (e−
X |X) = ∏
j ∈ch(X )
P (e−
XYj |X) (34)
π(X) d=ef P (X|e+
X) = ∑
U1 ...Un
P (X|U1, . . . , Un) ∏
i∈pa(X )
P (Ui|e+
UiX ) (35)
Finally, the messages from parents to children (e.g. X to Yj) and the messages from children to parents (e.g. X to Ui) can be computed as follows:
πYj (X) d=ef P (X|e+
XYj )
∝
[∏
k=j
P (e−
XYk |X)
]∑
U1 ,...,Un
P (X|U1 . . . Un) ∏
i
P (Ui|e+
UiX ) (36)
λX (Ui) d=ef P (e−
UiX |Ui)
=
∑
X
P (e−
X |X) ∑
Uk :k=i
P (X|U1 . . . Un) ∏
k=i
P (Uk|e+
UkX ) (37)
It is important to notice that in the computation of both the top-down message (36) and the bottom-up message (37) the recipient of the message is explicitly excluded. Pearl’s [36] mnemonic of calling these messages λ and π messages is meant to reflect their role in computing “likelihood” and “prior” terms.
4 There is considerably variety in the field regarding the naming of algorithms. Belief propagation is also known as the sum-product algorithm, a name which some people prefer since beliefs seem subjective.